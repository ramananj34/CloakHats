{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1b43cd",
   "metadata": {},
   "source": [
    "# CloakHat Patch Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed60",
   "metadata": {},
   "source": [
    "## 1: Conda Setup\n",
    "\n",
    "Set up the environment\n",
    "\n",
    "`conda create -n cloakhat python=3.10 -y` <br>\n",
    "`conda activate cloakhat`\n",
    "\n",
    "PyTorch with CUDA <br>\n",
    "`conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y`\n",
    "\n",
    "PyTorch3D for differentiable rendering <br>\n",
    "`pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"`\n",
    "\n",
    "Detection models <br>\n",
    "`pip install ultralytics`\n",
    "\n",
    "Pip stuff <br>\n",
    "`pip install opencv-python-headless matplotlib tqdm tensorboard pyyaml trimesh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc07834",
   "metadata": {},
   "source": [
    "## 2: Python Setup\n",
    "\n",
    "Get the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346291c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (look_at_view_transform, FoVPerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, SoftPhongShader, TexturesUV, PointLights)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c892",
   "metadata": {},
   "source": [
    "## 3: Config\n",
    "\n",
    "Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_dir': './data/drone_footage', #Drone footage\n",
    "    'mesh_path': './assets/hat.obj', #Hat meshes\n",
    "    'output_dir': './outputs',\n",
    "    \n",
    "    #Generator\n",
    "    'latent_channels': 128,\n",
    "    'latent_size': 9, #Spatial size of latent input\n",
    "    'texture_size': 288, #Output texture size from generator\n",
    "    \n",
    "    #Viewpoint sampling\n",
    "    'elevation_range': (60, 90), #Degrees from horizontal (90 = overhead)\n",
    "    'scale_range': (0.3, 1.2), #Altitude proxy\n",
    "    \n",
    "    #Training Stage 1\n",
    "    'stage1_epochs': 100,\n",
    "    'stage1_batch_size': 8,\n",
    "    'stage1_lr': 2e-4,\n",
    "    \n",
    "    #Training Stage 2  \n",
    "    'stage2_iterations': 2000,\n",
    "    'stage2_lr': 0.01,\n",
    "    'local_latent_size': 18, #Size of optimizable latent pattern\n",
    "    \n",
    "    #Loss weights\n",
    "    'lambda_tv': 2.5, #Total variation\n",
    "    'lambda_nps': 0.01, #Non-printability score\n",
    "    'lambda_info': 0.1, #Mutual information (Stage 1 only)\n",
    "    \n",
    "    #T-SEA Stuff\n",
    "    'cutout_prob': 0.9,\n",
    "    'cutout_ratio': 0.4,\n",
    "    'shakedrop_prob': 0.5,\n",
    "    \n",
    "    #Rendering\n",
    "    'render_size': 256,\n",
    "    \n",
    "    #Printing (PLACEHOLDER, need details from FABLAB)\n",
    "    'nps_threshold': 0.7,  #Saturation * brightness threshold\n",
    "    \n",
    "    #Attack config (white, gray, black)\n",
    "    'attack_mode': 'gray',\n",
    "}\n",
    "\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1445b0",
   "metadata": {},
   "source": [
    "## 4: FCN Generator\n",
    "\n",
    "Make the texture (turn noise into an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            #9 -> 9\n",
    "            nn.Conv2d(latent_channels, 512, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            #9 -> 18\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #18 -> 36\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 36 -> 72\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 72 -> 144\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 144 -> 288\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 288 -> 288 (to RGB)\n",
    "            nn.Conv2d(32, 3, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_size = 288\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "    \n",
    "    def generate(self, z=None, batch_size=1):\n",
    "        if z is None:\n",
    "            z = torch.randn(batch_size, 128, 9, 9, device=next(self.parameters()).device)\n",
    "        return (self.forward(z) + 1) / 2\n",
    "\n",
    "# Test\n",
    "generator = FCNGenerator().to(device)\n",
    "test_texture = generator.generate(batch_size=1)\n",
    "logger.info(f\"Generator output: {test_texture.shape}\")  #Should be (1, 3, 288, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec701a8",
   "metadata": {},
   "source": [
    "## 5: Auxiliary Network\n",
    "\n",
    "Forces the texture to derive from the latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryNetwork(nn.Module):\n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Texture encoder\n",
    "        self.tex_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Latent encoder\n",
    "        self.lat_enc = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 256, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Joint network\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texture, z):\n",
    "        tex_feat = self.tex_enc(texture)\n",
    "        lat_feat = self.lat_enc(z)\n",
    "        return self.joint(torch.cat([tex_feat, lat_feat], dim=1))\n",
    "\n",
    "\n",
    "def compute_mi_loss(aux_net, texture, z):\n",
    "\n",
    "    #Matched pairs\n",
    "    T_joint = aux_net(texture, z)\n",
    "    pos_term = -F.softplus(-T_joint).mean()\n",
    "    \n",
    "    #Mismatched pairs (shuffle z)\n",
    "    z_shuffle = z[torch.randperm(z.size(0))]\n",
    "    T_marginal = aux_net(texture, z_shuffle)\n",
    "    neg_term = F.softplus(T_marginal).mean()\n",
    "    \n",
    "    mi = pos_term - neg_term\n",
    "    return -mi  #Negate because we minimize loss but want to maximize MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb40bb",
   "metadata": {},
   "source": [
    "## 6: Render Hat\n",
    "\n",
    "Render the hat using the texture and capture angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatRenderer:\n",
    "    def __init__(self, mesh_path, render_size=256, device='cuda'):\n",
    "        self.device = device\n",
    "        self.render_size = render_size\n",
    "        \n",
    "        #Load mesh\n",
    "        self.mesh_loaded = False\n",
    "        if Path(mesh_path).exists():\n",
    "            verts, faces, aux = load_obj(mesh_path, device=device)\n",
    "            self.verts = verts\n",
    "            self.faces = faces.verts_idx\n",
    "            self.verts_uvs = aux.verts_uvs\n",
    "            self.faces_uvs = faces.textures_idx\n",
    "            self.mesh_loaded = True\n",
    "            logger.info(f\"Loaded mesh: {len(verts)} verts, {len(self.faces)} faces\")\n",
    "        else:\n",
    "            logger.warning(f\"Mesh not found at {mesh_path}. Using placeholder.\")\n",
    "            self._create_placeholder_mesh()\n",
    "            \n",
    "        #Rasterization settings\n",
    "        self.raster_settings = RasterizationSettings(image_size=render_size, blur_radius=0.0, faces_per_pixel=1)\n",
    "    \n",
    "    #This method was made with help from ChatGPT\n",
    "    def _create_placeholder_mesh(self):\n",
    "        #Simple disk\n",
    "        n_points = 32\n",
    "        angles = torch.linspace(0, 2*np.pi, n_points+1)[:-1]\n",
    "        #Vertices: center + rim\n",
    "        verts = [[0, 0, 0]]  # center\n",
    "        for a in angles:\n",
    "            verts.append([torch.cos(a).item(), torch.sin(a).item(), 0])\n",
    "        self.verts = torch.tensor(verts, dtype=torch.float32, device=self.device)\n",
    "        #Faces: triangles from center to rim\n",
    "        faces = []\n",
    "        for i in range(n_points):\n",
    "            faces.append([0, i+1, (i % n_points) + 2 if i < n_points-1 else 1])\n",
    "        self.faces = torch.tensor(faces, dtype=torch.int64, device=self.device)\n",
    "        #UVs: simple radial mapping\n",
    "        uvs = [[0.5, 0.5]]  # center\n",
    "        for a in angles:\n",
    "            uvs.append([0.5 + 0.5*torch.cos(a).item(), 0.5 + 0.5*torch.sin(a).item()])\n",
    "        self.verts_uvs = torch.tensor(uvs, dtype=torch.float32, device=self.device)\n",
    "        self.faces_uvs = self.faces.clone()\n",
    "        self.mesh_loaded = True\n",
    "        \n",
    "    def sample_viewpoint(self):\n",
    "        #Elevation: beta distribution skewed toward overhead\n",
    "        elev_norm = np.random.beta(3, 1)\n",
    "        elev = CONFIG['elevation_range'][0] + elev_norm * (CONFIG['elevation_range'][1] - CONFIG['elevation_range'][0])\n",
    "        #Azimuth: uniform\n",
    "        azim = np.random.uniform(0, 360)\n",
    "        #Scale: uniform\n",
    "        scale = np.random.uniform(*CONFIG['scale_range'])\n",
    "        return elev, azim, scale\n",
    "    \n",
    "    def render(self, texture, elevation=90, azimuth=0, scale=1.0):\n",
    "        batch_size = texture.shape[0]\n",
    "        #Scale vertices\n",
    "        verts = self.verts * scale\n",
    "        #Camera setup\n",
    "        dist = 2.5  #Camera distance\n",
    "        R, T = look_at_view_transform(dist=dist, elev=elevation, azim=azimuth, device=self.device)\n",
    "        cameras = FoVPerspectiveCameras(R=R, T=T, device=self.device)\n",
    "        #Lighting (varying lighting)\n",
    "        light_x = np.random.uniform(-1, 1)\n",
    "        light_y = np.random.uniform(1, 3)  #Always somewhat above\n",
    "        light_z = np.random.uniform(-1, 1)\n",
    "        lights = PointLights(device=self.device, location=[[light_x, light_y, light_z]],ambient_color=[[0.5, 0.5, 0.5]],diffuse_color=[[0.3, 0.3, 0.3]],specular_color=[[0.2, 0.2, 0.2]])\n",
    "        \n",
    "        rendered_images = []\n",
    "        alpha_masks = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            #Create texture for this sample\n",
    "            tex = TexturesUV(maps=texture[i:i+1].permute(0, 2, 3, 1), faces_uvs=[self.faces_uvs], verts_uvs=[self.verts_uvs])\n",
    "            #Create mesh\n",
    "            mesh = Meshes(verts=[verts], faces=[self.faces], textures=tex)\n",
    "            #Renderer\n",
    "            renderer = MeshRenderer(rasterizer=MeshRasterizer(cameras=cameras, raster_settings=self.raster_settings), shader=SoftPhongShader(device=self.device, cameras=cameras, lights=lights))\n",
    "            #Render\n",
    "            images = renderer(mesh)\n",
    "            rendered_images.append(images[..., :3].permute(0, 3, 1, 2))\n",
    "            alpha_masks.append(images[..., 3:4].permute(0, 3, 1, 2))\n",
    "            \n",
    "        return torch.cat(rendered_images, dim=0), torch.cat(alpha_masks, dim=0)\n",
    "\n",
    "#Test renderer\n",
    "renderer = HatRenderer(CONFIG['mesh_path'], CONFIG['render_size'], device)\n",
    "test_render, test_alpha = renderer.render(test_texture, elevation=85, azimuth=45)\n",
    "logger.info(f\"Rendered shape: {test_render.shape}, alpha shape: {test_alpha.shape}\")\n",
    "\n",
    "#Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(test_texture[0].permute(1,2,0).detach().cpu())\n",
    "axes[0].set_title('Texture')\n",
    "axes[1].imshow(test_render[0].permute(1,2,0).detach().cpu())\n",
    "axes[1].set_title('Rendered Hat')\n",
    "axes[2].imshow(test_alpha[0, 0].detach().cpu(), cmap='gray')\n",
    "axes[2].set_title('Alpha Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a086a",
   "metadata": {},
   "source": [
    "## 7: T-SEA Augmentations\n",
    "\n",
    "Helper methods for black/gray/white box transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly mask a region of the rendered hat. Prevents overfitting to specific texture patterns.\n",
    "def patch_cutout(rendered_hat, alpha_mask, prob=0.9, ratio=0.4, fill=0.5):\n",
    "    if np.random.random() > prob:\n",
    "        return rendered_hat\n",
    "    B, C, H, W = rendered_hat.shape\n",
    "    #Random cutout size\n",
    "    cut_h = int(H * ratio)\n",
    "    cut_w = int(W * ratio)\n",
    "    #Random position\n",
    "    top = np.random.randint(0, H - cut_h + 1)\n",
    "    left = np.random.randint(0, W - cut_w + 1)\n",
    "    #Apply cutout (only where alpha > 0)\n",
    "    mask = alpha_mask.clone()\n",
    "    mask[:, :, top:top+cut_h, left:left+cut_w] = 0\n",
    "    rendered_hat = rendered_hat * mask + fill * (1 - mask) * (alpha_mask > 0).float()\n",
    "    return rendered_hat\n",
    "\n",
    "#Mild augmentations that don't distort the image too much.\n",
    "def constrained_augmentation(image):\n",
    "    B, C, H, W = image.shape\n",
    "    #Random scale (0.9 - 1.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    new_size = int(H * scale)\n",
    "    image = F.interpolate(image, size=new_size, mode='bilinear', align_corners=False)\n",
    "    #Crop/pad back to original size\n",
    "    if new_size > H:\n",
    "        start = (new_size - H) // 2\n",
    "        image = image[:, :, start:start+H, start:start+W]\n",
    "    else:\n",
    "        pad = (H - new_size) // 2\n",
    "        image = F.pad(image, [pad, pad, pad, pad], mode='reflect')\n",
    "        image = image[:, :, :H, :W]\n",
    "    #Color jitter (mild)\n",
    "    brightness = np.random.uniform(0.9, 1.1)\n",
    "    image = image * brightness\n",
    "    #Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        image = torch.flip(image, dims=[3])\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#ShakeDrop reates virtual ensemble of model variants\n",
    "def shakedrop_forward(model, x, drop_prob=0.5, alpha_range=(0, 2)):\n",
    "    #I will make a simplified version: add noise to intermediate features\n",
    "    if np.random.random() < drop_prob:\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = torch.randn_like(x) * 0.1 * alpha\n",
    "        x = x + noise\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02ef7e",
   "metadata": {},
   "source": [
    "## 8: URAdv Augmentations\n",
    "\n",
    "For better performance under drone conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66890419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add simulated light reflections on the hat surface.\n",
    "def add_light_spots(image, alpha_mask, num_range=(0, 3), intensity_range=(0.1, 0.4)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_spots = np.random.randint(*num_range)\n",
    "    for _ in range(num_spots):\n",
    "        #Random spot position (within hat region)\n",
    "        cy = np.random.randint(H // 4, 3 * H // 4)\n",
    "        cx = np.random.randint(W // 4, 3 * W // 4)\n",
    "        #Spot parameters\n",
    "        radius = np.random.uniform(0.05, 0.15) * min(H, W)\n",
    "        intensity = np.random.uniform(*intensity_range)\n",
    "        #Create Gaussian spot\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "        dist = ((x - cx) ** 2 + (y - cy) ** 2).float()\n",
    "        spot = torch.exp(-dist / (2 * radius ** 2)) * intensity\n",
    "        #Apply only within hat (where alpha > 0)\n",
    "        spot = spot.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image + spot\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Add simulated shadows on the hat surface.\n",
    "def add_shadows(image, alpha_mask, num_range=(0, 2), opacity_range=(0.2, 0.5)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_shadows = np.random.randint(*num_range)\n",
    "    for _ in range(num_shadows):\n",
    "        #Random shadow as diagonal stripe\n",
    "        angle = np.random.uniform(0, np.pi)\n",
    "        opacity = np.random.uniform(*opacity_range)\n",
    "        width = np.random.uniform(0.1, 0.3) * min(H, W)\n",
    "        #Create shadow mask\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "        offset = np.random.uniform(0, H)\n",
    "        dist = torch.abs(x * np.cos(angle) + y * np.sin(angle) - offset)\n",
    "        shadow = (dist < width).float() * opacity\n",
    "        #Apply only within hat\n",
    "        shadow = shadow.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image * (1 - shadow)\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Simulate printer color/brightness variation.\n",
    "def simulate_printing(texture, mul_std=0.1, add_std=0.05):\n",
    "    #Multiplicative noise\n",
    "    mul_noise = torch.randn_like(texture) * mul_std + 1.0\n",
    "    texture = texture * mul_noise\n",
    "    #Additive noise\n",
    "    add_noise = torch.randn_like(texture) * add_std\n",
    "    texture = texture + add_noise\n",
    "    return texture.clamp(0, 1)\n",
    "\n",
    "#Apply camera artifacts: blur, noise.\n",
    "def apply_environmental_augmentation(image, prob=0.3):\n",
    "    #Motion blur\n",
    "    if np.random.random() < prob:\n",
    "        kernel_size = np.random.choice([3, 5, 7])\n",
    "        kernel = torch.zeros(kernel_size, kernel_size, device=image.device)\n",
    "        kernel[kernel_size//2, :] = 1.0 / kernel_size\n",
    "        #Random rotation of kernel\n",
    "        angle = np.random.uniform(0, 360)\n",
    "        # Simplified: just apply horizontal blur\n",
    "        image = F.conv2d(image, kernel.view(1, 1, kernel_size, kernel_size).expand(3, 1, -1, -1), padding=kernel_size//2, groups=3)\n",
    "    #Gaussian noise\n",
    "    if np.random.random() < prob:\n",
    "        noise_std = np.random.uniform(0.01, 0.05)\n",
    "        image = image + torch.randn_like(image) * noise_std\n",
    "    return image.clamp(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
