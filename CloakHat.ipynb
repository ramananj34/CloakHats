{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acba844-1e05-4948-82a5-43edc05a995e",
   "metadata": {},
   "source": [
    "What is left to make it work: \n",
    "* 3D scan hat meshes, put in /assets/hat.obj\n",
    "* Get drone images for /data/drone_footage (with annotations.json)\n",
    "* Get printer specs from FabLab for config\n",
    "* Calibrate drone scale in annotations.json metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b43cd",
   "metadata": {},
   "source": [
    "# CloakHat Patch Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed60",
   "metadata": {},
   "source": [
    "## 1: Conda Setup\n",
    "\n",
    "* Conda is the main env manager, pip is for Python packages\n",
    "* PyTorch is the main AI/ML library\n",
    "* NVIDIA CUDA is for GPU acceleration\n",
    "* PyTorch3D is for rendering the hat\n",
    "* ipykernel allows JupyterLab to use the Conda env\n",
    "* Ultralytics has YOLO models\n",
    "* opencv-python-headless is for image processing\n",
    "* matplotlib is for plots\n",
    "* tqdm is for progress bars\n",
    "* NumPy is for data manipulation\n",
    "\n",
    "### Option 1:\n",
    "\n",
    "Activate <br>\n",
    "`conda env create -f environment.yaml` <br>\n",
    "`conda activate cloakhat` <br>\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "### Option 2: \n",
    "\n",
    "Set up the environment\n",
    "\n",
    "`conda create -n cloakhat python=3.10 -y` <br>\n",
    "`conda activate cloakhat`\n",
    "\n",
    "PyTorch with CUDA. Also ipykernel. <br>\n",
    "`conda install pytorch ipykernel pytorch-cuda=11.8 -c pytorch -c nvidia -y`<br>\n",
    "\n",
    "PyTorch3D for differentiable rendering <br>\n",
    "`conda install -c pytorch3d pytorch3d -y`\n",
    "\n",
    "Detection models <br>\n",
    "`pip install ultralytics`\n",
    "\n",
    "Other stuff <br>\n",
    "`pip install opencv-python-headless matplotlib tqdm numpy`\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "### Option 3: \n",
    "\n",
    "Run the bash <br>\n",
    "`bash LabSetup.sh`\n",
    "\n",
    "Activate the kernel <br>\n",
    "`conda activate cloakhat` <br>\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc07834",
   "metadata": {},
   "source": [
    "## 2: Python Setup\n",
    "\n",
    "Get the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346291c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/jovyan/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 18:26:34,989 | Device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Deep learning stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Data manipulation\n",
    "import numpy as np\n",
    "\n",
    "#Image processing\n",
    "import cv2\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Working with the file system\n",
    "from pathlib import Path\n",
    "\n",
    "#Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Better logging than print statements\n",
    "import logging\n",
    "\n",
    "#JSON utilities\n",
    "import json\n",
    "\n",
    "#PyTorch3d utilities\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (look_at_view_transform, FoVPerspectiveCameras, RasterizationSettings, MeshRasterizer, SoftPhongShader, TexturesUV, PointLights)\n",
    "\n",
    "#Gets YOLO models\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#Logging with timestamps\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Check what device is being used (especially if we want GPU) and log it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c892",
   "metadata": {},
   "source": [
    "## 3: Config\n",
    "\n",
    "Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_dir': './data/drone_footage', #Drone footage/image samples\n",
    "    'mesh_path': './assets/hat.obj', #Hat mesh\n",
    "    'output_dir': './outputs', #Where out outputs will go (textures, evaluations, stuff like that)\n",
    "    \n",
    "    #Generator\n",
    "    'latent_channels': 128, #Channels\n",
    "    'latent_size': 9, #Spatial size of latent input\n",
    "    #(so the latent is 128x9x9)\n",
    "    'texture_size': 288, #Output texture size from generator\n",
    "    #(so the latent becomes a texture that is 3x288x288)\n",
    "    \n",
    "    #Viewpoint sampling\n",
    "    'scale_jitter': 0.1, #Fraction of scale variation\n",
    "    'camera_pitch_jitter': 5.0, #Alias for elevation jitter (degrees)\n",
    "    'heading_jitter': 10.0, #Alias for azimuth jitter (degrees)\n",
    "\n",
    "    \n",
    "    'num_workers': 8, #DataLoader\n",
    "    'det_conf_floor': 0.001, #Minimum confidence for detector loss\n",
    "    \n",
    "    #Training Stage 1\n",
    "    'stage1_epochs': 100, #100 epochs\n",
    "    'stage1_batch_size': 8, #8 batch minibatch gradient descent\n",
    "    'stage1_lr': 2e-4, #learning rate\n",
    "    \n",
    "    #Training Stage 2  \n",
    "    'stage2_iterations': 2000, #Now we optimize the single tensor\n",
    "    'stage2_lr': 0.01, #Bigger learning rate\n",
    "    'local_latent_size': 18, #Size of optimizable latent pattern. Bigger than 9x9 (input), so tile seamlessly\n",
    "    \n",
    "    #Loss weights\n",
    "    'lambda_tv': 2.5, #Total variation - makes the textures smoother/less noisy/able to be printed\n",
    "    'lambda_nps': 0.01, #Non-printability score - penalize colors that can't print well\n",
    "    'lambda_info': 0.1, #Mutual information (Stage 1 only) - ensures latent is correlated to the texture\n",
    "    \n",
    "    #T-SEA Stuff\n",
    "    'cutout_prob': 0.9, #90% of the time, cut off 40% of the hat\n",
    "    'cutout_ratio': 0.4,\n",
    "    'shakedrop_prob': 0.5, #50% of the time, mess with the \n",
    "    \n",
    "    #Rendering\n",
    "    'render_size': 256, #Output 256x256 images\n",
    "    \n",
    "    #Printer specifications (GET FROM FABLAB)\n",
    "    'printer': {\n",
    "        'dpi': 300,\n",
    "        'physical_size_inches': (8, 8),\n",
    "        'max_saturation': 0.85,\n",
    "        'max_brightness': 0.95,\n",
    "        'min_brightness': 0.08,\n",
    "        'nps_threshold': 0.7, #Saturation * brightness threshold (penalize when saturation × brightness > 0.7)\n",
    "        'gamut_samples_path': './assets/printer_gamut.npy',\n",
    "    },\n",
    "    \n",
    "    #Attack config (white, gray, black)\n",
    "    'attack_mode': 'gray',\n",
    "}\n",
    "\n",
    "#Calculate output size\n",
    "CONFIG['texture_output_size'] = int(CONFIG['printer']['dpi'] * CONFIG['printer']['physical_size_inches'][0])\n",
    "\n",
    "#Make sure the folder exists\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ae201-d285-4de7-9313-dea9d81753c3",
   "metadata": {},
   "source": [
    "## 4: Dataset Preparation\n",
    "\n",
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ae3ab-ca07-4d00-83ec-92db0313ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_dir/\n",
    "        frames/\n",
    "            frame_0001.png\n",
    "            frame_0002.png\n",
    "            ...\n",
    "        masks/\n",
    "            frame_0001_mask.png  (binary mask of green hat region)\n",
    "            ...\n",
    "        annotations.json  (person bounding boxes, metadata)\n",
    "\n",
    "Annotations.json:\n",
    "{\n",
    "    \"frames\": [\n",
    "        {\n",
    "            \"frame_id\": \"frame_0001\",\n",
    "            \"image_path\": \"frames/frame_0001.png\",\n",
    "            \"mask_path\": \"masks/frame_0001_mask.png\",\n",
    "            \"person_bbox\": [x1, y1, x2, y2],\n",
    "            \"viewpoint\": {\n",
    "                \"elevation\": 82.5,\n",
    "                \"azimuth\": 45.0,\n",
    "                \"altitude_meters\": 15.0\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"altitude_to_scale\": {\n",
    "            \"min_altitude\": 5.0,\n",
    "            \"max_altitude\": 50.0,\n",
    "            \"min_scale\": 0.3,\n",
    "            \"max_scale\": 1.2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "class DroneDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir, transform=None):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not self.dataset_dir.exists():\n",
    "            logger.warning(f\"Dataset directory not found: {dataset_dir}\")\n",
    "            logger.warning(\"Using placeholder data for testing.\")\n",
    "            self.use_placeholder = True\n",
    "            self.length = 100\n",
    "            return\n",
    "        \n",
    "        # Load annotations\n",
    "        annotations_path = self.dataset_dir / 'annotations.json'\n",
    "        if not annotations_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing annotations.json in {dataset_dir}\")\n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        self.frames = self.annotations['frames']\n",
    "        self.length = len(self.frames)\n",
    "        self.use_placeholder = False\n",
    "        \n",
    "        # Altitude to scale conversion params\n",
    "        meta = self.annotations['metadata']['altitude_to_scale']\n",
    "        self.alt_min = meta['min_altitude']\n",
    "        self.alt_max = meta['max_altitude']\n",
    "        self.scale_min = meta['min_scale']\n",
    "        self.scale_max = meta['max_scale']\n",
    "        \n",
    "        logger.info(f\"Loaded {self.length} frames from {dataset_dir}\")\n",
    "    \n",
    "    def _altitude_to_scale(self, altitude_meters):\n",
    "        \"\"\"Higher altitude = smaller hat (farther away).\"\"\"\n",
    "        t = (altitude_meters - self.alt_min) / (self.alt_max - self.alt_min)\n",
    "        t = np.clip(t, 0, 1)\n",
    "        scale = self.scale_max - t * (self.scale_max - self.scale_min)\n",
    "        return scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_placeholder:\n",
    "            #Placeholder with fake viewpoint data\n",
    "            hat_half_size = 50\n",
    "            person_pad_y, person_pad_x, person_pad_bottom = 150, 100, 200\n",
    "            \n",
    "            cy = np.random.randint(person_pad_y + hat_half_size, 1080 - person_pad_bottom - hat_half_size)\n",
    "            cx = np.random.randint(person_pad_x + hat_half_size, 1920 - person_pad_x - hat_half_size)\n",
    "            \n",
    "            image = torch.rand(3, 1080, 1920)\n",
    "            hat_mask = torch.zeros(1, 1080, 1920)\n",
    "            hat_mask[:, cy-hat_half_size:cy+hat_half_size, cx-hat_half_size:cx+hat_half_size] = 1.0\n",
    "            \n",
    "            person_bbox = torch.tensor([\n",
    "                cx - person_pad_x, cy - person_pad_y,\n",
    "                cx + person_pad_x, cy + person_pad_bottom\n",
    "            ], dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'hat_mask': hat_mask,\n",
    "                'person_bbox': person_bbox,\n",
    "                'elevation': torch.tensor(np.random.uniform(60, 90), dtype=torch.float32),\n",
    "                'azimuth': torch.tensor(np.random.uniform(0, 360), dtype=torch.float32),\n",
    "                'scale': torch.tensor(np.random.uniform(0.3, 1.2), dtype=torch.float32),\n",
    "            }\n",
    "        \n",
    "        # Real data\n",
    "        frame = self.frames[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image_path = self.dataset_dir / frame['image_path']\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        # Load mask\n",
    "        mask_path = self.dataset_dir / frame['mask_path']\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float() / 255.0\n",
    "        \n",
    "        # Bounding box\n",
    "        person_bbox = torch.tensor(frame['person_bbox'], dtype=torch.float32)\n",
    "        \n",
    "        # Viewpoint\n",
    "        drone = frame['drone']\n",
    "        elevation = drone['camera_pitch']\n",
    "        azimuth = drone.get('heading', np.random.uniform(0, 360))  # Randomize if not provided\n",
    "        scale = self._altitude_to_scale(drone['altitude_meters'])\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'hat_mask': mask,\n",
    "            'person_bbox': person_bbox,\n",
    "            'elevation': torch.tensor(elevation, dtype=torch.float32),\n",
    "            'azimuth': torch.tensor(azimuth, dtype=torch.float32),\n",
    "            'scale': torch.tensor(scale, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "\n",
    "def segment_green_hat(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    #Green range in HSV\n",
    "    lower_green = np.array([35, 100, 100])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    \n",
    "    #Clean up mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "#Create dataset (will use placeholder if data doesn't exist)\n",
    "dataset = DroneDataset(CONFIG['dataset_dir'])\n",
    "logger.info(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1445b0",
   "metadata": {},
   "source": [
    "## 5: FCN Generator\n",
    "\n",
    "Make the texture (turn noise into an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            #9 -> 9\n",
    "            nn.Conv2d(latent_channels, 512, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            #9 -> 18\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #18 -> 36\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 36 -> 72\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 72 -> 144\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 144 -> 288\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 288 -> 288 (to RGB)\n",
    "            nn.Conv2d(32, 3, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_size = 288\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "    \n",
    "    def generate(self, z=None, batch_size=1):\n",
    "        if z is None:\n",
    "            z = torch.randn(batch_size, 128, 9, 9, device=next(self.parameters()).device)\n",
    "        return (self.forward(z) + 1) / 2\n",
    "\n",
    "#Test\n",
    "generator = FCNGenerator().to(device)\n",
    "test_texture = generator.generate(batch_size=1)\n",
    "assert test_texture.shape[-1] == CONFIG['texture_size'], f\"Generator outputs {test_texture.shape[-1]}px but config expects {CONFIG['texture_size']}px\"\n",
    "logger.info(f\"Generator output: {test_texture.shape}\")  #Should be (1, 3, 288, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec701a8",
   "metadata": {},
   "source": [
    "## 6: Auxiliary Network\n",
    "\n",
    "Forces the texture to derive from the latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryNetwork(nn.Module):\n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Texture encoder\n",
    "        self.tex_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Latent encoder\n",
    "        self.lat_enc = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 256, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Joint network\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texture, z):\n",
    "        tex_feat = self.tex_enc(texture)\n",
    "        lat_feat = self.lat_enc(z)\n",
    "        return self.joint(torch.cat([tex_feat, lat_feat], dim=1))\n",
    "\n",
    "\n",
    "def compute_mi_loss(aux_net, texture, z):\n",
    "\n",
    "    #Matched pairs\n",
    "    T_joint = aux_net(texture, z)\n",
    "    pos_term = -F.softplus(-T_joint).mean()\n",
    "    \n",
    "    #Mismatched pairs (shuffle z)\n",
    "    z_shuffle = z[torch.randperm(z.size(0))]\n",
    "    T_marginal = aux_net(texture, z_shuffle)\n",
    "    neg_term = F.softplus(T_marginal).mean()\n",
    "    \n",
    "    mi = pos_term - neg_term\n",
    "    return -mi  #Negate because we minimize loss but want to maximize MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb40bb",
   "metadata": {},
   "source": [
    "## 7: Render Hat\n",
    "\n",
    "Render the hat using the texture and capture angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatRenderer:\n",
    "    def __init__(self, mesh_path, render_size=256, device='cuda'):\n",
    "        self.device = device\n",
    "        self.render_size = render_size\n",
    "        \n",
    "        #Load mesh\n",
    "        self.mesh_loaded = False\n",
    "        if Path(mesh_path).exists():\n",
    "            verts, faces, aux = load_obj(mesh_path, device=device)\n",
    "            self.verts = verts\n",
    "            self.faces = faces.verts_idx\n",
    "            self.verts_uvs = aux.verts_uvs\n",
    "            self.faces_uvs = faces.textures_idx\n",
    "            self.mesh_loaded = True\n",
    "            logger.info(f\"Loaded mesh: {len(verts)} verts, {len(self.faces)} faces\")\n",
    "        else:\n",
    "            logger.warning(f\"Mesh not found at {mesh_path}. Using placeholder.\")\n",
    "            self._create_placeholder_mesh()\n",
    "            \n",
    "        #Rasterization settings\n",
    "        self.raster_settings = RasterizationSettings(\n",
    "            image_size=render_size, \n",
    "            blur_radius=0.0, \n",
    "            faces_per_pixel=1\n",
    "        )\n",
    "        \n",
    "        #Create rasterizer once\n",
    "        self.rasterizer = MeshRasterizer(raster_settings=self.raster_settings)\n",
    "    \n",
    "    def _create_placeholder_mesh(self):\n",
    "        #Simple disk\n",
    "        n_points = 32\n",
    "        angles = torch.linspace(0, 2*np.pi, n_points+1)[:-1]\n",
    "        #Vertices: center + rim\n",
    "        verts = [[0, 0, 0]]  # center\n",
    "        for a in angles:\n",
    "            verts.append([torch.cos(a).item(), torch.sin(a).item(), 0])\n",
    "        self.verts = torch.tensor(verts, dtype=torch.float32, device=self.device)\n",
    "        #Faces: triangles from center to rim\n",
    "        faces = []\n",
    "        for i in range(n_points):\n",
    "            faces.append([0, i+1, (i % n_points) + 2 if i < n_points-1 else 1])\n",
    "        self.faces = torch.tensor(faces, dtype=torch.int64, device=self.device)\n",
    "        #UVs: simple radial mapping\n",
    "        uvs = [[0.5, 0.5]]  # center\n",
    "        for a in angles:\n",
    "            uvs.append([0.5 + 0.5*torch.cos(a).item(), 0.5 + 0.5*torch.sin(a).item()])\n",
    "        self.verts_uvs = torch.tensor(uvs, dtype=torch.float32, device=self.device)\n",
    "        self.faces_uvs = self.faces.clone()\n",
    "        self.mesh_loaded = True\n",
    "    \n",
    "    def render(self, texture, elevation=90, azimuth=0, scale=1.0):\n",
    "        batch_size = texture.shape[0]\n",
    "        \n",
    "        #Scale vertices\n",
    "        verts = self.verts * scale\n",
    "        \n",
    "        #Camera setup\n",
    "        dist = 2.5  #Camera distance\n",
    "        R, T = look_at_view_transform(dist=dist, elev=elevation, azim=azimuth, device=self.device)\n",
    "        cameras = FoVPerspectiveCameras(R=R, T=T, device=self.device)\n",
    "        \n",
    "        #Lighting (varying lighting)\n",
    "        light_x = np.random.uniform(-1, 1)\n",
    "        light_y = np.random.uniform(1, 3)  #Always somewhat above\n",
    "        light_z = np.random.uniform(-1, 1)\n",
    "        lights = PointLights(\n",
    "            device=self.device, \n",
    "            location=[[light_x, light_y, light_z]],\n",
    "            ambient_color=[[0.5, 0.5, 0.5]],\n",
    "            diffuse_color=[[0.3, 0.3, 0.3]],\n",
    "            specular_color=[[0.2, 0.2, 0.2]]\n",
    "        )\n",
    "        \n",
    "        #Create shader once per render call (lighting changes)\n",
    "        shader = SoftPhongShader(device=self.device, cameras=cameras, lights=lights)\n",
    "        \n",
    "        #Create batched texture\n",
    "        tex_maps = texture.permute(0, 2, 3, 1)  # (B, H, W, 3)\n",
    "        textures = TexturesUV(\n",
    "            maps=tex_maps,\n",
    "            faces_uvs=[self.faces_uvs] * batch_size,\n",
    "            verts_uvs=[self.verts_uvs] * batch_size\n",
    "        )\n",
    "        \n",
    "        #Create batched mesh\n",
    "        meshes = Meshes(\n",
    "            verts=[verts] * batch_size,\n",
    "            faces=[self.faces] * batch_size,\n",
    "            textures=textures\n",
    "        )\n",
    "        \n",
    "        #Render entire batch at once\n",
    "        fragments = self.rasterizer(meshes, cameras=cameras)\n",
    "        images = shader(fragments, meshes, cameras=cameras, lights=lights)\n",
    "        \n",
    "        rendered_images = images[..., :3].permute(0, 3, 1, 2)\n",
    "        alpha_masks = images[..., 3:4].permute(0, 3, 1, 2)\n",
    "            \n",
    "        return rendered_images, alpha_masks\n",
    "\n",
    "#Test renderer\n",
    "renderer = HatRenderer(CONFIG['mesh_path'], CONFIG['render_size'], device)\n",
    "test_render, test_alpha = renderer.render(test_texture, elevation=85, azimuth=45)\n",
    "logger.info(f\"Rendered shape: {test_render.shape}, alpha shape: {test_alpha.shape}\")\n",
    "\n",
    "#Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(test_texture[0].permute(1,2,0).detach().cpu())\n",
    "axes[0].set_title('Texture')\n",
    "axes[1].imshow(test_render[0].permute(1,2,0).detach().cpu())\n",
    "axes[1].set_title('Rendered Hat')\n",
    "axes[2].imshow(test_alpha[0, 0].detach().cpu(), cmap='gray')\n",
    "axes[2].set_title('Alpha Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a086a",
   "metadata": {},
   "source": [
    "## 8: T-SEA Augmentations\n",
    "\n",
    "Helper methods for black/gray/white box transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly mask a region of the rendered hat. Prevents overfitting to specific texture patterns.\n",
    "def patch_cutout(rendered_hat, alpha_mask, prob=0.9, ratio=0.4, fill=0.5):\n",
    "    if np.random.random() > prob:\n",
    "        return rendered_hat\n",
    "    B, C, H, W = rendered_hat.shape\n",
    "    #Random cutout size\n",
    "    cut_h = int(H * ratio)\n",
    "    cut_w = int(W * ratio)\n",
    "    #Random position\n",
    "    top = np.random.randint(0, H - cut_h + 1)\n",
    "    left = np.random.randint(0, W - cut_w + 1)\n",
    "    #Apply cutout (only where alpha > 0)\n",
    "    mask = alpha_mask.clone()\n",
    "    mask[:, :, top:top+cut_h, left:left+cut_w] = 0\n",
    "    rendered_hat = rendered_hat * mask + fill * (1 - mask) * (alpha_mask > 0).float()\n",
    "    return rendered_hat\n",
    "\n",
    "#Mild augmentations that don't distort the image too much.\n",
    "def constrained_augmentation(image):\n",
    "    B, C, H, W = image.shape\n",
    "    #Random scale (0.9 - 1.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    new_size = int(H * scale)\n",
    "    image = F.interpolate(image, size=new_size, mode='bilinear', align_corners=False)\n",
    "    #Crop/pad back to original size\n",
    "    if new_size > H:\n",
    "        start = (new_size - H) // 2\n",
    "        image = image[:, :, start:start+H, start:start+W]\n",
    "    else:\n",
    "        pad = (H - new_size) // 2\n",
    "        image = F.pad(image, [pad, pad, pad, pad], mode='reflect')\n",
    "        image = image[:, :, :H, :W]\n",
    "    #Color jitter (mild)\n",
    "    brightness = np.random.uniform(0.9, 1.1)\n",
    "    image = image * brightness\n",
    "    #Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        image = torch.flip(image, dims=[3])\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#ShakeDrop reates virtual ensemble of model variants\n",
    "def shakedrop_forward(model, x, drop_prob=0.5, alpha_range=(0, 2)):\n",
    "    #I will make a simplified version: add noise to intermediate features\n",
    "    if np.random.random() < drop_prob:\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = torch.randn_like(x) * 0.1 * alpha\n",
    "        x = x + noise\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02ef7e",
   "metadata": {},
   "source": [
    "## 9: URAdv Augmentations\n",
    "\n",
    "For better performance under drone conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66890419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add simulated light reflections on the hat surface.\n",
    "def add_light_spots(image, alpha_mask, num_range=(0, 3), intensity_range=(0.1, 0.4)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_spots = np.random.randint(*num_range)\n",
    "    for _ in range(num_spots):\n",
    "        #Random spot position (within hat region)\n",
    "        cy = np.random.randint(H // 4, 3 * H // 4)\n",
    "        cx = np.random.randint(W // 4, 3 * W // 4)\n",
    "        #Spot parameters\n",
    "        radius = np.random.uniform(0.05, 0.15) * min(H, W)\n",
    "        intensity = np.random.uniform(*intensity_range)\n",
    "        #Create Gaussian spot\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "        dist = ((x - cx) ** 2 + (y - cy) ** 2).float()\n",
    "        spot = torch.exp(-dist / (2 * radius ** 2)) * intensity\n",
    "        #Apply only within hat (where alpha > 0)\n",
    "        spot = spot.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image + spot\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Add simulated shadows on the hat surface.\n",
    "def add_shadows(image, alpha_mask, num_range=(0, 2), opacity_range=(0.2, 0.5)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_shadows = np.random.randint(*num_range)\n",
    "    for _ in range(num_shadows):\n",
    "        #Random shadow as diagonal stripe\n",
    "        angle = np.random.uniform(0, np.pi)\n",
    "        opacity = np.random.uniform(*opacity_range)\n",
    "        width = np.random.uniform(0.1, 0.3) * min(H, W)\n",
    "        #Create shadow mask\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "        offset = np.random.uniform(0, H)\n",
    "        dist = torch.abs(x * np.cos(angle) + y * np.sin(angle) - offset)\n",
    "        shadow = (dist < width).float() * opacity\n",
    "        #Apply only within hat\n",
    "        shadow = shadow.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image * (1 - shadow)\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Simulate printer color/brightness variation.\n",
    "def simulate_printing(texture, mul_std=0.1, add_std=0.05):\n",
    "    #Multiplicative noise\n",
    "    mul_noise = torch.randn_like(texture) * mul_std + 1.0\n",
    "    texture = texture * mul_noise\n",
    "    #Additive noise\n",
    "    add_noise = torch.randn_like(texture) * add_std\n",
    "    texture = texture + add_noise\n",
    "    return texture.clamp(0, 1\n",
    "\n",
    "class PrinterGamut:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config['printer']\n",
    "        \n",
    "        gamut_path = self.config.get('gamut_samples_path')\n",
    "        if gamut_path and Path(gamut_path).exists():\n",
    "            self.gamut_samples = torch.from_numpy(np.load(gamut_path)).float().to(device)\n",
    "            self.use_measured_gamut = True\n",
    "            logger.info(f\"Loaded {len(self.gamut_samples)} gamut samples\")\n",
    "        else:\n",
    "            self.use_measured_gamut = False\n",
    "            logger.info(\"Using simplified gamut constraints\")\n",
    "    \n",
    "    def nps_loss(self, texture):\n",
    "        max_ch = texture.max(dim=1)[0]\n",
    "        min_ch = texture.min(dim=1)[0]\n",
    "        saturation = (max_ch - min_ch) / (max_ch + 1e-8)\n",
    "        brightness = max_ch\n",
    "        \n",
    "        loss = 0.0\n",
    "        \n",
    "        #Saturation * brightness threshold\n",
    "        loss = loss + F.relu(saturation * brightness - self.config['nps_threshold']).mean()\n",
    "        \n",
    "        #Saturation cap\n",
    "        loss = loss + F.relu(saturation - self.config['max_saturation']).mean()\n",
    "        \n",
    "        #Brightness bounds\n",
    "        loss = loss + F.relu(brightness - self.config['max_brightness']).mean()\n",
    "        loss = loss + F.relu(self.config['min_brightness'] - brightness).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def clamp_to_gamut(self, texture):\n",
    "        return texture.clamp(self.config['min_brightness'], self.config['max_brightness'])\n",
    "\n",
    "\n",
    "#Initialize globally\n",
    "printer_gamut = PrinterGamut(CONFIG)\n",
    "\n",
    "#Apply camera artifacts: blur, noise.\n",
    "def apply_environmental_augmentation(image, prob=0.3):\n",
    "    #Motion blur\n",
    "    if np.random.random() < prob:\n",
    "        kernel_size = np.random.choice([3, 5, 7])\n",
    "        kernel = torch.zeros(kernel_size, kernel_size, device=image.device)\n",
    "        kernel[kernel_size//2, :] = 1.0 / kernel_size\n",
    "        #Random rotation of kernel\n",
    "        angle = np.random.uniform(0, 360)\n",
    "        # Simplified: just apply horizontal blur\n",
    "        image = F.conv2d(image, kernel.view(1, 1, kernel_size, kernel_size).expand(3, 1, -1, -1), padding=kernel_size//2, groups=3)\n",
    "    #Gaussian noise\n",
    "    if np.random.random() < prob:\n",
    "        noise_std = np.random.uniform(0.01, 0.05)\n",
    "        image = image + torch.randn_like(image) * noise_std\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "def apply_viewpoint_jitter(elevation, azimuth, scale, config):\n",
    "    elev = elevation + np.random.uniform(-config['camera_pitch_jitter'], config['camera_pitch_jitter'])\n",
    "    elev = np.clip(elev, 0, 90)\n",
    "    \n",
    "    azim = azimuth + np.random.uniform(-config['heading_jitter'], config['heading_jitter'])\n",
    "    azim = azim % 360\n",
    "    \n",
    "    scl = scale * (1 + np.random.uniform(-config['scale_jitter'], config['scale_jitter']))\n",
    "    scl = np.clip(scl, 0.1, 2.0)\n",
    "    \n",
    "    return elev, azim, scl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296071",
   "metadata": {},
   "source": [
    "## 10: Toroidal Cropping\n",
    "\n",
    "Wrapping the texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToroidalLatent(nn.Module):\n",
    "    \n",
    "    def __init__(self, local_size, crop_size=9, latent_channels=128, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.local_size = local_size\n",
    "        self.crop_size = crop_size\n",
    "        self.latent_channels = latent_channels\n",
    "        \n",
    "        #Initialize local latent pattern as registered parameter\n",
    "        self.z_local = nn.Parameter(\n",
    "            torch.randn(1, latent_channels, local_size, local_size) * 0.1\n",
    "        )\n",
    "        \n",
    "        #Move to device\n",
    "        self.to(device)\n",
    "        \n",
    "    def get_random_crops(self, batch_size):\n",
    "        #Tile 3x3 for wraparound\n",
    "        z_tiled = self.z_local.repeat(1, 1, 3, 3)\n",
    "        \n",
    "        crops = []\n",
    "        for _ in range(batch_size):\n",
    "            #Random offset within middle tile (to enable wraparound)\n",
    "            i = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            j = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            crop = z_tiled[:, :, i:i+self.crop_size, j:j+self.crop_size]\n",
    "            crops.append(crop)\n",
    "            \n",
    "        return torch.cat(crops, dim=0)\n",
    "    \n",
    "    def get_full_latent(self, target_spatial_size):\n",
    "        reps = (target_spatial_size + self.local_size - 1) // self.local_size + 1\n",
    "        z_tiled = self.z_local.repeat(1, 1, reps, reps)\n",
    "        return z_tiled[:, :, :target_spatial_size, :target_spatial_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fd5cd-3724-44f2-ab9f-f1d289d211a8",
   "metadata": {},
   "source": [
    "## 11: Sceen Composition\n",
    "\n",
    "Render the sceen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3c7d8-1f11-40fd-9e03-3d34c614c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_hat_on_scene(scene_image, hat_mask, rendered_hat, alpha_mask):\n",
    "\n",
    "    B, C, H, W = scene_image.shape\n",
    "    \n",
    "    #For each image in batch, place hat at mask location\n",
    "    composited = scene_image.clone()\n",
    "    \n",
    "    for i in range(B):\n",
    "        #Find bounding box of hat mask\n",
    "        mask = hat_mask[i, 0]\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        ys, xs = torch.where(mask > 0.5)\n",
    "        y1, y2 = ys.min().item(), ys.max().item()\n",
    "        x1, x2 = xs.min().item(), xs.max().item()\n",
    "        \n",
    "        hat_h = y2 - y1\n",
    "        hat_w = x2 - x1\n",
    "        \n",
    "        #Resize rendered hat to fit\n",
    "        hat_resized = F.interpolate(rendered_hat[i:i+1], size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        alpha_resized = F.interpolate(alpha_mask[i:i+1], size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        #Composite\n",
    "        region = composited[i:i+1, :, y1:y2, x1:x2]\n",
    "        composited[i:i+1, :, y1:y2, x1:x2] = (hat_resized * alpha_resized + region * (1 - alpha_resized))\n",
    "        \n",
    "    return composited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64a18b-3226-42bf-80fc-0de9e91e51a5",
   "metadata": {},
   "source": [
    "## 12: Ensamble\n",
    "\n",
    "Ensamble detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e065e4-b4d7-41a4-95e3-7148c56e3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorEnsemble:\n",
    "    \n",
    "    def __init__(self, attack_mode='gray', device='cuda', conf_floor=0.001):\n",
    "        self.device = device\n",
    "        self.conf_floor = conf_floor\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        \n",
    "        if attack_mode == 'white':\n",
    "            self.models['yolov8m'] = YOLO('yolov8m.pt')\n",
    "            self.weights['yolov8m'] = 1.0\n",
    "            \n",
    "        elif attack_mode == 'gray':\n",
    "            model_configs = [\n",
    "                ('yolov8s', 0.20),\n",
    "                ('yolov8m', 0.25),\n",
    "                ('yolov8l', 0.20),\n",
    "                ('yolov5m', 0.20),\n",
    "                ('yolov5l', 0.15),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.weights[name] = weight\n",
    "                    logger.info(f\"Loaded {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\")\n",
    "                    \n",
    "        elif attack_mode == 'black':\n",
    "            #Add More\n",
    "            model_configs = [\n",
    "                ('yolov8m', 0.30),\n",
    "                ('yolov8l', 0.25),\n",
    "                ('yolov5l', 0.25),\n",
    "                ('yolov5m', 0.20),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.weights[name] = weight\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\")\n",
    "                    \n",
    "        #Normalize weights\n",
    "        total = sum(self.weights.values())\n",
    "        self.weights = {k: v/total for k, v in self.weights.items()}\n",
    "        \n",
    "        logger.info(f\"Detector ensemble ({attack_mode}): {list(self.weights.keys())}\")\n",
    "        \n",
    "    def compute_loss(self, images, return_detections=False):\n",
    "        total_loss = 0.0\n",
    "        all_detections = [] if return_detections else None\n",
    "        \n",
    "        #Convert to uint8 numpy for YOLO\n",
    "        images_np = (images * 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            weight = self.weights[name]\n",
    "            \n",
    "            #Run detection with low confidence to get gradients\n",
    "            results = model.predict(images_np, conf=self.conf_floor, classes=[0], verbose=False)\n",
    "            \n",
    "            #Collect person detection confidences\n",
    "            batch_confs = []\n",
    "            for r in results:\n",
    "                if len(r.boxes) > 0:\n",
    "                    confs = r.boxes.conf.to(self.device)\n",
    "                    batch_confs.append(confs)\n",
    "                    \n",
    "            if batch_confs:\n",
    "                #Loss = mean of top-k confidences per image\n",
    "                all_confs = torch.cat(batch_confs)\n",
    "                k = min(10, len(all_confs))\n",
    "                top_confs, _ = torch.topk(all_confs, k)\n",
    "                loss = top_confs.mean()\n",
    "                total_loss = total_loss + weight * loss\n",
    "                \n",
    "            if return_detections:\n",
    "                all_detections.append({\n",
    "                    'model': name,\n",
    "                    'results': results\n",
    "                })\n",
    "                \n",
    "        if return_detections:\n",
    "            return total_loss, all_detections\n",
    "        return total_loss\n",
    "    \n",
    "    def detect(self, images, conf_threshold=0.5):\n",
    "        images_np = (images * 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        all_results = {}\n",
    "        for name, model in self.models.items():\n",
    "            results = model.predict(images_np, conf=self.conf_floor, classes=[0], verbose=False)\n",
    "            all_results[name] = []\n",
    "            for r in results:\n",
    "                all_results[name].append({\n",
    "                    'boxes': r.boxes.xyxy.cpu().numpy() if len(r.boxes) > 0 else np.array([]),\n",
    "                    'scores': r.boxes.conf.cpu().numpy() if len(r.boxes) > 0 else np.array([]),\n",
    "                })\n",
    "        return all_results\n",
    "\n",
    "\n",
    "#Initialize detector ensemble\n",
    "detector = DetectorEnsemble(CONFIG['attack_mode'], device, conf_floor=CONFIG['det_conf_floor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d5947-45e8-4fae-9c2a-8241df2d548a",
   "metadata": {},
   "source": [
    "## 13: Loss Calculation\n",
    "\n",
    "Custom Loss Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a4efc-b63c-4ba9-97fb-d29ea0fe120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to get smooth textures\n",
    "def total_variation_loss(texture):\n",
    "    diff_h = texture[:, :, 1:, :] - texture[:, :, :-1, :]\n",
    "    diff_w = texture[:, :, :, 1:] - texture[:, :, :, :-1]\n",
    "    return (diff_h.pow(2).mean() + diff_w.pow(2).mean()) / 2\n",
    "\n",
    "#Try to get printable colors\n",
    "def nps_loss(texture, threshold=0.7):\n",
    "    #Compute saturation and brightness\n",
    "    max_ch = texture.max(dim=1)[0]\n",
    "    min_ch = texture.min(dim=1)[0]\n",
    "    saturation = (max_ch - min_ch) / (max_ch + 1e-8)\n",
    "    brightness = max_ch\n",
    "    \n",
    "    #Penalize when saturation * brightness > threshold\n",
    "    penalty = F.relu(saturation * brightness - threshold)\n",
    "    return penalty.mean()\n",
    "\n",
    "#Everything together\n",
    "def compute_total_loss(texture, detector, config, stage='stage2'):\n",
    "    loss_det = detector.compute_loss(texture)\n",
    "    loss_tv = total_variation_loss(texture)\n",
    "    loss_nps = printer_gamut.nps_loss(texture)  # Use new class\n",
    "    \n",
    "    total = loss_det + config['lambda_tv'] * loss_tv + config['lambda_nps'] * loss_nps\n",
    "    \n",
    "    return total, {\n",
    "        'total': total.item() if isinstance(total, torch.Tensor) else total,\n",
    "        'detection': loss_det.item() if isinstance(loss_det, torch.Tensor) else loss_det,\n",
    "        'tv': loss_tv.item(),\n",
    "        'nps': loss_nps.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cf65e-225e-43cf-82f7-0390b5b60c12",
   "metadata": {},
   "source": [
    "## 14: Stage 1: Generator Training\n",
    "\n",
    "Train the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e1d11-926e-4fb4-a190-166106a0c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(generator, aux_net, detector, dataset, config):\n",
    "\n",
    "    generator.train()\n",
    "    aux_net.train()\n",
    "    \n",
    "    #Optimizers\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    opt_aux = torch.optim.Adam(aux_net.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    \n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=config['stage1_batch_size'], shuffle=True,\n",
    "        num_workers=config['num_workers'], pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    #Training loop\n",
    "    logger.info(\"Starting Stage 1 training...\")\n",
    "    \n",
    "    for epoch in range(config['stage1_epochs']):\n",
    "        epoch_losses = {'total': 0, 'detection': 0, 'tv': 0, 'nps': 0, 'mi': 0}\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config['stage1_epochs']}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            #Sample latent\n",
    "            z = torch.randn(config['stage1_batch_size'], 128, 9, 9, device=device)\n",
    "            \n",
    "            #Generate texture\n",
    "            texture = generator.generate(z)  # [0, 1]\n",
    "            \n",
    "            #Sample viewpoint and render\n",
    "            rendered_hats = []\n",
    "            alphas = []\n",
    "            for i in range(texture.shape[0]):\n",
    "                elev, azim, scl = apply_viewpoint_jitter(\n",
    "                    batch['elevation'][i].item(),\n",
    "                    batch['azimuth'][i].item(),\n",
    "                    batch['scale'][i].item(),\n",
    "                    CONFIG\n",
    "                )\n",
    "                rh, al = renderer.render(texture[i:i+1], elev, azim, scl)\n",
    "                rendered_hats.append(rh)\n",
    "                alphas.append(al)\n",
    "            \n",
    "            rendered_hat = torch.cat(rendered_hats, dim=0)\n",
    "            alpha = torch.cat(alphas, dim=0)\n",
    "            \n",
    "            #Apply T-SEA augmentations\n",
    "            rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "            \n",
    "            #Apply URAdv augmentations\n",
    "            rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "            rendered_hat = add_shadows(rendered_hat, alpha)\n",
    "            rendered_hat = simulate_printing(rendered_hat)\n",
    "            \n",
    "            #Composite onto scene (using placeholder or real data)\n",
    "            if not dataset.use_placeholder:\n",
    "                scene = batch['image'].to(device)\n",
    "                hat_mask = batch['hat_mask'].to(device)\n",
    "                composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            else:\n",
    "                #For placeholder, just use rendered hat directly\n",
    "                composite = rendered_hat\n",
    "                \n",
    "            #Apply environmental augmentations\n",
    "            composite = constrained_augmentation(composite)\n",
    "            composite = apply_environmental_augmentation(composite)\n",
    "\n",
    "            #ShakeDrop: perturb composite to create virtual ensemble variants\n",
    "            composite = shakedrop_forward(composite, drop_prob=config['shakedrop_prob'])\n",
    "            \n",
    "            #Compute losses\n",
    "            #Detection loss\n",
    "            loss_det = detector.compute_loss(composite)\n",
    "            \n",
    "            #Regularization losses\n",
    "            loss_tv = total_variation_loss(texture)\n",
    "            loss_nps = nps_loss(texture, config['printer']['nps_threshold'])\n",
    "            \n",
    "            #Mutual information (maximize = negate for min)\n",
    "            loss_mi = compute_mi_loss(aux_net, texture, z)\n",
    "            \n",
    "            #Total loss\n",
    "            loss = (loss_det + \n",
    "                   config['lambda_tv'] * loss_tv + \n",
    "                   config['lambda_nps'] * loss_nps + \n",
    "                   config['lambda_info'] * loss_mi)  # loss_mi is already negated\n",
    "            \n",
    "            #Optimize\n",
    "            opt_g.zero_grad()\n",
    "            opt_aux.zero_grad()\n",
    "            \n",
    "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(aux_net.parameters(), 1.0)\n",
    "                opt_g.step()\n",
    "                opt_aux.step()\n",
    "            \n",
    "            #Track losses\n",
    "            epoch_losses['total'] += loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "            epoch_losses['detection'] += loss_det.item() if isinstance(loss_det, torch.Tensor) else loss_det\n",
    "            epoch_losses['tv'] += loss_tv.item()\n",
    "            epoch_losses['nps'] += loss_nps.item()\n",
    "            epoch_losses['mi'] += loss_mi.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\" if isinstance(loss, torch.Tensor) else f\"{loss:.4f}\",\n",
    "                'det': f\"{loss_det.item():.4f}\" if isinstance(loss_det, torch.Tensor) else f\"{loss_det:.4f}\"\n",
    "            })\n",
    "            \n",
    "        #Epoch summary\n",
    "        n_batches = len(dataloader)\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= n_batches\n",
    "            \n",
    "        logger.info(f\"Epoch {epoch+1} - Loss: {epoch_losses['total']:.4f}, \"\n",
    "                   f\"Det: {epoch_losses['detection']:.4f}, \"\n",
    "                   f\"TV: {epoch_losses['tv']:.4f}, \"\n",
    "                   f\"MI: {epoch_losses['mi']:.4f}\")\n",
    "        \n",
    "        #Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'aux_net': aux_net.state_dict(),\n",
    "                'epoch': epoch,\n",
    "            }, f\"{config['output_dir']}/stage1_epoch{epoch+1}.pth\")\n",
    "            \n",
    "            #Save sample texture\n",
    "            with torch.no_grad():\n",
    "                sample = generator.generate(batch_size=1)\n",
    "                save_texture(sample[0], f\"{config['output_dir']}/texture_epoch{epoch+1}.png\")\n",
    "                \n",
    "    return generator\n",
    "\n",
    "\n",
    "def save_texture(texture, path):\n",
    "    \"\"\"Save texture for preview.\"\"\"\n",
    "    if isinstance(texture, torch.Tensor):\n",
    "        texture = texture.detach().cpu()\n",
    "        if texture.dim() == 3:\n",
    "            texture = texture.permute(1, 2, 0).numpy()\n",
    "        texture = (texture * 255).astype(np.uint8)\n",
    "    cv2.imwrite(str(path), cv2.cvtColor(texture, cv2.COLOR_RGB2BGR))\n",
    "    logger.info(f\"Saved texture to {path}\")\n",
    "\n",
    "\n",
    "def save_final_texture(texture, config, path):\n",
    "    \"\"\"Save print-ready texture at correct DPI.\"\"\"\n",
    "    from PIL import Image\n",
    "    \n",
    "    output_size = config['texture_output_size']\n",
    "    \n",
    "    # Resize to print resolution\n",
    "    texture_highres = F.interpolate(\n",
    "        texture, \n",
    "        size=(output_size, output_size), \n",
    "        mode='bilinear', \n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    # Clamp to printable gamut\n",
    "    texture_highres = printer_gamut.clamp_to_gamut(texture_highres)\n",
    "    \n",
    "    # Convert to uint8\n",
    "    texture_np = texture_highres[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    texture_np = (texture_np * 255).astype(np.uint8)\n",
    "    \n",
    "    img = Image.fromarray(texture_np)\n",
    "    \n",
    "    # Save TIFF with DPI metadata (for print)\n",
    "    tiff_path = Path(path).with_suffix('.tiff')\n",
    "    img.save(tiff_path, dpi=(config['printer']['dpi'], config['printer']['dpi']))\n",
    "    \n",
    "    # Save PNG for preview\n",
    "    img.save(path)\n",
    "    \n",
    "    logger.info(f\"Saved: {path} and {tiff_path}\")\n",
    "    logger.info(f\"  {output_size}x{output_size}px at {config['printer']['dpi']} DPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db60a2-2d8a-4248-9b01-9df76cb4185d",
   "metadata": {},
   "source": [
    "## 15: Stage 2: Latent Optimization\n",
    "\n",
    "Optimize the Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c3ed1-8530-48e9-9a72-aae0cbbd04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(generator, detector, dataset, config, z_local=None):\n",
    "\n",
    "    generator.eval()  #Freeze generator\n",
    "    \n",
    "    #Initialize toroidal latent\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=9,\n",
    "        latent_channels=128,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    if z_local is not None:\n",
    "        toroidal.z_local.data = z_local\n",
    "        \n",
    "    #Optimizer for latent only\n",
    "    optimizer = torch.optim.Adam(toroidal.parameters(), lr=config['stage2_lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config['stage2_iterations'], eta_min=1e-4\n",
    "    )\n",
    "    \n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=config['stage1_batch_size'], shuffle=True,\n",
    "        num_workers=config['num_workers'], pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    logger.info(\"Starting Stage 2 latent optimization...\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(config['stage2_iterations']), desc=\"Stage 2\")\n",
    "    for iteration in pbar:\n",
    "        #Get batch (cycle through dataset)\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            batch = next(data_iter)\n",
    "            \n",
    "        #Get random crops from toroidal latent\n",
    "        z_crops = toroidal.get_random_crops(config['stage1_batch_size'])\n",
    "            \n",
    "        #We need gradients through generator for z\n",
    "        texture = generator.generate(z_crops)\n",
    "        \n",
    "        #Sample viewpoints and render\n",
    "        rendered_hats = []\n",
    "        alphas = []\n",
    "        for i in range(config['stage1_batch_size']):\n",
    "            elev, azim, scl = apply_viewpoint_jitter(\n",
    "                batch['elevation'][i].item(),\n",
    "                batch['azimuth'][i].item(),\n",
    "                batch['scale'][i].item(),\n",
    "                config\n",
    "            )\n",
    "            rh, al = renderer.render(texture[i:i+1], elev, azim, scl)\n",
    "            rendered_hats.append(rh)\n",
    "            alphas.append(al)\n",
    "        rendered_hat = torch.cat(rendered_hats, dim=0)\n",
    "        alpha = torch.cat(alphas, dim=0)\n",
    "        \n",
    "        #Apply augmentations\n",
    "        rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "        rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "        rendered_hat = simulate_printing(rendered_hat)\n",
    "        \n",
    "        #Composite (placeholder mode)\n",
    "        composite = constrained_augmentation(rendered_hat)\n",
    "        composite = apply_environmental_augmentation(composite)\n",
    "\n",
    "        #ShakeDrop: virtual ensemble\n",
    "        composite = shakedrop_forward(composite, drop_prob=config['shakedrop_prob'])\n",
    "        \n",
    "        #Compute loss (no MI term in Stage 2)\n",
    "        loss_det = detector.compute_loss(composite)\n",
    "        loss_tv = total_variation_loss(texture)\n",
    "        loss_nps = nps_loss(texture, config['printer']['nps_threshold'])\n",
    "        \n",
    "        loss = (loss_det + \n",
    "               config['lambda_tv'] * loss_tv + \n",
    "               config['lambda_nps'] * loss_nps)\n",
    "        \n",
    "        #Optimize\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        #Track best\n",
    "        loss_val = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_z_local = toroidal.z_local.data.clone()\n",
    "            \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss_val:.4f}\",\n",
    "            'best': f\"{best_loss:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
    "        })\n",
    "        \n",
    "        #Periodic logging\n",
    "        if (iteration + 1) % 200 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Generate final texture at full resolution\n",
    "                z_full = toroidal.get_full_latent(config['latent_size'])\n",
    "                final_texture = generator.generate(z_full)\n",
    "                save_texture(final_texture[0], \n",
    "                           f\"{config['output_dir']}/texture_stage2_iter{iteration+1}.png\")\n",
    "                \n",
    "    #Save final results\n",
    "    logger.info(f\"Stage 2 complete. Best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    #Generate final texture\n",
    "    with torch.no_grad():\n",
    "        z_full = toroidal.get_full_latent(config['latent_size'])\n",
    "        final_texture = generator.generate(z_full)\n",
    "        \n",
    "        # Resize to target output resolution\n",
    "        def tile_texture(tex, target):\n",
    "            _, _, h, w = tex.shape\n",
    "            reps = (target + h - 1) // h\n",
    "            tiled = tex.repeat(1, 1, reps, reps)\n",
    "            return tiled[:, :, :target, :target]\n",
    "        \n",
    "        final_texture = tile_texture(final_texture, 1024)\n",
    "        \n",
    "    save_final_texture(final_texture, config, f\"{config['output_dir']}/final_texture.png\")\n",
    "    \n",
    "    torch.save({\n",
    "        'z_local': best_z_local,\n",
    "        'generator': generator.state_dict(),\n",
    "    }, f\"{config['output_dir']}/stage2_final.pth\")\n",
    "    \n",
    "    return best_z_local, final_texture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203957cc-502f-45f4-94ad-5c4c9320c6d7",
   "metadata": {},
   "source": [
    "## 16: Evaluation\n",
    "\n",
    "See how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13817e3-c57f-4511-a7a6-733797dabb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asr(detector, images, gt_boxes, conf_threshold=0.5, iou_threshold=0.5):\n",
    "\n",
    "    results = detector.detect(images, conf_threshold)\n",
    "    \n",
    "    n_success = 0\n",
    "    for model_name, model_results in results.items():\n",
    "        for i, det in enumerate(model_results):\n",
    "            gt_box = gt_boxes[i].cpu().numpy()\n",
    "            \n",
    "            detected = False\n",
    "            for box, score in zip(det['boxes'], det['scores']):\n",
    "                #Compute IoU\n",
    "                x1 = max(box[0], gt_box[0])\n",
    "                y1 = max(box[1], gt_box[1])\n",
    "                x2 = min(box[2], gt_box[2])\n",
    "                y2 = min(box[3], gt_box[3])\n",
    "                \n",
    "                inter = max(0, x2-x1) * max(0, y2-y1)\n",
    "                area1 = (box[2]-box[0]) * (box[3]-box[1])\n",
    "                area2 = (gt_box[2]-gt_box[0]) * (gt_box[3]-gt_box[1])\n",
    "                iou = inter / (area1 + area2 - inter + 1e-8)\n",
    "                \n",
    "                if iou >= iou_threshold:\n",
    "                    detected = True\n",
    "                    break\n",
    "                    \n",
    "            if not detected:\n",
    "                n_success += 1\n",
    "                \n",
    "    #Average across models\n",
    "    total = len(results) * len(gt_boxes)\n",
    "    return n_success / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_texture(generator, detector, dataset, z_local, config, num_samples=100):\n",
    "    generator.eval()\n",
    "    \n",
    "    #Create toroidal latent with optimized pattern\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=9,\n",
    "        device=device\n",
    "    )\n",
    "    toroidal.z_local.data = z_local\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=8, shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=True\n",
    "    )\n",
    "    \n",
    "    asr_scores = {thresh: [] for thresh in [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            #Generate texture\n",
    "            z_crops = toroidal.get_random_crops(8)\n",
    "            texture = generator.generate(z_crops)\n",
    "            \n",
    "            #Render and composite\n",
    "            rendered_hat, alpha = renderer.render(texture, 85, 0, 1.0)\n",
    "            \n",
    "            #For placeholder, use rendered directly\n",
    "            composite = rendered_hat\n",
    "            \n",
    "            # Test at multiple thresholds\n",
    "            gt_boxes = batch.get('person_bbox', torch.zeros(8, 4))\n",
    "            for thresh in asr_scores.keys():\n",
    "                asr = compute_asr(detector, composite, gt_boxes, conf_threshold=thresh)\n",
    "                asr_scores[thresh].append(asr)\n",
    "                \n",
    "    #Compute mean ASR\n",
    "    mean_asr = {}\n",
    "    for thresh, scores in asr_scores.items():\n",
    "        mean_asr[thresh] = np.mean(scores)\n",
    "        logger.info(f\"ASR@{thresh}: {mean_asr[thresh]:.2%}\")\n",
    "        \n",
    "    overall_masr = np.mean(list(mean_asr.values()))\n",
    "    logger.info(f\"Mean ASR: {overall_masr:.2%}\")\n",
    "    \n",
    "    return mean_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cbc12-7460-42f4-a62e-3076cd91fdd0",
   "metadata": {},
   "source": [
    "## 17: Main\n",
    "\n",
    "Bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62395d-f498-4292-9c3b-49ffa50c131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Initialize models\n",
    "    logger.info(\"Initializing models...\")\n",
    "    \n",
    "    generator = FCNGenerator(latent_channels=CONFIG['latent_channels']).to(device)\n",
    "    aux_net = AuxiliaryNetwork().to(device)\n",
    "    \n",
    "    logger.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    logger.info(f\"Auxiliary params: {sum(p.numel() for p in aux_net.parameters()):,}\")\n",
    "    \n",
    "    #Stage 1: Train generator\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 1: Generator Training\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    generator = train_stage1(generator, aux_net, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Stage 2: Optimize latent\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 2: Latent Optimization\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    best_z_local, final_texture = train_stage2(generator, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Evaluation\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"EVALUATION\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    asr_results = evaluate_texture(generator, detector, dataset, best_z_local, CONFIG)\n",
    "    \n",
    "    #Final visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Final texture\n",
    "    axes[0].imshow(final_texture[0].permute(1, 2, 0).cpu())\n",
    "    axes[0].set_title('Final Adversarial Texture')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    #Rendered examples\n",
    "    with torch.no_grad():\n",
    "        rendered, alpha = renderer.render(final_texture, 85, 45, 1.0)\n",
    "    axes[1].imshow(rendered[0].permute(1, 2, 0).cpu())\n",
    "    axes[1].set_title('Rendered Hat (85°, 45°)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    #ASR plot\n",
    "    thresholds = list(asr_results.keys())\n",
    "    values = [asr_results[t] for t in thresholds]\n",
    "    axes[2].bar(range(len(thresholds)), values)\n",
    "    axes[2].set_xticks(range(len(thresholds)))\n",
    "    axes[2].set_xticklabels([f'{t}' for t in thresholds])\n",
    "    axes[2].set_xlabel('Confidence Threshold')\n",
    "    axes[2].set_ylabel('Attack Success Rate')\n",
    "    axes[2].set_title('ASR vs Threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/final_results.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"Training complete!\")\n",
    "    logger.info(f\"Final texture saved to: {CONFIG['output_dir']}/final_texture.png\")\n",
    "    \n",
    "    return generator, best_z_local, final_texture\n",
    "\n",
    "\n",
    "#Run if this is the main notebook\n",
    "if __name__ == \"__main__\":\n",
    "    generator, z_local, texture = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cloakhat)",
   "language": "python",
   "name": "cloakhat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
