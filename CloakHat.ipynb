{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acba844-1e05-4948-82a5-43edc05a995e",
   "metadata": {},
   "source": [
    "What is left to make it work: \n",
    "* 3D scan hat for hat.obj\n",
    "* Print green fabric and put it on the hat\n",
    "* Fly drone, capture footage of person wearing green hat\n",
    "* Pre-process: run segment_green_hat to generate masks, annotate person_bbox and drone metadata into annotations.json\n",
    "* Calibrate altitude_to_scale in annotations metadata\n",
    "* Measure UV unwrapped hat, set mesh_unit_to_inches in CONFIG\n",
    "* Get printer specs from FabLab → set DPI and gamut values in CONFIG\n",
    "* Add more diverse models to blackbox: RT-DETR, YOLO-NAS, or a Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b43cd",
   "metadata": {},
   "source": [
    "# CloakHat Patch Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed60",
   "metadata": {},
   "source": [
    "## 1: Conda Setup\n",
    "\n",
    "* Conda is the main env manager, pip is for Python packages\n",
    "* PyTorch is the main AI/ML library\n",
    "* NVIDIA CUDA is for GPU acceleration\n",
    "* PyTorch3D is for rendering the hat\n",
    "* ipykernel allows JupyterLab to use the Conda env\n",
    "* Ultralytics has YOLO models\n",
    "* opencv-python-headless is for image processing\n",
    "* matplotlib is for plots\n",
    "* tqdm is for progress bars\n",
    "* NumPy is for data manipulation\n",
    "\n",
    "### Option 1:\n",
    "\n",
    "Activate <br>\n",
    "`conda env create -f environment.yaml` <br>\n",
    "`conda activate cloakhat` <br>\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "### Option 2: \n",
    "\n",
    "Set up the environment\n",
    "\n",
    "`conda create -n cloakhat python=3.10 -y` <br>\n",
    "`conda activate cloakhat`\n",
    "\n",
    "PyTorch with CUDA. Also ipykernel. <br>\n",
    "`conda install pytorch ipykernel pytorch-cuda=11.8 -c pytorch -c nvidia -y`<br>\n",
    "\n",
    "PyTorch3D for differentiable rendering <br>\n",
    "`conda install -c pytorch3d pytorch3d -y`\n",
    "\n",
    "Detection models <br>\n",
    "`pip install ultralytics`\n",
    "\n",
    "Other stuff <br>\n",
    "`pip install opencv-python-headless matplotlib tqdm numpy`\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "### Option 3: \n",
    "\n",
    "Run the bash <br>\n",
    "`bash LabSetup.sh`\n",
    "\n",
    "Activate the kernel <br>\n",
    "`conda activate cloakhat` <br>\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc07834",
   "metadata": {},
   "source": [
    "## 2: Python Setup\n",
    "\n",
    "Get the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346291c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Data manipulation\n",
    "import numpy as np\n",
    "\n",
    "#Image processing\n",
    "import cv2\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Working with the file system\n",
    "from pathlib import Path\n",
    "\n",
    "#Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Better logging than print statements\n",
    "import logging\n",
    "\n",
    "#JSON utilities\n",
    "import json\n",
    "\n",
    "#PyTorch3d utilities\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (look_at_view_transform, FoVPerspectiveCameras, RasterizationSettings, MeshRasterizer, SoftPhongShader, TexturesUV, PointLights)\n",
    "\n",
    "#Gets YOLO models\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#Logging with timestamps\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Check what device is being used (especially if we want GPU) and log it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c892",
   "metadata": {},
   "source": [
    "## 3: Config\n",
    "\n",
    "Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_dir': './data/drone_footage', #Drone footage/image samples\n",
    "    'mesh_path': './assets/hat.obj', #Hat mesh\n",
    "    'output_dir': './outputs', #Where out outputs will go (textures, evaluations, stuff like that)\n",
    "    \n",
    "    #Generator\n",
    "    'latent_channels': 128, #Channels\n",
    "    'latent_size': 9, #Spatial size of latent input\n",
    "    #(so the latent is 128x9x9)\n",
    "    'texture_size': 288, #Output texture size from generator\n",
    "    #(so the latent becomes a texture that is 3x288x288)\n",
    "    \n",
    "    #Viewpoint sampling\n",
    "    'scale_jitter': 0.1, #Fraction of scale variation\n",
    "    'camera_pitch_jitter': 5.0, #Alias for elevation jitter (degrees)\n",
    "    'heading_jitter': 10.0, #Alias for azimuth jitter (degrees)\n",
    "\n",
    "    'num_workers': 8, #for DataLoader\n",
    "    'det_conf_floor': 0.001, #Minimum confidence for detector loss\n",
    "    \n",
    "    #Training Stage 1\n",
    "    'stage1_epochs': 100, #100 epochs\n",
    "    'stage1_batch_size': 8, #8 batch minibatch gradient descent\n",
    "    'stage1_lr': 2e-4, #learning rate\n",
    "    \n",
    "    #Training Stage 2  \n",
    "    'stage2_iterations': 2000, #Now we optimize the single tensor\n",
    "    'stage2_lr': 0.01, #Bigger learning rate\n",
    "    'local_latent_size': 18, #Size of optimizable latent pattern. Bigger than 9x9 (input), so tile seamlessly\n",
    "    \n",
    "    #Loss weights\n",
    "    'lambda_tv': 2.5, #Total variation - makes the textures smoother/less noisy/able to be printed\n",
    "    'lambda_nps': 0.01, #Non-printability score - penalize colors that can't print well\n",
    "    'lambda_info': 0.1, #Mutual information (Stage 1 only) - ensures latent is correlated to the texture\n",
    "    \n",
    "    #T-SEA Stuff\n",
    "    'cutout_prob': 0.9, #90% of the time, \n",
    "    'cutout_ratio': 0.4, #cut off 40% of the hat\n",
    "    'shakedrop_prob': 0.5, #50% of the time, mess with the model (for self-ensemble)\n",
    "    \n",
    "    #Rendering\n",
    "    'render_size': 256, #Output 256x256 images\n",
    "    'image_size': 640, #Standardized input image size for all frames\n",
    "    \n",
    "    #Printer specifications (GET FROM FABLAB)\n",
    "    'printer': {\n",
    "        'dpi': 300,\n",
    "        'mesh_unit_to_inches': None, #CALIBRATE: how many inches = 1 unit in the .obj file. None to auto-estimate.\n",
    "        'seam_allowance_inches': 0.25, #Extra border for cutting\n",
    "        'max_saturation': 0.85,\n",
    "        'max_brightness': 0.95,\n",
    "        'min_brightness': 0.08,\n",
    "        'nps_threshold': 0.7, #Saturation * brightness threshold (penalize when saturation × brightness > 0.7)\n",
    "        'gamut_samples_path': './assets/printer_gamut.npy',\n",
    "    },\n",
    "    \n",
    "    #Attack config (white, gray, black)\n",
    "    'attack_mode': 'gray',\n",
    "}\n",
    "\n",
    "#Make sure the folder exists\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ae201-d285-4de7-9313-dea9d81753c3",
   "metadata": {},
   "source": [
    "## 4: Dataset Preparation\n",
    "\n",
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ae3ab-ca07-4d00-83ec-92db0313ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset will look like\n",
    "\"\"\"\n",
    "dataset_dir/\n",
    "        frames/\n",
    "            frame_0001.png\n",
    "            frame_0002.png\n",
    "            ...\n",
    "        masks/\n",
    "            frame_0001_mask.png  (binary mask of green hat region)\n",
    "            ...\n",
    "        annotations.json  (person bounding boxes, metadata)\n",
    "\n",
    "Annotations.json:\n",
    "{\n",
    "    \"frames\": [\n",
    "        {\n",
    "            \"frame_id\": \"frame_0001\",\n",
    "            \"image_path\": \"frames/frame_0001.png\",\n",
    "            \"mask_path\": \"masks/frame_0001_mask.png\",\n",
    "            \"person_bbox\": [x1, y1, x2, y2],\n",
    "            \"drone\": {\n",
    "                \"camera_pitch\": 82.5,\n",
    "                \"heading\": 45.0,\n",
    "                \"altitude_meters\": 15.0\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"altitude_to_scale\": {\n",
    "            \"min_altitude\": 5.0,\n",
    "            \"max_altitude\": 50.0,\n",
    "            \"min_scale\": 0.3,\n",
    "            \"max_scale\": 1.2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "#Dataset of the drone images\n",
    "class DroneDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    #Loads annotations\n",
    "    def __init__(self, dataset_dir):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        #Makes sure placeholder exists\n",
    "        if not self.dataset_dir.exists():\n",
    "            raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n",
    "        #Load annotations\n",
    "        annotations_path = self.dataset_dir / 'annotations.json'\n",
    "        if not annotations_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing annotations.json in {dataset_dir}\")\n",
    "        #Load the JSON\n",
    "        with open(annotations_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        #Get the lengths/number of samples\n",
    "        self.frames = self.annotations['frames']\n",
    "        self.length = len(self.frames)\n",
    "        #Altitude to scale conversion params\n",
    "        meta = self.annotations['metadata']['altitude_to_scale']\n",
    "        self.alt_min = meta['min_altitude']\n",
    "        self.alt_max = meta['max_altitude']\n",
    "        self.scale_min = meta['min_scale']\n",
    "        self.scale_max = meta['max_scale']\n",
    "        logger.info(f\"Loaded {self.length} frames from {dataset_dir}\")\n",
    "\n",
    "    #Convert altitutde to scale - needs to be tuned\n",
    "    def _altitude_to_scale(self, altitude_meters):\n",
    "        t = (altitude_meters - self.alt_min) / (self.alt_max - self.alt_min)\n",
    "        t = np.clip(t, 0, 1)\n",
    "        scale = self.scale_max - t * (self.scale_max - self.scale_min)\n",
    "        return scale\n",
    "\n",
    "    #Gets the number of images\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    #Get a sample\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        #Load image\n",
    "        image_path = self.dataset_dir / frame['image_path']\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "        target_size = CONFIG['image_size']\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        image = F.interpolate(image.unsqueeze(0), size=(target_size, target_size), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        #Load mask\n",
    "        mask_path = self.dataset_dir / frame['mask_path']\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float() / 255.0\n",
    "        mask = F.interpolate(mask.unsqueeze(0), size=(target_size, target_size), mode='nearest').squeeze(0)\n",
    "        #Bounding box (scaled to match resized image)\n",
    "        person_bbox = torch.tensor(frame['person_bbox'], dtype=torch.float32)\n",
    "        person_bbox[0] *= target_size / orig_w\n",
    "        person_bbox[1] *= target_size / orig_h\n",
    "        person_bbox[2] *= target_size / orig_w\n",
    "        person_bbox[3] *= target_size / orig_h\n",
    "        #Viewpoint\n",
    "        drone = frame['drone']\n",
    "        elevation = drone['camera_pitch']\n",
    "        azimuth = drone.get('heading', np.random.uniform(0, 360))  # Randomize if not provided\n",
    "        scale = self._altitude_to_scale(drone['altitude_meters'])\n",
    "        return {\n",
    "            'image': image,\n",
    "            'hat_mask': mask,\n",
    "            'person_bbox': person_bbox,\n",
    "            'elevation': torch.tensor(elevation, dtype=torch.float32),\n",
    "            'azimuth': torch.tensor(azimuth, dtype=torch.float32),\n",
    "            'scale': torch.tensor(scale, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "#Pre-processing utility\n",
    "def segment_green_hat(frame):\n",
    "    #Convert to HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    #Narrow lime chroma green range in HSV\n",
    "    lower_green = np.array([50, 150, 150])\n",
    "    upper_green = np.array([70, 255, 255])\n",
    "    #Calculate the mask\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    #Clean up mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    #Dilate to recover edges lost to shadows/curves\n",
    "    dilate_kernel = np.ones((15, 15), np.uint8)\n",
    "    mask = cv2.dilate(mask, dilate_kernel, iterations=1)\n",
    "    return mask\n",
    "\n",
    "#Create dataset\n",
    "dataset = DroneDataset(CONFIG['dataset_dir'])\n",
    "logger.info(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1445b0",
   "metadata": {},
   "source": [
    "## 5: FCN Generator\n",
    "\n",
    "Make the texture (turn noise into an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_channels=128, latent_size=9):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        #Core network, upsample throughout, zero padding for invaraince, and LeakyReLU activation\n",
    "        self.net = nn.Sequential(\n",
    "            #9 -> 9\n",
    "            nn.Conv2d(latent_channels, 512, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            #9 -> 18\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #18 -> 36\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #36 -> 72\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #72 -> 144\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #144 -> 288\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #288 -> 288 (to RGB)\n",
    "            nn.Conv2d(32, 3, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.Tanh() #Squash\n",
    "        )\n",
    "        self.output_size = 288\n",
    "        self._init_weights()\n",
    "\n",
    "    #Initialize the weights\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    #Standard forward method\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "    #Create random noise for testing\n",
    "    def generate(self, z=None, batch_size=1):\n",
    "        if z is None:\n",
    "            z = torch.randn(batch_size, self.net[0].in_channels, self.latent_size, self.latent_size, device=next(self.parameters()).device)\n",
    "        return (self.forward(z) + 1) / 2\n",
    "\n",
    "#Test\n",
    "generator = FCNGenerator(latent_channels=CONFIG['latent_channels'], latent_size=CONFIG['latent_size']).to(device)\n",
    "test_texture = generator.generate(batch_size=1)\n",
    "#Make sure it is the correct size\n",
    "assert test_texture.shape[-1] == CONFIG['texture_size'], f\"Generator outputs {test_texture.shape[-1]}px but config expects {CONFIG['texture_size']}px\"\n",
    "logger.info(f\"Generator output: {test_texture.shape}\")  #Should be (1, 3, 288, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec701a8",
   "metadata": {},
   "source": [
    "## 6: Auxiliary Network\n",
    "\n",
    "Forces the texture to derive from the latent (JSD MINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Texture encoder\n",
    "        self.tex_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Latent encoder\n",
    "        self.lat_enc = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Joint network\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texture, z):\n",
    "        tex_feat = self.tex_enc(texture)\n",
    "        lat_feat = self.lat_enc(z)\n",
    "        return self.joint(torch.cat([tex_feat, lat_feat], dim=1))\n",
    "\n",
    "def compute_mi_loss(aux_net, texture, z):\n",
    "    #Matched pairs\n",
    "    T_joint = aux_net(texture, z)\n",
    "    pos_term = -F.softplus(-T_joint).mean()\n",
    "    #Mismatched pairs (shuffle z)\n",
    "    z_shuffle = z[torch.randperm(z.size(0))]\n",
    "    T_marginal = aux_net(texture, z_shuffle)\n",
    "    neg_term = F.softplus(T_marginal).mean()\n",
    "    mi = pos_term - neg_term\n",
    "    #MI ≥ E_joint[-softplus(-T)] - E_marginal[softplus(T)]\n",
    "    return -mi  #Negate because we minimize loss but want to maximize MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb40bb",
   "metadata": {},
   "source": [
    "## 7: Render Hat\n",
    "\n",
    "Render the hat using the texture and capture angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatRenderer:\n",
    "    \n",
    "    def __init__(self, mesh_path, render_size=256, device='cuda'):\n",
    "        self.device = device\n",
    "        self.render_size = render_size\n",
    "        #Load mesh - getting the verts, faces, and uv mappings\n",
    "        self.mesh_loaded = False\n",
    "        if Path(mesh_path).exists():\n",
    "            verts, faces, aux = load_obj(mesh_path, device=device)\n",
    "            self.verts = verts\n",
    "            self.faces = faces.verts_idx\n",
    "            self.verts_uvs = aux.verts_uvs\n",
    "            self.faces_uvs = faces.textures_idx\n",
    "            self.mesh_loaded = True\n",
    "            logger.info(f\"Loaded mesh: {len(verts)} verts, {len(self.faces)} faces\")\n",
    "        else:\n",
    "            #Use the placeholder\n",
    "            logger.warning(f\"Mesh not found at {mesh_path}. Using placeholder.\")\n",
    "            self._create_placeholder_mesh()\n",
    "        #Rasterization settings\n",
    "        self.raster_settings = RasterizationSettings(\n",
    "            image_size=render_size, \n",
    "            blur_radius=0.0, \n",
    "            faces_per_pixel=1\n",
    "        )\n",
    "        #Create rasterizer once\n",
    "        self.rasterizer = MeshRasterizer(raster_settings=self.raster_settings)\n",
    "\n",
    "    #Make a placeholder if we don't have a mesh which is a disk.\n",
    "    def _create_placeholder_mesh(self):\n",
    "        #Simple disk\n",
    "        n_points = 32\n",
    "        angles = torch.linspace(0, 2*np.pi, n_points+1)[:-1]\n",
    "        #Vertices: center + rim\n",
    "        verts = [[0, 0, 0]]  # center\n",
    "        for a in angles:\n",
    "            verts.append([torch.cos(a).item(), torch.sin(a).item(), 0])\n",
    "        self.verts = torch.tensor(verts, dtype=torch.float32, device=self.device)\n",
    "        #Faces: triangles from center to rim\n",
    "        faces = []\n",
    "        for i in range(n_points):\n",
    "            faces.append([0, i + 1, (i + 1) % n_points + 1])\n",
    "        self.faces = torch.tensor(faces, dtype=torch.int64, device=self.device)\n",
    "        #UVs: simple radial mapping\n",
    "        uvs = [[0.5, 0.5]]  # center\n",
    "        for a in angles:\n",
    "            uvs.append([0.5 + 0.5*torch.cos(a).item(), 0.5 + 0.5*torch.sin(a).item()])\n",
    "        self.verts_uvs = torch.tensor(uvs, dtype=torch.float32, device=self.device)\n",
    "        self.faces_uvs = self.faces.clone()\n",
    "        self.mesh_loaded = True\n",
    "\n",
    "    #Now we render the texture on the hat\n",
    "    def render(self, texture, elevation=90, azimuth=0, scale=1.0):\n",
    "        batch_size = texture.shape[0]\n",
    "        #Handle both scalar and per-sample scale (so scale the geometry)\n",
    "        if isinstance(scale, (int, float)):\n",
    "            verts = self.verts * scale\n",
    "            verts_list = [verts] * batch_size\n",
    "        elif isinstance(scale, torch.Tensor) and scale.dim() == 0:\n",
    "            verts = self.verts * scale.item()\n",
    "            verts_list = [verts] * batch_size\n",
    "        else:\n",
    "            verts_list = [self.verts * s.item() for s in scale]\n",
    "        #Camera setup\n",
    "        dist = 2.5  #Camera distance\n",
    "        if not isinstance(elevation, torch.Tensor):\n",
    "            elevation = torch.tensor([elevation], device=self.device).expand(batch_size)\n",
    "            azimuth = torch.tensor([azimuth], device=self.device).expand(batch_size)\n",
    "        elif elevation.dim() == 0:\n",
    "            elevation = elevation.unsqueeze(0).expand(batch_size)\n",
    "            azimuth = azimuth.unsqueeze(0).expand(batch_size)\n",
    "        R, T = look_at_view_transform(dist=dist, elev=elevation, azim=azimuth, device=self.device)\n",
    "        cameras = FoVPerspectiveCameras(R=R, T=T, device=self.device)\n",
    "        #Random Lighting (varying lighting)\n",
    "        light_x = np.random.uniform(-1, 1)\n",
    "        light_y = np.random.uniform(1, 3)  #Always somewhat above\n",
    "        light_z = np.random.uniform(-1, 1)\n",
    "        lights = PointLights(\n",
    "            device=self.device, \n",
    "            location=[[light_x, light_y, light_z]],\n",
    "            ambient_color=[[0.5, 0.5, 0.5]],\n",
    "            diffuse_color=[[0.3, 0.3, 0.3]],\n",
    "            specular_color=[[0.2, 0.2, 0.2]]\n",
    "        )\n",
    "        #Create shader once per render call (lighting changes)\n",
    "        shader = SoftPhongShader(device=self.device, cameras=cameras, lights=lights)\n",
    "        #Create batched texture and bind to mesh\n",
    "        tex_maps = texture.permute(0, 2, 3, 1)  #(B, H, W, 3)\n",
    "        textures = TexturesUV(\n",
    "            maps=tex_maps,\n",
    "            faces_uvs=[self.faces_uvs] * batch_size,\n",
    "            verts_uvs=[self.verts_uvs] * batch_size\n",
    "        )\n",
    "        #Assemble and render\n",
    "        meshes = Meshes(\n",
    "            verts=verts_list,\n",
    "            faces=[self.faces] * batch_size,\n",
    "            textures=textures\n",
    "        )\n",
    "        #Render entire batch at once - 2-phase raster than shade\n",
    "        fragments = self.rasterizer(meshes, cameras=cameras)\n",
    "        images = shader(fragments, meshes, cameras=cameras, lights=lights)\n",
    "        #Match PyTorch convention, format output\n",
    "        rendered_images = images[..., :3].permute(0, 3, 1, 2)\n",
    "        alpha_masks = images[..., 3:4].permute(0, 3, 1, 2)\n",
    "        #Return result\n",
    "        return rendered_images, alpha_masks\n",
    "\n",
    "def analyze_mesh_for_printing(mesh_path, config, preloaded=None):\n",
    "    from collections import defaultdict, deque\n",
    "    _p = config['printer']\n",
    "    #Check if the mesh is here\n",
    "    if not Path(mesh_path).exists():\n",
    "        logger.warning(\"No mesh file — defaulting to 12x12 inch print area\")\n",
    "        config['physical_size_inches'] = (12.0, 12.0)\n",
    "        config['texture_output_size_w'] = int(_p['dpi'] * 12)\n",
    "        config['texture_output_size_h'] = int(_p['dpi'] * 12)\n",
    "        config['uv_islands'] = []\n",
    "        return\n",
    "    #Check if we have already parsed the .obj\n",
    "    if preloaded is not None:\n",
    "        verts_np = preloaded['verts'].cpu().numpy()\n",
    "        verts_uvs = preloaded['verts_uvs'].cpu().numpy()\n",
    "        faces_verts = preloaded['faces_verts'].cpu().numpy()\n",
    "        faces_uvs = preloaded['faces_uvs'].cpu().numpy()\n",
    "    else:\n",
    "        verts, faces, aux = load_obj(mesh_path)\n",
    "        verts_np = verts.cpu().numpy()\n",
    "        verts_uvs = aux.verts_uvs.cpu().numpy()\n",
    "        faces_verts = faces.verts_idx.cpu().numpy()\n",
    "        faces_uvs = faces.textures_idx.cpu().numpy()\n",
    "    #Auto-callibration fallback\n",
    "    if _p['mesh_unit_to_inches'] is None:\n",
    "        x_span = verts_np[:, 0].max() - verts_np[:, 0].min()\n",
    "        y_span = verts_np[:, 1].max() - verts_np[:, 1].min()\n",
    "        z_span = verts_np[:, 2].max() - verts_np[:, 2].min()\n",
    "        max_span = max(x_span, y_span, z_span)\n",
    "        _p['mesh_unit_to_inches'] = 10.0 / max_span if max_span > 0 else 1.0\n",
    "        logger.warning(f\"Auto-estimated mesh scale: {_p['mesh_unit_to_inches']:.3f} in/unit \" f\"(mesh widest: {max_span:.3f} units -> ~10 inches)\")\n",
    "        logger.warning(\"SET printer.mesh_unit_to_inches manually for accuracy\")\n",
    "    scale = _p['mesh_unit_to_inches']\n",
    "    #Adjacency map\n",
    "    uv_vert_to_faces = defaultdict(set)\n",
    "    for fi, face in enumerate(faces_uvs):\n",
    "        for vi in face:\n",
    "            uv_vert_to_faces[vi].add(fi)\n",
    "    #BFS\n",
    "    visited = set()\n",
    "    islands = []\n",
    "    for fi in range(len(faces_uvs)):\n",
    "        if fi in visited:\n",
    "            continue\n",
    "        island = []\n",
    "        queue = deque([fi])\n",
    "        while queue:\n",
    "            f = queue.popleft()\n",
    "            if f in visited:\n",
    "                continue\n",
    "            visited.add(f)\n",
    "            island.append(f)\n",
    "            for vi in faces_uvs[f]:\n",
    "                for neighbor in uv_vert_to_faces[vi]:\n",
    "                    if neighbor not in visited:\n",
    "                        queue.append(neighbor)\n",
    "        islands.append(island)\n",
    "    #Measure the size of the islands\n",
    "    island_info = []\n",
    "    for idx, island_faces in enumerate(islands):\n",
    "        island_uv_indices = set()\n",
    "        for fi in island_faces:\n",
    "            for vi in faces_uvs[fi]:\n",
    "                island_uv_indices.add(vi)\n",
    "        island_uvs_arr = verts_uvs[list(island_uv_indices)]\n",
    "        u_min, v_min = island_uvs_arr.min(axis=0)\n",
    "        u_max, v_max = island_uvs_arr.max(axis=0)\n",
    "        #Triangle area via cross product - sum the area\n",
    "        total_area_3d = 0.0\n",
    "        for fi in island_faces:\n",
    "            v0 = verts_np[faces_verts[fi][0]]\n",
    "            v1 = verts_np[faces_verts[fi][1]]\n",
    "            v2 = verts_np[faces_verts[fi][2]]\n",
    "            total_area_3d += 0.5 * np.linalg.norm(np.cross(v1 - v0, v2 - v0))\n",
    "        #Ratio between 3D surface and UV area\n",
    "        physical_area = total_area_3d * (scale ** 2)\n",
    "        #Store measurements on the islands\n",
    "        island_info.append({\n",
    "            'index': idx,\n",
    "            'num_faces': len(island_faces),\n",
    "            'uv_bounds': (u_min, v_min, u_max, v_max),\n",
    "            'physical_area_sq_inches': physical_area,\n",
    "        })\n",
    "        logger.info(f\"  Island {idx}: {len(island_faces)} faces, ~{physical_area:.1f} sq in\")\n",
    "    #Overall print dimensions\n",
    "    config['uv_islands'] = island_info\n",
    "    logger.info(f\"Found {len(islands)} UV island(s)\")\n",
    "    #Total area\n",
    "    total_area = sum(i['physical_area_sq_inches'] for i in island_info)\n",
    "    all_u_min = min(i['uv_bounds'][0] for i in island_info)\n",
    "    all_v_min = min(i['uv_bounds'][1] for i in island_info)\n",
    "    all_u_max = max(i['uv_bounds'][2] for i in island_info)\n",
    "    all_v_max = max(i['uv_bounds'][3] for i in island_info)\n",
    "    uv_width = all_u_max - all_u_min\n",
    "    uv_height = all_v_max - all_v_min\n",
    "    #Bounding box uv area for printing\n",
    "    sa = _p['seam_allowance_inches']\n",
    "    #Average 3D-to-UV scale: compare total 3D area to total UV area\n",
    "    total_uv_area = 0.0\n",
    "    for island_faces_idx in islands:\n",
    "        for fi in island_faces_idx:\n",
    "            uv0 = verts_uvs[faces_uvs[fi][0]]\n",
    "            uv1 = verts_uvs[faces_uvs[fi][1]]\n",
    "            uv2 = verts_uvs[faces_uvs[fi][2]]\n",
    "            total_uv_area += 0.5 * abs(np.cross(uv1 - uv0, uv2 - uv0))\n",
    "    #physical_area / uv_area = (scale_per_uv_unit)^2\n",
    "    scale_per_uv = np.sqrt(total_area / (total_uv_area + 1e-8))\n",
    "    phys_w = uv_width * scale_per_uv + 2 * sa\n",
    "    phys_h = uv_height * scale_per_uv + 2 * sa\n",
    "    #Store info\n",
    "    config['physical_size_inches'] = (phys_w, phys_h)\n",
    "    config['texture_output_size_w'] = int(_p['dpi'] * phys_w)\n",
    "    config['texture_output_size_h'] = int(_p['dpi'] * phys_h)\n",
    "    #Log the info\n",
    "    logger.info(f\"Print size: {phys_w:.1f} x {phys_h:.1f} inches \" f\"({config['texture_output_size_w']}x{config['texture_output_size_h']}px @ {_p['dpi']} DPI)\")\n",
    "\n",
    "#Test renderer\n",
    "renderer = HatRenderer(CONFIG['mesh_path'], CONFIG['render_size'], device)\n",
    "#Do fresh load if we don't have the mesh\n",
    "if renderer.mesh_loaded and Path(CONFIG['mesh_path']).exists():\n",
    "    analyze_mesh_for_printing(CONFIG['mesh_path'], CONFIG, preloaded={\n",
    "        'verts': renderer.verts,\n",
    "        'verts_uvs': renderer.verts_uvs,\n",
    "        'faces_verts': renderer.faces,\n",
    "        'faces_uvs': renderer.faces_uvs,\n",
    "    })\n",
    "else:\n",
    "    analyze_mesh_for_printing(CONFIG['mesh_path'], CONFIG)\n",
    "#Render overhead\n",
    "test_render, test_alpha = renderer.render(test_texture, elevation=85, azimuth=45)\n",
    "logger.info(f\"Rendered shape: {test_render.shape}, alpha shape: {test_alpha.shape}\")\n",
    "#Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(test_texture[0].permute(1,2,0).detach().cpu())\n",
    "axes[0].set_title('Texture')\n",
    "axes[1].imshow(test_render[0].permute(1,2,0).detach().cpu())\n",
    "axes[1].set_title('Rendered Hat')\n",
    "axes[2].imshow(test_alpha[0, 0].detach().cpu(), cmap='gray')\n",
    "axes[2].set_title('Alpha Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a086a",
   "metadata": {},
   "source": [
    "## 8: T-SEA Augmentations\n",
    "\n",
    "Helper methods for black/gray/white box transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly mask a region of the rendered hat. Prevents overfitting to specific texture patterns.\n",
    "def patch_cutout(rendered_hat, alpha_mask, prob=0.9, ratio=0.4, fill=0.5):\n",
    "    if np.random.random() > prob:\n",
    "        return rendered_hat\n",
    "    B, C, H, W = rendered_hat.shape\n",
    "    #Random cutout size\n",
    "    cut_h = int(H * ratio)\n",
    "    cut_w = int(W * ratio)\n",
    "    #Random position\n",
    "    top = np.random.randint(0, H - cut_h + 1)\n",
    "    left = np.random.randint(0, W - cut_w + 1)\n",
    "    #Apply cutout (only where alpha > 0)\n",
    "    mask = alpha_mask.clone()\n",
    "    mask[:, :, top:top+cut_h, left:left+cut_w] = 0\n",
    "    rendered_hat = rendered_hat * mask + fill * (1 - mask) * (alpha_mask > 0).float()\n",
    "    return rendered_hat\n",
    "\n",
    "#Mild augmentations that don't distort the image too much.\n",
    "def constrained_augmentation(image):\n",
    "    B, C, H, W = image.shape\n",
    "    #Random scale (0.9 - 1.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    new_h = int(H * scale)\n",
    "    new_w = int(W * scale)\n",
    "    image = F.interpolate(image, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "    #Crop/pad back to original size\n",
    "    if new_h > H:\n",
    "        start_h = (new_h - H) // 2\n",
    "        start_w = (new_w - W) // 2\n",
    "        image = image[:, :, start_h:start_h+H, start_w:start_w+W]\n",
    "    else:\n",
    "        pad_h_top = (H - new_h) // 2\n",
    "        pad_h_bot = H - new_h - pad_h_top\n",
    "        pad_w_left = (W - new_w) // 2\n",
    "        pad_w_right = W - new_w - pad_w_left\n",
    "        image = F.pad(image, [pad_w_left, pad_w_right, pad_h_top, pad_h_bot], mode='reflect')\n",
    "    #Color jitter (mild)\n",
    "    brightness = np.random.uniform(0.9, 1.1)\n",
    "    image = image * brightness\n",
    "    #Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        image = torch.flip(image, dims=[3])\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#ShakeDrop reates virtual ensemble of model variants\n",
    "def shakedrop_forward(x, drop_prob=0.5, alpha_range=(0, 2)):\n",
    "    #Simplified ShakeDrop: add scaled noise to create virtual ensemble variants\n",
    "    if np.random.random() < drop_prob:\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = torch.randn_like(x) * 0.1 * alpha\n",
    "        x = x + noise\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02ef7e",
   "metadata": {},
   "source": [
    "## 9: URAdv Augmentations\n",
    "\n",
    "For better performance under drone conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66890419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add simulated light reflections on the hat surface.\n",
    "def add_light_spots(image, alpha_mask, num_range=(0, 3), intensity_range=(0.1, 0.4)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_spots = np.random.randint(*num_range)\n",
    "    y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "    for _ in range(num_spots):\n",
    "        #Random spot position (within hat region)\n",
    "        cy = np.random.randint(H // 4, 3 * H // 4)\n",
    "        cx = np.random.randint(W // 4, 3 * W // 4)\n",
    "        #Spot parameters\n",
    "        radius = np.random.uniform(0.05, 0.15) * min(H, W)\n",
    "        intensity = np.random.uniform(*intensity_range)\n",
    "        #Create Gaussian spot\n",
    "        dist = ((x - cx) ** 2 + (y - cy) ** 2).float()\n",
    "        spot = torch.exp(-dist / (2 * radius ** 2)) * intensity\n",
    "        #Apply only within hat (where alpha > 0)\n",
    "        spot = spot.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image + spot\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Add simulated shadows on the hat surface.\n",
    "def add_shadows(image, alpha_mask, num_range=(0, 2), opacity_range=(0.2, 0.5)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_shadows = np.random.randint(*num_range)\n",
    "    y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "    for _ in range(num_shadows):\n",
    "        #Random shadow as diagonal stripe\n",
    "        angle = np.random.uniform(0, np.pi)\n",
    "        opacity = np.random.uniform(*opacity_range)\n",
    "        width = np.random.uniform(0.1, 0.3) * min(H, W)\n",
    "        #Create shadow mask\n",
    "        offset = np.random.uniform(0, H)\n",
    "        dist = torch.abs(x * np.cos(angle) + y * np.sin(angle) - offset)\n",
    "        shadow = (dist < width).float() * opacity\n",
    "        #Apply only within hat\n",
    "        shadow = shadow.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image * (1 - shadow)\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Simulate printer color/brightness variation.\n",
    "def simulate_printing(texture, mul_std=0.1, add_std=0.05):\n",
    "    #Multiplicative noise\n",
    "    mul_noise = torch.randn_like(texture) * mul_std + 1.0\n",
    "    texture = texture * mul_noise\n",
    "    #Additive noise\n",
    "    add_noise = torch.randn_like(texture) * add_std\n",
    "    texture = texture + add_noise\n",
    "    return texture.clamp(0, 1)\n",
    "\n",
    "class PrinterGamut:\n",
    "\n",
    "    #Load parameters, or use simplified constraints\n",
    "    def __init__(self, config, device='cuda'):\n",
    "        self.config = config['printer']\n",
    "        self.device = device\n",
    "        gamut_path = self.config.get('gamut_samples_path')\n",
    "        if gamut_path and Path(gamut_path).exists():\n",
    "            self.gamut_samples = torch.from_numpy(np.load(gamut_path)).float().to(self.device)\n",
    "            self.use_measured_gamut = True\n",
    "            logger.info(f\"Loaded {len(self.gamut_samples)} gamut samples\")\n",
    "        else:\n",
    "            self.use_measured_gamut = False\n",
    "            logger.info(\"Using simplified gamut constraints\")\n",
    "\n",
    "    #Non-printability score loss\n",
    "    def nps_loss(self, texture):\n",
    "        loss = 0.0\n",
    "        if self.use_measured_gamut:\n",
    "            #Compare each texture pixel to the nearest printable color\n",
    "            #texture: (B, 3, H, W) -> (N, 3) flat pixel list\n",
    "            B, C, H, W = texture.shape\n",
    "            pixels = texture.permute(0, 2, 3, 1).reshape(-1, 3)  # (N, 3)\n",
    "            #gamut_samples: (G, 3) — measured printable RGB values\n",
    "            #Compute distance from each pixel to nearest gamut sample\n",
    "            chunk_size = 4096\n",
    "            gamut_dists = []\n",
    "            for i in range(0, pixels.shape[0], chunk_size):\n",
    "                chunk = pixels[i:i+chunk_size]  # (chunk, 3)\n",
    "                dists = torch.cdist(chunk.unsqueeze(0), self.gamut_samples.unsqueeze(0)).squeeze(0)  # (chunk, G)\n",
    "                min_dists = dists.min(dim=1)[0]  # (chunk,)\n",
    "                gamut_dists.append(min_dists)\n",
    "            gamut_dists = torch.cat(gamut_dists)\n",
    "            loss = loss + gamut_dists.mean()\n",
    "        else:\n",
    "            #Simplified constraints when no gamut data available\n",
    "            max_ch = texture.max(dim=1)[0]\n",
    "            min_ch = texture.min(dim=1)[0]\n",
    "            saturation = (max_ch - min_ch) / (max_ch + 1e-8)\n",
    "            brightness = max_ch\n",
    "            #Saturation * brightness threshold\n",
    "            loss = loss + F.relu(saturation * brightness - self.config['nps_threshold']).mean()\n",
    "            #Saturation cap\n",
    "            loss = loss + F.relu(saturation - self.config['max_saturation']).mean()\n",
    "            #Brightness bounds\n",
    "            loss = loss + F.relu(brightness - self.config['max_brightness']).mean()\n",
    "            loss = loss + F.relu(self.config['min_brightness'] - brightness).mean()\n",
    "        return loss\n",
    "\n",
    "    #Hard clamp for final export\n",
    "    def clamp_to_gamut(self, texture):\n",
    "        return texture.clamp(self.config['min_brightness'], self.config['max_brightness'])\n",
    "\n",
    "#Initialize globally\n",
    "printer_gamut = PrinterGamut(CONFIG, device)\n",
    "\n",
    "#Apply camera artifacts: blur, noise.\n",
    "def apply_environmental_augmentation(image, prob=0.3):\n",
    "    #Motion blur\n",
    "    if np.random.random() < prob:\n",
    "        kernel_size = np.random.choice([3, 5, 7])\n",
    "        kernel = torch.zeros(kernel_size, kernel_size, device=image.device)\n",
    "        kernel[kernel_size//2, :] = 1.0 / kernel_size\n",
    "        #Rotate kernel to random angle for directional blur\n",
    "        angle = np.random.uniform(0, 360)\n",
    "        angle_rad = np.deg2rad(angle)\n",
    "        cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
    "        #Build rotation matrix and affine grid\n",
    "        theta = torch.tensor([[cos_a, -sin_a, 0], [sin_a,  cos_a, 0]], dtype=torch.float32, device=image.device).unsqueeze(0)\n",
    "        grid = F.affine_grid(theta, [1, 1, kernel_size, kernel_size], align_corners=False)\n",
    "        kernel_rotated = F.grid_sample(kernel.unsqueeze(0).unsqueeze(0), grid, align_corners=False, mode='bilinear', padding_mode='zeros').squeeze()\n",
    "        #Renormalize so kernel sums to 1\n",
    "        kernel_rotated = kernel_rotated / (kernel_rotated.sum() + 1e-8)\n",
    "        image = F.conv2d(image, kernel_rotated.view(1, 1, kernel_size, kernel_size).expand(3, 1, -1, -1), padding=kernel_size//2, groups=3)\n",
    "    #Gaussian noise\n",
    "    if np.random.random() < prob:\n",
    "        noise_std = np.random.uniform(0.01, 0.05)\n",
    "        image = image + torch.randn_like(image) * noise_std\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Viewpoint jitter for robustness\n",
    "def apply_viewpoint_jitter(elevation, azimuth, scale, config):\n",
    "    elev = elevation + np.random.uniform(-config['camera_pitch_jitter'], config['camera_pitch_jitter'])\n",
    "    elev = np.clip(elev, 0, 90)\n",
    "    azim = azimuth + np.random.uniform(-config['heading_jitter'], config['heading_jitter'])\n",
    "    azim = azim % 360\n",
    "    scl = scale * (1 + np.random.uniform(-config['scale_jitter'], config['scale_jitter']))\n",
    "    scl = np.clip(scl, 0.1, 2.0)\n",
    "    return elev, azim, scl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296071",
   "metadata": {},
   "source": [
    "## 10: Toroidal Cropping\n",
    "\n",
    "Wrapping the texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to get smooth textures\n",
    "def total_variation_loss(texture):\n",
    "    diff_h = texture[:, :, 1:, :] - texture[:, :, :-1, :]\n",
    "    diff_w = texture[:, :, :, 1:] - texture[:, :, :, :-1]\n",
    "    return (diff_h.pow(2).mean() + diff_w.pow(2).mean()) / 2\n",
    "\n",
    "class ToroidalLatent(nn.Module):\n",
    "\n",
    "    def __init__(self, local_size, crop_size=9, latent_channels=128, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.local_size = local_size\n",
    "        self.crop_size = crop_size\n",
    "        self.latent_channels = latent_channels\n",
    "        self.z_local = nn.Parameter(torch.randn(1, latent_channels, local_size, local_size, device=device) * 0.1)\n",
    "\n",
    "    #Random crop\n",
    "    def get_random_crops(self, batch_size):\n",
    "        #Tile 3x3 for wraparound\n",
    "        z_tiled = self.z_local.repeat(1, 1, 3, 3)\n",
    "        crops = []\n",
    "        for _ in range(batch_size):\n",
    "            #Random offset within middle tile (to enable wraparound)\n",
    "            i = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            j = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            crop = z_tiled[:, :, i:i+self.crop_size, j:j+self.crop_size]\n",
    "            crops.append(crop)\n",
    "        return torch.cat(crops, dim=0)\n",
    "\n",
    "    #Crop to fill size\n",
    "    def get_full_latent(self, target_spatial_size):\n",
    "        reps = (target_spatial_size + self.local_size - 1) // self.local_size + 1\n",
    "        z_tiled = self.z_local.repeat(1, 1, reps, reps)\n",
    "        return z_tiled[:, :, :target_spatial_size, :target_spatial_size]\n",
    "\n",
    "    #Deterministic crop\n",
    "    def get_canonical_crop(self):\n",
    "        offset = (self.local_size - self.crop_size) // 2\n",
    "        return self.z_local[:, :, offset:offset+self.crop_size, offset:offset+self.crop_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fd5cd-3724-44f2-ab9f-f1d289d211a8",
   "metadata": {},
   "source": [
    "## 11: Sceen Composition\n",
    "\n",
    "Render the sceen (put hat on image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3c7d8-1f11-40fd-9e03-3d34c614c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_hat_on_scene(scene_image, hat_mask, rendered_hat, alpha_mask):\n",
    "    B, C, H, W = scene_image.shape\n",
    "    #For each image in batch, place hat at mask location\n",
    "    composited = scene_image.clone()\n",
    "    for i in range(B):\n",
    "        #Find bounding box of hat mask\n",
    "        mask = hat_mask[i, 0]\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        ys, xs = torch.where(mask > 0.5)\n",
    "        y1, y2 = ys.min().item(), ys.max().item()\n",
    "        x1, x2 = xs.min().item(), xs.max().item()\n",
    "        hat_h = y2 - y1 + 1\n",
    "        hat_w = x2 - x1 + 1\n",
    "        if hat_h < 2 or hat_w < 2:\n",
    "            continue\n",
    "        #Resize rendered hat to fit\n",
    "        hat_resized = F.interpolate(rendered_hat[i:i+1], size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        alpha_resized = F.interpolate(alpha_mask[i:i+1], size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        #Composite\n",
    "        region = composited[i:i+1, :, y1:y2+1, x1:x2+1]\n",
    "        composited[i:i+1, :, y1:y2+1, x1:x2+1] = (hat_resized * alpha_resized + region * (1 - alpha_resized))\n",
    "    return composited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64a18b-3226-42bf-80fc-0de9e91e51a5",
   "metadata": {},
   "source": [
    "## 12: Ensamble\n",
    "\n",
    "Ensamble detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e065e4-b4d7-41a4-95e3-7148c56e3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorEnsemble:\n",
    "    \n",
    "    def __init__(self, attack_mode='gray', device='cuda', conf_floor=0.001):\n",
    "        self.device = device\n",
    "        self.conf_floor = conf_floor\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        if attack_mode == 'white':\n",
    "            self.models['yolov8m'] = YOLO('yolov8m.pt')\n",
    "            self.weights['yolov8m'] = 1.0\n",
    "        elif attack_mode == 'gray':\n",
    "            model_configs = [\n",
    "                ('yolov8s', 0.20),\n",
    "                ('yolov8m', 0.25),\n",
    "                ('yolov8l', 0.20),\n",
    "                ('yolov5m', 0.20),\n",
    "                ('yolov5l', 0.15),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.weights[name] = weight\n",
    "                    logger.info(f\"Loaded {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\")\n",
    "        elif attack_mode == 'black':\n",
    "            #Add More\n",
    "            model_configs = [\n",
    "                ('yolov8m', 0.30),\n",
    "                ('yolov8l', 0.25),\n",
    "                ('yolov5l', 0.25),\n",
    "                ('yolov5m', 0.20),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.weights[name] = weight\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\") \n",
    "        #Normalize weights\n",
    "        total = sum(self.weights.values())\n",
    "        if total == 0:\n",
    "            logger.error(\"No detector models loaded. Cannot proceed.\")\n",
    "            raise RuntimeError(\"No detector models loaded successfully\")\n",
    "        self.weights = {k: v/total for k, v in self.weights.items()}\n",
    "        logger.info(f\"Detector ensemble ({attack_mode}): {list(self.weights.keys())}\")\n",
    "        \n",
    "    def compute_loss(self, images, return_detections=False):\n",
    "        total_loss = 0.0\n",
    "        all_detections = [] if return_detections else None\n",
    "        for name, model in self.models.items():\n",
    "            weight = self.weights[name]\n",
    "            #Differentiable forward  preserves gradient graph\n",
    "            preds = model.model(images)\n",
    "            #YOLOv8 raw output: (batch, 84, num_anchors) — no objectness head\n",
    "            #Row 0-3: bbox, row 4-83: class scores. Class 0 = person.\n",
    "            if isinstance(preds, (list, tuple)):\n",
    "                preds = preds[0]\n",
    "            if preds.dim() == 3 and preds.shape[1] == 84:\n",
    "                #YOLOv8 format\n",
    "                person_conf = preds[:, 4, :]  # class 0 scores across all anchors\n",
    "            elif preds.dim() == 3 and preds.shape[2] >= 85:\n",
    "                #YOLOv5 format: (batch, num_anchors, 85)\n",
    "                person_conf = preds[..., 4] * preds[..., 5]  # objectness × class 0\n",
    "            else:\n",
    "                logger.warning(f\"{name}: unexpected pred shape {preds.shape}, skipping\")\n",
    "                continue\n",
    "            B = person_conf.shape[0]\n",
    "            per_image_loss = []\n",
    "            for b in range(B):\n",
    "                img_confs = person_conf[b].reshape(-1)\n",
    "                k = min(10, img_confs.numel())\n",
    "                top_confs, _ = img_confs.topk(k)\n",
    "                per_image_loss.append(top_confs.mean())\n",
    "            model_loss = torch.stack(per_image_loss).mean()\n",
    "            total_loss = total_loss + weight * model_loss\n",
    "            if return_detections:\n",
    "                all_detections.append({'model': name,'preds': preds.detach()})\n",
    "        if return_detections:\n",
    "            return total_loss, all_detections\n",
    "        return total_loss\n",
    "    \n",
    "    def detect(self, images, conf_threshold=0.5):\n",
    "        images_np = (images * 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        all_results = {}\n",
    "        for name, model in self.models.items():\n",
    "            results = model.predict(images_np, conf=self.conf_floor, classes=[0], verbose=False)\n",
    "            all_results[name] = []\n",
    "            for r in results:\n",
    "                if len(r.boxes) > 0:\n",
    "                    scores = r.boxes.conf.cpu().numpy()\n",
    "                    boxes = r.boxes.xyxy.cpu().numpy()\n",
    "                    #Filter by the actual requested threshold\n",
    "                    keep = scores >= conf_threshold\n",
    "                    all_results[name].append({'boxes': boxes[keep],'scores': scores[keep]})\n",
    "                else:\n",
    "                    all_results[name].append({'boxes': np.array([]),'scores': np.array([])})\n",
    "        return all_results\n",
    "\n",
    "#Initialize detector ensemble\n",
    "detector = DetectorEnsemble(CONFIG['attack_mode'], device, conf_floor=CONFIG['det_conf_floor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cf65e-225e-43cf-82f7-0390b5b60c12",
   "metadata": {},
   "source": [
    "## 13: Stage 1: Generator Training\n",
    "\n",
    "Train the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e1d11-926e-4fb4-a190-166106a0c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(generator, aux_net, detector, dataset, config):\n",
    "\n",
    "    generator.train()\n",
    "    aux_net.train()\n",
    "    #Optimizers\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    opt_aux = torch.optim.Adam(aux_net.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=config['stage1_batch_size'], shuffle=True, num_workers=config['num_workers'], pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    #Training loop\n",
    "    logger.info(\"Starting Stage 1 training...\")\n",
    "    for epoch in range(config['stage1_epochs']):\n",
    "        epoch_losses = {'total': 0, 'detection': 0, 'tv': 0, 'nps': 0, 'mi': 0}\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config['stage1_epochs']}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            \n",
    "            #Sample latent\n",
    "            actual_batch = batch['image'].shape[0]\n",
    "            z = torch.randn(actual_batch, config['latent_channels'], config['latent_size'], config['latent_size'], device=device)\n",
    "            #Generate texture\n",
    "            texture = generator.generate(z)  # [0, 1]\n",
    "            #Sample viewpoint and render\n",
    "            elevs, azims, scls = [], [], []\n",
    "            for i in range(texture.shape[0]):\n",
    "                elev, azim, scl = apply_viewpoint_jitter(batch['elevation'][i].item(),batch['azimuth'][i].item(),batch['scale'][i].item(),CONFIG)\n",
    "                elevs.append(elev)\n",
    "                azims.append(azim)\n",
    "                scls.append(scl)\n",
    "            rendered_hat, alpha = renderer.render(texture,elevation=torch.tensor(elevs, device=device),azimuth=torch.tensor(azims, device=device),scale=torch.tensor(scls, device=device))\n",
    "            \n",
    "            #Apply T-SEA augmentations\n",
    "            rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "            #Apply URAdv augmentations\n",
    "            rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "            rendered_hat = add_shadows(rendered_hat, alpha)\n",
    "            rendered_hat = simulate_printing(rendered_hat)\n",
    "    \n",
    "            #Composite onto scene\n",
    "            scene = batch['image'].to(device)\n",
    "            hat_mask = batch['hat_mask'].to(device)\n",
    "            composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            #Apply environmental augmentations\n",
    "            composite = constrained_augmentation(composite)\n",
    "            composite = apply_environmental_augmentation(composite)\n",
    "            #ShakeDrop: perturb composite to create virtual ensemble variants\n",
    "            composite = shakedrop_forward(composite, drop_prob=config['shakedrop_prob'])\n",
    "            \n",
    "            #Detection loss\n",
    "            loss_det = detector.compute_loss(composite)\n",
    "            #Regularization losses\n",
    "            loss_tv = total_variation_loss(texture)\n",
    "            loss_nps = printer_gamut.nps_loss(texture)\n",
    "            #Mutual information (maximize = negate for min)\n",
    "            loss_mi = compute_mi_loss(aux_net, texture, z)\n",
    "            #Total loss\n",
    "            loss = (loss_det + config['lambda_tv'] * loss_tv + config['lambda_nps'] * loss_nps + config['lambda_info'] * loss_mi)  # loss_mi is already negated\n",
    "            \n",
    "            #Optimize\n",
    "            opt_g.zero_grad()\n",
    "            opt_aux.zero_grad()\n",
    "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(aux_net.parameters(), 1.0)\n",
    "                opt_g.step()\n",
    "                opt_aux.step()\n",
    "            #Track losses\n",
    "            epoch_losses['total'] += loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "            epoch_losses['detection'] += loss_det.item() if isinstance(loss_det, torch.Tensor) else loss_det\n",
    "            epoch_losses['tv'] += loss_tv.item()\n",
    "            epoch_losses['nps'] += loss_nps.item()\n",
    "            epoch_losses['mi'] += loss_mi.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\" if isinstance(loss, torch.Tensor) else f\"{loss:.4f}\", 'det': f\"{loss_det.item():.4f}\" if isinstance(loss_det, torch.Tensor) else f\"{loss_det:.4f}\"})\n",
    "        \n",
    "        #Epoch summary\n",
    "        n_batches = len(dataloader)\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= n_batches\n",
    "        logger.info(f\"Epoch {epoch+1} - Loss: {epoch_losses['total']:.4f}, \" f\"Det: {epoch_losses['detection']:.4f}, \" f\"TV: {epoch_losses['tv']:.4f}, \" f\"MI: {epoch_losses['mi']:.4f}\")\n",
    "        #Save sample texture every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                sample = generator.generate(batch_size=1)\n",
    "                save_texture(sample[0], f\"{config['output_dir']}/texture_epoch{epoch+1}.png\")   \n",
    "    return generator\n",
    "\n",
    "#Preview\n",
    "def save_texture(texture, path):\n",
    "    if isinstance(texture, torch.Tensor):\n",
    "        texture = texture.detach().cpu()\n",
    "        if texture.dim() == 3:\n",
    "            texture = texture.permute(1, 2, 0).numpy()\n",
    "        texture = (texture * 255).astype(np.uint8)\n",
    "    cv2.imwrite(str(path), cv2.cvtColor(texture, cv2.COLOR_RGB2BGR))\n",
    "    logger.info(f\"Saved texture to {path}\")\n",
    "\n",
    "#Pinter ready\n",
    "def save_final_texture(texture, config, path):\n",
    "    from PIL import Image\n",
    "    out_w = config['texture_output_size_w']\n",
    "    out_h = config['texture_output_size_h']\n",
    "    #Resize to print resolution\n",
    "    texture_highres = F.interpolate(texture, size=(out_h, out_w), mode='bilinear', align_corners=False)\n",
    "    #Clamp to printable gamut\n",
    "    texture_highres = printer_gamut.clamp_to_gamut(texture_highres)\n",
    "    #Convert to uint8\n",
    "    texture_np = texture_highres[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    texture_np = (texture_np * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(texture_np)\n",
    "    #Save TIFF with DPI metadata (for print)\n",
    "    tiff_path = Path(path).with_suffix('.tiff')\n",
    "    img.save(tiff_path, dpi=(config['printer']['dpi'], config['printer']['dpi']))\n",
    "    #Save PNG for preview\n",
    "    img.save(path)\n",
    "    logger.info(f\"Saved: {path} and {tiff_path}\")\n",
    "    logger.info(f\"  {out_w}x{out_h}px at {config['printer']['dpi']} DPI\")\n",
    "\n",
    "#Export texture with UV seam cut lines drawn on top.\n",
    "def export_print_template(texture, mesh_path, config, path):\n",
    "    from PIL import Image, ImageDraw\n",
    "    from collections import Counter\n",
    "    _p = config['printer']\n",
    "    out_w = config['texture_output_size_w']\n",
    "    out_h = config['texture_output_size_h']\n",
    "    texture_highres = F.interpolate(texture, size=(out_h, out_w), mode='bilinear', align_corners=False)\n",
    "    texture_highres = printer_gamut.clamp_to_gamut(texture_highres)\n",
    "    texture_np = texture_highres[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    texture_np = (texture_np * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(texture_np)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    if Path(mesh_path).exists():\n",
    "        _, faces, aux = load_obj(mesh_path)\n",
    "        verts_uvs = aux.verts_uvs.cpu().numpy()\n",
    "        faces_uvs = faces.textures_idx.cpu().numpy()\n",
    "        edge_count = Counter()\n",
    "        for face in faces_uvs:\n",
    "            for i in range(3):\n",
    "                e = (min(face[i], face[(i+1)%3]), max(face[i], face[(i+1)%3]))\n",
    "                edge_count[e] += 1\n",
    "        for face in faces_uvs:\n",
    "            for i in range(3):\n",
    "                v0 = verts_uvs[face[i]]\n",
    "                v1 = verts_uvs[face[(i+1) % 3]]\n",
    "                x0, y0 = int(v0[0] * out_w), int((1 - v0[1]) * out_h)\n",
    "                x1, y1 = int(v1[0] * out_w), int((1 - v1[1]) * out_h)\n",
    "                e = (min(face[i], face[(i+1)%3]), max(face[i], face[(i+1)%3]))\n",
    "                \n",
    "                if edge_count[e] == 1:\n",
    "                    draw.line([(x0, y0), (x1, y1)], fill=(255, 0, 0), width=3)\n",
    "        if 'uv_islands' in config:\n",
    "            for info in config['uv_islands']:\n",
    "                bounds = info['uv_bounds']\n",
    "                cx = int(((bounds[0] + bounds[2]) / 2) * out_w)\n",
    "                cy = int((1 - (bounds[1] + bounds[3]) / 2) * out_h)\n",
    "                area = info['physical_area_sq_inches']\n",
    "                draw.text((cx, cy), f\"Panel {info['index']} ({area:.1f} sq in)\", fill=(255, 255, 0))\n",
    "    tiff_path = Path(path).with_suffix('.tiff')\n",
    "    img.save(tiff_path, dpi=(_p['dpi'], _p['dpi']))\n",
    "    img.save(path)\n",
    "    clean_img = Image.fromarray(texture_np)\n",
    "    clean_path = Path(path).with_name(Path(path).stem + '_clean.png')\n",
    "    clean_img.save(clean_path)\n",
    "    logger.info(f\"Print template: {tiff_path}\")\n",
    "    logger.info(f\"  Red lines = CUT here\")\n",
    "    logger.info(f\"  {out_w}x{out_h}px, {config['physical_size_inches'][0]:.1f} x {config['physical_size_inches'][1]:.1f} inches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db60a2-2d8a-4248-9b01-9df76cb4185d",
   "metadata": {},
   "source": [
    "## 14: Stage 2: Latent Optimization\n",
    "\n",
    "Optimize the Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c3ed1-8530-48e9-9a72-aae0cbbd04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(generator, detector, dataset, config, z_local=None):\n",
    "\n",
    "    generator.eval()  #Freeze generator\n",
    "    \n",
    "    #Initialize toroidal latent\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=config['latent_size'],\n",
    "        latent_channels=config['latent_channels'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    if z_local is not None:\n",
    "        toroidal.z_local.data = z_local\n",
    "        \n",
    "    #Optimizer for latent only\n",
    "    optimizer = torch.optim.Adam(toroidal.parameters(), lr=config['stage2_lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config['stage2_iterations'], eta_min=1e-4\n",
    "    )\n",
    "    \n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=config['stage1_batch_size'], shuffle=True,\n",
    "        num_workers=config['num_workers'], pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    logger.info(\"Starting Stage 2 latent optimization...\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(config['stage2_iterations']), desc=\"Stage 2\")\n",
    "    for iteration in pbar:\n",
    "        #Get batch (cycle through dataset)\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            batch = next(data_iter)\n",
    "            \n",
    "        #Get random crops from toroidal latent\n",
    "        z_crops = toroidal.get_random_crops(config['stage1_batch_size'])\n",
    "            \n",
    "        #We need gradients through generator for z\n",
    "        texture = generator.generate(z_crops)\n",
    "        \n",
    "        #Sample viewpoints and render\n",
    "        elevs, azims, scls = [], [], []\n",
    "        for i in range(texture.shape[0]):\n",
    "            elev, azim, scl = apply_viewpoint_jitter(\n",
    "                batch['elevation'][i].item(),\n",
    "                batch['azimuth'][i].item(),\n",
    "                batch['scale'][i].item(),\n",
    "                CONFIG\n",
    "            )\n",
    "            elevs.append(elev)\n",
    "            azims.append(azim)\n",
    "            scls.append(scl)\n",
    "        rendered_hat, alpha = renderer.render(\n",
    "            texture,\n",
    "            elevation=torch.tensor(elevs, device=device),\n",
    "            azimuth=torch.tensor(azims, device=device),\n",
    "            scale=torch.tensor(scls, device=device),\n",
    "        )\n",
    "        \n",
    "        #Apply augmentations\n",
    "        rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "        rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "        rendered_hat = simulate_printing(rendered_hat)\n",
    "        \n",
    "        #Composite\n",
    "        scene = batch['image'].to(device)\n",
    "        hat_mask = batch['hat_mask'].to(device)\n",
    "        composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "        composite = constrained_augmentation(composite)\n",
    "        composite = apply_environmental_augmentation(composite)\n",
    "\n",
    "        #ShakeDrop: virtual ensemble\n",
    "        composite = shakedrop_forward(composite, drop_prob=config['shakedrop_prob'])\n",
    "        \n",
    "        #Compute loss (no MI term in Stage 2)\n",
    "        loss_det = detector.compute_loss(composite)\n",
    "        loss_tv = total_variation_loss(texture)\n",
    "        loss_nps = printer_gamut.nps_loss(texture)\n",
    "        \n",
    "        loss = (loss_det + \n",
    "               config['lambda_tv'] * loss_tv + \n",
    "               config['lambda_nps'] * loss_nps)\n",
    "        \n",
    "        #Optimize\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        #Track best\n",
    "        loss_val = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_z_local = toroidal.z_local.data.clone()\n",
    "            \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss_val:.4f}\",\n",
    "            'best': f\"{best_loss:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
    "        })\n",
    "        \n",
    "        #Periodic logging\n",
    "        if (iteration + 1) % 200 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Generate final texture at full resolution\n",
    "                z_canon = toroidal.get_canonical_crop()\n",
    "                final_texture = generator.generate(z_canon)\n",
    "                save_texture(final_texture[0], \n",
    "                           f\"{config['output_dir']}/texture_stage2_iter{iteration+1}.png\")\n",
    "                \n",
    "    #Save final results\n",
    "    logger.info(f\"Stage 2 complete. Best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    #Generate final texture\n",
    "    with torch.no_grad():\n",
    "        z_canon = toroidal.get_canonical_crop()\n",
    "        final_texture = generator.generate(z_canon)\n",
    "        \n",
    "        # Tile to cover full print area\n",
    "        def tile_texture(tex, target_h, target_w):\n",
    "            _, _, h, w = tex.shape\n",
    "            reps_h = (target_h + h - 1) // h\n",
    "            reps_w = (target_w + w - 1) // w\n",
    "            tiled = tex.repeat(1, 1, reps_h, reps_w)\n",
    "            return tiled[:, :, :target_h, :target_w]\n",
    "        \n",
    "        target_h = config.get('texture_output_size_h', 1024)\n",
    "        target_w = config.get('texture_output_size_w', 1024)\n",
    "        final_texture = tile_texture(final_texture, target_h, target_w)\n",
    "        \n",
    "    save_final_texture(final_texture, config, f\"{config['output_dir']}/final_texture.png\")\n",
    "    \n",
    "    torch.save({\n",
    "        'z_local': best_z_local,\n",
    "        'generator': generator.state_dict(),\n",
    "    }, f\"{config['output_dir']}/stage2_final.pth\")\n",
    "    \n",
    "    return best_z_local, final_texture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203957cc-502f-45f4-94ad-5c4c9320c6d7",
   "metadata": {},
   "source": [
    "## 15: Evaluation\n",
    "\n",
    "See how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13817e3-c57f-4511-a7a6-733797dabb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asr(detector, images, gt_boxes, conf_threshold=0.5, iou_threshold=0.5):\n",
    "\n",
    "    results = detector.detect(images, conf_threshold)\n",
    "    \n",
    "    n_success = 0\n",
    "    for model_name, model_results in results.items():\n",
    "        for i, det in enumerate(model_results):\n",
    "            gt_box = gt_boxes[i].cpu().numpy()\n",
    "            \n",
    "            detected = False\n",
    "            for box, score in zip(det['boxes'], det['scores']):\n",
    "                #Compute IoU\n",
    "                x1 = max(box[0], gt_box[0])\n",
    "                y1 = max(box[1], gt_box[1])\n",
    "                x2 = min(box[2], gt_box[2])\n",
    "                y2 = min(box[3], gt_box[3])\n",
    "                \n",
    "                inter = max(0, x2-x1) * max(0, y2-y1)\n",
    "                area1 = (box[2]-box[0]) * (box[3]-box[1])\n",
    "                area2 = (gt_box[2]-gt_box[0]) * (gt_box[3]-gt_box[1])\n",
    "                iou = inter / (area1 + area2 - inter + 1e-8)\n",
    "                \n",
    "                if iou >= iou_threshold:\n",
    "                    detected = True\n",
    "                    break\n",
    "                    \n",
    "            if not detected:\n",
    "                n_success += 1\n",
    "                \n",
    "    #Average across models\n",
    "    total = len(results) * len(gt_boxes)\n",
    "    return n_success / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_texture(generator, detector, dataset, z_local, config, num_samples=100):\n",
    "    generator.eval()\n",
    "    \n",
    "    #Create toroidal latent with optimized pattern\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=config['latent_size'],\n",
    "        latent_channels=config['latent_channels'],\n",
    "        device=device\n",
    "    )\n",
    "    toroidal.z_local.data = z_local\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=8, shuffle=False,\n",
    "        num_workers=config['num_workers'], pin_memory=True\n",
    "    )\n",
    "    \n",
    "    asr_scores = {thresh: [] for thresh in [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            #Generate texture\n",
    "            z_crops = toroidal.get_random_crops(8)\n",
    "            texture = generator.generate(z_crops)\n",
    "            \n",
    "            #Render and composite\n",
    "            rendered_hat, alpha = renderer.render(texture, 85, 0, 1.0)\n",
    "            \n",
    "            #For placeholder, use rendered directly\n",
    "            composite = rendered_hat\n",
    "            \n",
    "            # Test at multiple thresholds\n",
    "            gt_boxes = batch.get('person_bbox', torch.zeros(8, 4))\n",
    "            for thresh in asr_scores.keys():\n",
    "                asr = compute_asr(detector, composite, gt_boxes, conf_threshold=thresh)\n",
    "                asr_scores[thresh].append(asr)\n",
    "                \n",
    "    #Compute mean ASR\n",
    "    mean_asr = {}\n",
    "    for thresh, scores in asr_scores.items():\n",
    "        mean_asr[thresh] = np.mean(scores)\n",
    "        logger.info(f\"ASR@{thresh}: {mean_asr[thresh]:.2%}\")\n",
    "        \n",
    "    overall_masr = np.mean(list(mean_asr.values()))\n",
    "    logger.info(f\"Mean ASR: {overall_masr:.2%}\")\n",
    "    \n",
    "    return mean_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cbc12-7460-42f4-a62e-3076cd91fdd0",
   "metadata": {},
   "source": [
    "## 16: Main\n",
    "\n",
    "Bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62395d-f498-4292-9c3b-49ffa50c131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Initialize models\n",
    "    logger.info(\"Initializing models...\")\n",
    "    \n",
    "    generator = FCNGenerator(latent_channels=CONFIG['latent_channels']).to(device)\n",
    "    aux_net = AuxiliaryNetwork().to(device)\n",
    "    \n",
    "    logger.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    logger.info(f\"Auxiliary params: {sum(p.numel() for p in aux_net.parameters()):,}\")\n",
    "    \n",
    "    #Stage 1: Train generator\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 1: Generator Training\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    generator = train_stage1(generator, aux_net, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Stage 2: Optimize latent\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 2: Latent Optimization\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    best_z_local, final_texture = train_stage2(generator, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Evaluation\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"EVALUATION\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    asr_results = evaluate_texture(generator, detector, dataset, best_z_local, CONFIG)\n",
    "    export_print_template(final_texture, CONFIG['mesh_path'], CONFIG, f\"{CONFIG['output_dir']}/print_template.png\")\n",
    "    \n",
    "    #Final visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Final texture\n",
    "    axes[0].imshow(final_texture[0].permute(1, 2, 0).cpu())\n",
    "    axes[0].set_title('Final Adversarial Texture')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    #Rendered examples\n",
    "    with torch.no_grad():\n",
    "        rendered, alpha = renderer.render(final_texture, 85, 45, 1.0)\n",
    "    axes[1].imshow(rendered[0].permute(1, 2, 0).cpu())\n",
    "    axes[1].set_title('Rendered Hat (85°, 45°)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    #ASR plot\n",
    "    thresholds = list(asr_results.keys())\n",
    "    values = [asr_results[t] for t in thresholds]\n",
    "    axes[2].bar(range(len(thresholds)), values)\n",
    "    axes[2].set_xticks(range(len(thresholds)))\n",
    "    axes[2].set_xticklabels([f'{t}' for t in thresholds])\n",
    "    axes[2].set_xlabel('Confidence Threshold')\n",
    "    axes[2].set_ylabel('Attack Success Rate')\n",
    "    axes[2].set_title('ASR vs Threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/final_results.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"Training complete!\")\n",
    "    logger.info(f\"Final texture saved to: {CONFIG['output_dir']}/final_texture.png\")\n",
    "    \n",
    "    return generator, best_z_local, final_texture\n",
    "\n",
    "\n",
    "#Run if this is the main notebook\n",
    "if __name__ == \"__main__\":\n",
    "    generator, z_local, texture = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cloakhat)",
   "language": "python",
   "name": "cloakhat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
