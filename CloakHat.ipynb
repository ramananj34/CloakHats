{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1b43cd",
   "metadata": {},
   "source": [
    "# CloakHat Patch Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed60",
   "metadata": {},
   "source": [
    "## 1: Conda Setup\n",
    "\n",
    "Activate <br>\n",
    "`conda env create -f environment.yaml` <br>\n",
    "`conda activate cloakhat` <br>\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "\n",
    "Or\n",
    "\n",
    "Set up the environment\n",
    "\n",
    "`conda create -n cloakhat python=3.10 -y` <br>\n",
    "`conda activate cloakhat`\n",
    "\n",
    "PyTorch with CUDA <br>\n",
    "`conda install pytorch ipykernel torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y`\n",
    "\n",
    "PyTorch3D for differentiable rendering <br>\n",
    "`conda install -c pytorch3d pytorch3d -y`\n",
    "\n",
    "Detection models <br>\n",
    "`pip install ultralytics`\n",
    "\n",
    "Pip stuff <br>\n",
    "`pip install opencv-python-headless matplotlib tqdm tensorboard pyyaml trimesh`\n",
    "\n",
    "\n",
    "Either way\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc07834",
   "metadata": {},
   "source": [
    "## 2: Python Setup\n",
    "\n",
    "Get the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346291c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (look_at_view_transform, FoVPerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, SoftPhongShader, TexturesUV, PointLights)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c892",
   "metadata": {},
   "source": [
    "## 3: Config\n",
    "\n",
    "Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_dir': './data/drone_footage', #Drone footage\n",
    "    'mesh_path': './assets/hat.obj', #Hat meshes\n",
    "    'output_dir': './outputs',\n",
    "    \n",
    "    #Generator\n",
    "    'latent_channels': 128,\n",
    "    'latent_size': 9, #Spatial size of latent input\n",
    "    'texture_size': 288, #Output texture size from generator\n",
    "    \n",
    "    #Viewpoint sampling\n",
    "    'elevation_range': (60, 90), #Degrees from horizontal (90 = overhead)\n",
    "    'scale_range': (0.3, 1.2), #Altitude proxy\n",
    "    \n",
    "    #Training Stage 1\n",
    "    'stage1_epochs': 100,\n",
    "    'stage1_batch_size': 8,\n",
    "    'stage1_lr': 2e-4,\n",
    "    \n",
    "    #Training Stage 2  \n",
    "    'stage2_iterations': 2000,\n",
    "    'stage2_lr': 0.01,\n",
    "    'local_latent_size': 18, #Size of optimizable latent pattern\n",
    "    \n",
    "    #Loss weights\n",
    "    'lambda_tv': 2.5, #Total variation\n",
    "    'lambda_nps': 0.01, #Non-printability score\n",
    "    'lambda_info': 0.1, #Mutual information (Stage 1 only)\n",
    "    \n",
    "    #T-SEA Stuff\n",
    "    'cutout_prob': 0.9,\n",
    "    'cutout_ratio': 0.4,\n",
    "    'shakedrop_prob': 0.5,\n",
    "    \n",
    "    #Rendering\n",
    "    'render_size': 256,\n",
    "    \n",
    "    #Printing (PLACEHOLDER, need details from FABLAB)\n",
    "    'nps_threshold': 0.7,  #Saturation * brightness threshold\n",
    "    \n",
    "    #Attack config (white, gray, black)\n",
    "    'attack_mode': 'gray',\n",
    "}\n",
    "\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ae201-d285-4de7-9313-dea9d81753c3",
   "metadata": {},
   "source": [
    "## 4: Dataset Preparation\n",
    "\n",
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ae3ab-ca07-4d00-83ec-92db0313ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset_dir/\n",
    "        frames/\n",
    "            frame_0001.png\n",
    "            frame_0002.png\n",
    "            ...\n",
    "        masks/\n",
    "            frame_0001_mask.png  (binary mask of green hat region)\n",
    "            ...\n",
    "        annotations.json  (person bounding boxes, metadata)\n",
    "\"\"\"\n",
    "\n",
    "class DroneDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir, transform=None):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        #PLACEHOLDER\n",
    "        #Check if dataset exists\n",
    "        if not self.dataset_dir.exists():\n",
    "            logger.warning(f\"Dataset directory not found: {dataset_dir}\")\n",
    "            logger.warning(\"Using placeholder data for testing.\")\n",
    "            self.use_placeholder = True\n",
    "            self.length = 100  # Fake dataset size\n",
    "        else:\n",
    "            self.use_placeholder = False\n",
    "            # Load annotations\n",
    "            # self.annotations = json.load(open(self.dataset_dir / 'annotations.json'))\n",
    "            # self.length = len(self.annotations)\n",
    "            self.length = 100  # PLACEHOLDER\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_placeholder:\n",
    "            #Return placeholder data for testing pipeline\n",
    "            #Random background image\n",
    "            image = torch.rand(3, 1080, 1920)\n",
    "            \n",
    "            #Random hat mask (small region)\n",
    "            hat_mask = torch.zeros(1, 1080, 1920)\n",
    "            cy, cx = np.random.randint(200, 880), np.random.randint(200, 1720)\n",
    "            hat_mask[:, cy-50:cy+50, cx-50:cx+50] = 1.0\n",
    "            \n",
    "            #Person bounding box (around hat)\n",
    "            person_bbox = torch.tensor([cx-100, cy-150, cx+100, cy+200], dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'hat_mask': hat_mask,\n",
    "                'person_bbox': person_bbox,\n",
    "            }\n",
    "            \n",
    "        #Implement actual data loading\n",
    "        #frame_path = self.dataset_dir / 'frames' / f'frame_{idx:04d}.png'\n",
    "        #mask_path = self.dataset_dir / 'masks' / f'frame_{idx:04d}_mask.png'\n",
    "        #image = load_image(frame_path)\n",
    "        #hat_mask = load_mask(mask_path)\n",
    "        #person_bbox = self.annotations[idx]['person_bbox']\n",
    "        \n",
    "        raise NotImplementedError(\"Implement when I have real data\")\n",
    "\n",
    "\n",
    "def segment_green_hat(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    #Green range in HSV\n",
    "    lower_green = np.array([35, 100, 100])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    \n",
    "    #Clean up mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "#Create dataset (will use placeholder if data doesn't exist)\n",
    "dataset = DroneDataset(CONFIG['dataset_dir'])\n",
    "logger.info(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1445b0",
   "metadata": {},
   "source": [
    "## 5: FCN Generator\n",
    "\n",
    "Make the texture (turn noise into an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            #9 -> 9\n",
    "            nn.Conv2d(latent_channels, 512, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            #9 -> 18\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #18 -> 36\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 36 -> 72\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 72 -> 144\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 144 -> 288\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 288 -> 288 (to RGB)\n",
    "            nn.Conv2d(32, 3, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_size = 288\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "    \n",
    "    def generate(self, z=None, batch_size=1):\n",
    "        if z is None:\n",
    "            z = torch.randn(batch_size, 128, 9, 9, device=next(self.parameters()).device)\n",
    "        return (self.forward(z) + 1) / 2\n",
    "\n",
    "# Test\n",
    "generator = FCNGenerator().to(device)\n",
    "test_texture = generator.generate(batch_size=1)\n",
    "logger.info(f\"Generator output: {test_texture.shape}\")  #Should be (1, 3, 288, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec701a8",
   "metadata": {},
   "source": [
    "## 6: Auxiliary Network\n",
    "\n",
    "Forces the texture to derive from the latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryNetwork(nn.Module):\n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Texture encoder\n",
    "        self.tex_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Latent encoder\n",
    "        self.lat_enc = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 256, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Joint network\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texture, z):\n",
    "        tex_feat = self.tex_enc(texture)\n",
    "        lat_feat = self.lat_enc(z)\n",
    "        return self.joint(torch.cat([tex_feat, lat_feat], dim=1))\n",
    "\n",
    "\n",
    "def compute_mi_loss(aux_net, texture, z):\n",
    "\n",
    "    #Matched pairs\n",
    "    T_joint = aux_net(texture, z)\n",
    "    pos_term = -F.softplus(-T_joint).mean()\n",
    "    \n",
    "    #Mismatched pairs (shuffle z)\n",
    "    z_shuffle = z[torch.randperm(z.size(0))]\n",
    "    T_marginal = aux_net(texture, z_shuffle)\n",
    "    neg_term = F.softplus(T_marginal).mean()\n",
    "    \n",
    "    mi = pos_term - neg_term\n",
    "    return -mi  #Negate because we minimize loss but want to maximize MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb40bb",
   "metadata": {},
   "source": [
    "## 7: Render Hat\n",
    "\n",
    "Render the hat using the texture and capture angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatRenderer:\n",
    "    def __init__(self, mesh_path, render_size=256, device='cuda'):\n",
    "        self.device = device\n",
    "        self.render_size = render_size\n",
    "        \n",
    "        #Load mesh\n",
    "        self.mesh_loaded = False\n",
    "        if Path(mesh_path).exists():\n",
    "            verts, faces, aux = load_obj(mesh_path, device=device)\n",
    "            self.verts = verts\n",
    "            self.faces = faces.verts_idx\n",
    "            self.verts_uvs = aux.verts_uvs\n",
    "            self.faces_uvs = faces.textures_idx\n",
    "            self.mesh_loaded = True\n",
    "            logger.info(f\"Loaded mesh: {len(verts)} verts, {len(self.faces)} faces\")\n",
    "        else:\n",
    "            logger.warning(f\"Mesh not found at {mesh_path}. Using placeholder.\")\n",
    "            self._create_placeholder_mesh()\n",
    "            \n",
    "        #Rasterization settings\n",
    "        self.raster_settings = RasterizationSettings(image_size=render_size, blur_radius=0.0, faces_per_pixel=1)\n",
    "    \n",
    "    #This method was made with help from ChatGPT\n",
    "    def _create_placeholder_mesh(self):\n",
    "        #Simple disk\n",
    "        n_points = 32\n",
    "        angles = torch.linspace(0, 2*np.pi, n_points+1)[:-1]\n",
    "        #Vertices: center + rim\n",
    "        verts = [[0, 0, 0]]  # center\n",
    "        for a in angles:\n",
    "            verts.append([torch.cos(a).item(), torch.sin(a).item(), 0])\n",
    "        self.verts = torch.tensor(verts, dtype=torch.float32, device=self.device)\n",
    "        #Faces: triangles from center to rim\n",
    "        faces = []\n",
    "        for i in range(n_points):\n",
    "            faces.append([0, i+1, (i % n_points) + 2 if i < n_points-1 else 1])\n",
    "        self.faces = torch.tensor(faces, dtype=torch.int64, device=self.device)\n",
    "        #UVs: simple radial mapping\n",
    "        uvs = [[0.5, 0.5]]  # center\n",
    "        for a in angles:\n",
    "            uvs.append([0.5 + 0.5*torch.cos(a).item(), 0.5 + 0.5*torch.sin(a).item()])\n",
    "        self.verts_uvs = torch.tensor(uvs, dtype=torch.float32, device=self.device)\n",
    "        self.faces_uvs = self.faces.clone()\n",
    "        self.mesh_loaded = True\n",
    "        \n",
    "    def sample_viewpoint(self):\n",
    "        #Elevation: beta distribution skewed toward overhead\n",
    "        elev_norm = np.random.beta(3, 1)\n",
    "        elev = CONFIG['elevation_range'][0] + elev_norm * (CONFIG['elevation_range'][1] - CONFIG['elevation_range'][0])\n",
    "        #Azimuth: uniform\n",
    "        azim = np.random.uniform(0, 360)\n",
    "        #Scale: uniform\n",
    "        scale = np.random.uniform(*CONFIG['scale_range'])\n",
    "        return elev, azim, scale\n",
    "    \n",
    "    def render(self, texture, elevation=90, azimuth=0, scale=1.0):\n",
    "        batch_size = texture.shape[0]\n",
    "        #Scale vertices\n",
    "        verts = self.verts * scale\n",
    "        #Camera setup\n",
    "        dist = 2.5  #Camera distance\n",
    "        R, T = look_at_view_transform(dist=dist, elev=elevation, azim=azimuth, device=self.device)\n",
    "        cameras = FoVPerspectiveCameras(R=R, T=T, device=self.device)\n",
    "        #Lighting (varying lighting)\n",
    "        light_x = np.random.uniform(-1, 1)\n",
    "        light_y = np.random.uniform(1, 3)  #Always somewhat above\n",
    "        light_z = np.random.uniform(-1, 1)\n",
    "        lights = PointLights(device=self.device, location=[[light_x, light_y, light_z]],ambient_color=[[0.5, 0.5, 0.5]],diffuse_color=[[0.3, 0.3, 0.3]],specular_color=[[0.2, 0.2, 0.2]])\n",
    "        \n",
    "        rendered_images = []\n",
    "        alpha_masks = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            #Create texture for this sample\n",
    "            tex = TexturesUV(maps=texture[i:i+1].permute(0, 2, 3, 1), faces_uvs=[self.faces_uvs], verts_uvs=[self.verts_uvs])\n",
    "            #Create mesh\n",
    "            mesh = Meshes(verts=[verts], faces=[self.faces], textures=tex)\n",
    "            #Renderer\n",
    "            renderer = MeshRenderer(rasterizer=MeshRasterizer(cameras=cameras, raster_settings=self.raster_settings), shader=SoftPhongShader(device=self.device, cameras=cameras, lights=lights))\n",
    "            #Render\n",
    "            images = renderer(mesh)\n",
    "            rendered_images.append(images[..., :3].permute(0, 3, 1, 2))\n",
    "            alpha_masks.append(images[..., 3:4].permute(0, 3, 1, 2))\n",
    "            \n",
    "        return torch.cat(rendered_images, dim=0), torch.cat(alpha_masks, dim=0)\n",
    "\n",
    "#Test renderer\n",
    "renderer = HatRenderer(CONFIG['mesh_path'], CONFIG['render_size'], device)\n",
    "test_render, test_alpha = renderer.render(test_texture, elevation=85, azimuth=45)\n",
    "logger.info(f\"Rendered shape: {test_render.shape}, alpha shape: {test_alpha.shape}\")\n",
    "\n",
    "#Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(test_texture[0].permute(1,2,0).detach().cpu())\n",
    "axes[0].set_title('Texture')\n",
    "axes[1].imshow(test_render[0].permute(1,2,0).detach().cpu())\n",
    "axes[1].set_title('Rendered Hat')\n",
    "axes[2].imshow(test_alpha[0, 0].detach().cpu(), cmap='gray')\n",
    "axes[2].set_title('Alpha Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a086a",
   "metadata": {},
   "source": [
    "## 8: T-SEA Augmentations\n",
    "\n",
    "Helper methods for black/gray/white box transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly mask a region of the rendered hat. Prevents overfitting to specific texture patterns.\n",
    "def patch_cutout(rendered_hat, alpha_mask, prob=0.9, ratio=0.4, fill=0.5):\n",
    "    if np.random.random() > prob:\n",
    "        return rendered_hat\n",
    "    B, C, H, W = rendered_hat.shape\n",
    "    #Random cutout size\n",
    "    cut_h = int(H * ratio)\n",
    "    cut_w = int(W * ratio)\n",
    "    #Random position\n",
    "    top = np.random.randint(0, H - cut_h + 1)\n",
    "    left = np.random.randint(0, W - cut_w + 1)\n",
    "    #Apply cutout (only where alpha > 0)\n",
    "    mask = alpha_mask.clone()\n",
    "    mask[:, :, top:top+cut_h, left:left+cut_w] = 0\n",
    "    rendered_hat = rendered_hat * mask + fill * (1 - mask) * (alpha_mask > 0).float()\n",
    "    return rendered_hat\n",
    "\n",
    "#Mild augmentations that don't distort the image too much.\n",
    "def constrained_augmentation(image):\n",
    "    B, C, H, W = image.shape\n",
    "    #Random scale (0.9 - 1.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    new_size = int(H * scale)\n",
    "    image = F.interpolate(image, size=new_size, mode='bilinear', align_corners=False)\n",
    "    #Crop/pad back to original size\n",
    "    if new_size > H:\n",
    "        start = (new_size - H) // 2\n",
    "        image = image[:, :, start:start+H, start:start+W]\n",
    "    else:\n",
    "        pad = (H - new_size) // 2\n",
    "        image = F.pad(image, [pad, pad, pad, pad], mode='reflect')\n",
    "        image = image[:, :, :H, :W]\n",
    "    #Color jitter (mild)\n",
    "    brightness = np.random.uniform(0.9, 1.1)\n",
    "    image = image * brightness\n",
    "    #Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        image = torch.flip(image, dims=[3])\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#ShakeDrop reates virtual ensemble of model variants\n",
    "def shakedrop_forward(model, x, drop_prob=0.5, alpha_range=(0, 2)):\n",
    "    #I will make a simplified version: add noise to intermediate features\n",
    "    if np.random.random() < drop_prob:\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = torch.randn_like(x) * 0.1 * alpha\n",
    "        x = x + noise\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02ef7e",
   "metadata": {},
   "source": [
    "## 9: URAdv Augmentations\n",
    "\n",
    "For better performance under drone conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66890419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add simulated light reflections on the hat surface.\n",
    "def add_light_spots(image, alpha_mask, num_range=(0, 3), intensity_range=(0.1, 0.4)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_spots = np.random.randint(*num_range)\n",
    "    for _ in range(num_spots):\n",
    "        #Random spot position (within hat region)\n",
    "        cy = np.random.randint(H // 4, 3 * H // 4)\n",
    "        cx = np.random.randint(W // 4, 3 * W // 4)\n",
    "        #Spot parameters\n",
    "        radius = np.random.uniform(0.05, 0.15) * min(H, W)\n",
    "        intensity = np.random.uniform(*intensity_range)\n",
    "        #Create Gaussian spot\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "        dist = ((x - cx) ** 2 + (y - cy) ** 2).float()\n",
    "        spot = torch.exp(-dist / (2 * radius ** 2)) * intensity\n",
    "        #Apply only within hat (where alpha > 0)\n",
    "        spot = spot.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image + spot\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Add simulated shadows on the hat surface.\n",
    "def add_shadows(image, alpha_mask, num_range=(0, 2), opacity_range=(0.2, 0.5)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_shadows = np.random.randint(*num_range)\n",
    "    for _ in range(num_shadows):\n",
    "        #Random shadow as diagonal stripe\n",
    "        angle = np.random.uniform(0, np.pi)\n",
    "        opacity = np.random.uniform(*opacity_range)\n",
    "        width = np.random.uniform(0.1, 0.3) * min(H, W)\n",
    "        #Create shadow mask\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "        offset = np.random.uniform(0, H)\n",
    "        dist = torch.abs(x * np.cos(angle) + y * np.sin(angle) - offset)\n",
    "        shadow = (dist < width).float() * opacity\n",
    "        #Apply only within hat\n",
    "        shadow = shadow.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image * (1 - shadow)\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Simulate printer color/brightness variation.\n",
    "def simulate_printing(texture, mul_std=0.1, add_std=0.05):\n",
    "    #Multiplicative noise\n",
    "    mul_noise = torch.randn_like(texture) * mul_std + 1.0\n",
    "    texture = texture * mul_noise\n",
    "    #Additive noise\n",
    "    add_noise = torch.randn_like(texture) * add_std\n",
    "    texture = texture + add_noise\n",
    "    return texture.clamp(0, 1)\n",
    "\n",
    "#Apply camera artifacts: blur, noise.\n",
    "def apply_environmental_augmentation(image, prob=0.3):\n",
    "    #Motion blur\n",
    "    if np.random.random() < prob:\n",
    "        kernel_size = np.random.choice([3, 5, 7])\n",
    "        kernel = torch.zeros(kernel_size, kernel_size, device=image.device)\n",
    "        kernel[kernel_size//2, :] = 1.0 / kernel_size\n",
    "        #Random rotation of kernel\n",
    "        angle = np.random.uniform(0, 360)\n",
    "        # Simplified: just apply horizontal blur\n",
    "        image = F.conv2d(image, kernel.view(1, 1, kernel_size, kernel_size).expand(3, 1, -1, -1), padding=kernel_size//2, groups=3)\n",
    "    #Gaussian noise\n",
    "    if np.random.random() < prob:\n",
    "        noise_std = np.random.uniform(0.01, 0.05)\n",
    "        image = image + torch.randn_like(image) * noise_std\n",
    "    return image.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296071",
   "metadata": {},
   "source": [
    "## 10: Toroidal Cropping\n",
    "\n",
    "Wrapping the texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToroidalLatent:\n",
    "    \n",
    "    def __init__(self, local_size, crop_size=9, latent_channels=128, device='cuda'):\n",
    "        self.local_size = local_size\n",
    "        self.crop_size = crop_size\n",
    "        self.latent_channels = latent_channels\n",
    "        self.device = device\n",
    "        #Initialize local latent pattern\n",
    "        self.z_local = nn.Parameter(torch.randn(1, latent_channels, local_size, local_size, device=device) * 0.1)\n",
    "        \n",
    "    def get_random_crops(self, batch_size):\n",
    "        #Tile 3x3 for wraparound\n",
    "        z_tiled = self.z_local.repeat(1, 1, 3, 3)\n",
    "        \n",
    "        crops = []\n",
    "        for _ in range(batch_size):\n",
    "            #Random offset within middle tile (to enable wraparound)\n",
    "            i = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            j = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            crop = z_tiled[:, :, i:i+self.crop_size, j:j+self.crop_size]\n",
    "            crops.append(crop)\n",
    "            \n",
    "        return torch.cat(crops, dim=0)\n",
    "    \n",
    "    def get_full_latent(self, target_spatial_size):\n",
    "        reps = (target_spatial_size + self.local_size - 1) // self.local_size + 1\n",
    "        z_tiled = self.z_local.repeat(1, 1, reps, reps)\n",
    "        return z_tiled[:, :, :target_spatial_size, :target_spatial_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fd5cd-3724-44f2-ab9f-f1d289d211a8",
   "metadata": {},
   "source": [
    "## 11: Sceen Composition\n",
    "\n",
    "Render the sceen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3c7d8-1f11-40fd-9e03-3d34c614c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_hat_on_scene(scene_image, hat_mask, rendered_hat, alpha_mask):\n",
    "\n",
    "    B, C, H, W = scene_image.shape\n",
    "    \n",
    "    #For each image in batch, place hat at mask location\n",
    "    composited = scene_image.clone()\n",
    "    \n",
    "    for i in range(B):\n",
    "        #Find bounding box of hat mask\n",
    "        mask = hat_mask[i, 0]\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        ys, xs = torch.where(mask > 0.5)\n",
    "        y1, y2 = ys.min().item(), ys.max().item()\n",
    "        x1, x2 = xs.min().item(), xs.max().item()\n",
    "        \n",
    "        hat_h = y2 - y1\n",
    "        hat_w = x2 - x1\n",
    "        \n",
    "        #Resize rendered hat to fit\n",
    "        hat_resized = F.interpolate(rendered_hat[i:i+1], size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        alpha_resized = F.interpolate(alpha_mask[i:i+1], size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        #Composite\n",
    "        region = composited[i:i+1, :, y1:y2, x1:x2]\n",
    "        composited[i:i+1, :, y1:y2, x1:x2] = (hat_resized * alpha_resized + region * (1 - alpha_resized))\n",
    "        \n",
    "    return composited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64a18b-3226-42bf-80fc-0de9e91e51a5",
   "metadata": {},
   "source": [
    "## 12: Ensamble\n",
    "\n",
    "Ensamble detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e065e4-b4d7-41a4-95e3-7148c56e3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectorEnsemble:\n",
    "    \n",
    "    def __init__(self, attack_mode='gray', device='cuda'):\n",
    "        self.device = device\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        \n",
    "        if attack_mode == 'white':\n",
    "            self.models['yolov8m'] = YOLO('yolov8m.pt')\n",
    "            self.weights['yolov8m'] = 1.0\n",
    "            \n",
    "        elif attack_mode == 'gray':\n",
    "            model_configs = [\n",
    "                ('yolov8s', 0.20),\n",
    "                ('yolov8m', 0.25),\n",
    "                ('yolov8l', 0.20),\n",
    "                ('yolov5m', 0.20),\n",
    "                ('yolov5l', 0.15),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.weights[name] = weight\n",
    "                    logger.info(f\"Loaded {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\")\n",
    "                    \n",
    "        elif attack_mode == 'black':\n",
    "            #Add More\n",
    "            model_configs = [\n",
    "                ('yolov8m', 0.30),\n",
    "                ('yolov8l', 0.25),\n",
    "                ('yolov5l', 0.25),\n",
    "                ('yolov5m', 0.20),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.weights[name] = weight\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\")\n",
    "                    \n",
    "        #Normalize weights\n",
    "        total = sum(self.weights.values())\n",
    "        self.weights = {k: v/total for k, v in self.weights.items()}\n",
    "        \n",
    "        logger.info(f\"Detector ensemble ({attack_mode}): {list(self.weights.keys())}\")\n",
    "        \n",
    "    def compute_loss(self, images, return_detections=False):\n",
    "        total_loss = 0.0\n",
    "        all_detections = [] if return_detections else None\n",
    "        \n",
    "        #Convert to uint8 numpy for YOLO\n",
    "        images_np = (images * 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            weight = self.weights[name]\n",
    "            \n",
    "            #Run detection with low confidence to get gradients\n",
    "            results = model.predict(images_np, conf=0.001, classes=[0], verbose=False)\n",
    "            \n",
    "            #Collect person detection confidences\n",
    "            batch_confs = []\n",
    "            for r in results:\n",
    "                if len(r.boxes) > 0:\n",
    "                    confs = r.boxes.conf.to(self.device)\n",
    "                    batch_confs.append(confs)\n",
    "                    \n",
    "            if batch_confs:\n",
    "                #Loss = mean of top-k confidences per image\n",
    "                all_confs = torch.cat(batch_confs)\n",
    "                k = min(10, len(all_confs))\n",
    "                top_confs, _ = torch.topk(all_confs, k)\n",
    "                loss = top_confs.mean()\n",
    "                total_loss = total_loss + weight * loss\n",
    "                \n",
    "            if return_detections:\n",
    "                all_detections.append({\n",
    "                    'model': name,\n",
    "                    'results': results\n",
    "                })\n",
    "                \n",
    "        if return_detections:\n",
    "            return total_loss, all_detections\n",
    "        return total_loss\n",
    "    \n",
    "    def detect(self, images, conf_threshold=0.5):\n",
    "        images_np = (images * 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "        \n",
    "        all_results = {}\n",
    "        for name, model in self.models.items():\n",
    "            results = model.predict(images_np, conf=conf_threshold, classes=[0], verbose=False)\n",
    "            all_results[name] = []\n",
    "            for r in results:\n",
    "                all_results[name].append({\n",
    "                    'boxes': r.boxes.xyxy.cpu().numpy() if len(r.boxes) > 0 else np.array([]),\n",
    "                    'scores': r.boxes.conf.cpu().numpy() if len(r.boxes) > 0 else np.array([]),\n",
    "                })\n",
    "        return all_results\n",
    "\n",
    "\n",
    "#Initialize detector ensemble\n",
    "detector = DetectorEnsemble(CONFIG['attack_mode'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d5947-45e8-4fae-9c2a-8241df2d548a",
   "metadata": {},
   "source": [
    "## 13: Loss Calculation\n",
    "\n",
    "Custom Loss Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a4efc-b63c-4ba9-97fb-d29ea0fe120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to get smooth textures\n",
    "def total_variation_loss(texture):\n",
    "    diff_h = texture[:, :, 1:, :] - texture[:, :, :-1, :]\n",
    "    diff_w = texture[:, :, :, 1:] - texture[:, :, :, :-1]\n",
    "    return (diff_h.pow(2).mean() + diff_w.pow(2).mean()) / 2\n",
    "\n",
    "#Try to get printable colors\n",
    "def nps_loss(texture, threshold=0.7):\n",
    "    #Compute saturation and brightness\n",
    "    max_ch = texture.max(dim=1)[0]\n",
    "    min_ch = texture.min(dim=1)[0]\n",
    "    saturation = (max_ch - min_ch) / (max_ch + 1e-8)\n",
    "    brightness = max_ch\n",
    "    \n",
    "    #Penalize when saturation * brightness > threshold\n",
    "    penalty = F.relu(saturation * brightness - threshold)\n",
    "    return penalty.mean()\n",
    "\n",
    "#Everything together\n",
    "def compute_total_loss(texture, detector, lambda_tv, lambda_nps, stage='stage2'):\n",
    "    #Detection loss (main objective)\n",
    "    loss_det = detector.compute_loss(texture)\n",
    "    \n",
    "    #Regularization\n",
    "    loss_tv = total_variation_loss(texture)\n",
    "    loss_nps = nps_loss(texture)\n",
    "    \n",
    "    total = loss_det + lambda_tv * loss_tv + lambda_nps * loss_nps\n",
    "    \n",
    "    return total, {\n",
    "        'total': total.item() if isinstance(total, torch.Tensor) else total,\n",
    "        'detection': loss_det.item() if isinstance(loss_det, torch.Tensor) else loss_det,\n",
    "        'tv': loss_tv.item(),\n",
    "        'nps': loss_nps.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cf65e-225e-43cf-82f7-0390b5b60c12",
   "metadata": {},
   "source": [
    "## 14: Stage 1: Generator Training\n",
    "\n",
    "Train the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e1d11-926e-4fb4-a190-166106a0c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(generator, aux_net, detector, dataset, config):\n",
    "\n",
    "    generator.train()\n",
    "    aux_net.train()\n",
    "    \n",
    "    #Optimizers\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    opt_aux = torch.optim.Adam(aux_net.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    \n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=config['stage1_batch_size'], shuffle=True, num_workers=4\n",
    "    )\n",
    "    \n",
    "    #Training loop\n",
    "    logger.info(\"Starting Stage 1 training...\")\n",
    "    \n",
    "    for epoch in range(config['stage1_epochs']):\n",
    "        epoch_losses = {'total': 0, 'detection': 0, 'tv': 0, 'nps': 0, 'mi': 0}\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config['stage1_epochs']}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            #Sample latent\n",
    "            z = torch.randn(config['stage1_batch_size'], 128, 9, 9, device=device)\n",
    "            \n",
    "            #Generate texture\n",
    "            texture = generator.generate(z)  # [0, 1]\n",
    "            \n",
    "            #Sample viewpoint and render\n",
    "            elev, azim, scale = renderer.sample_viewpoint()\n",
    "            rendered_hat, alpha = renderer.render(texture, elev, azim, scale)\n",
    "            \n",
    "            #Apply T-SEA augmentations\n",
    "            rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "            \n",
    "            #Apply URAdv augmentations\n",
    "            rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "            rendered_hat = add_shadows(rendered_hat, alpha)\n",
    "            rendered_hat = simulate_printing(rendered_hat)\n",
    "            \n",
    "            #Composite onto scene (using placeholder or real data)\n",
    "            if not dataset.use_placeholder:\n",
    "                scene = batch['image'].to(device)\n",
    "                hat_mask = batch['hat_mask'].to(device)\n",
    "                composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            else:\n",
    "                #For placeholder, just use rendered hat directly\n",
    "                composite = rendered_hat\n",
    "                \n",
    "            #Apply environmental augmentations\n",
    "            composite = constrained_augmentation(composite)\n",
    "            composite = apply_environmental_augmentation(composite)\n",
    "            \n",
    "            #Compute losses\n",
    "            #Detection loss\n",
    "            loss_det = detector.compute_loss(composite)\n",
    "            \n",
    "            #Regularization losses\n",
    "            loss_tv = total_variation_loss(texture)\n",
    "            loss_nps = nps_loss(texture, config['nps_threshold'])\n",
    "            \n",
    "            #Mutual information (maximize = negate for min)\n",
    "            loss_mi = compute_mi_loss(aux_net, texture, z)\n",
    "            \n",
    "            #Total loss\n",
    "            loss = (loss_det + \n",
    "                   config['lambda_tv'] * loss_tv + \n",
    "                   config['lambda_nps'] * loss_nps + \n",
    "                   config['lambda_info'] * loss_mi)  # loss_mi is already negated\n",
    "            \n",
    "            #Optimize\n",
    "            opt_g.zero_grad()\n",
    "            opt_aux.zero_grad()\n",
    "            \n",
    "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(aux_net.parameters(), 1.0)\n",
    "                opt_g.step()\n",
    "                opt_aux.step()\n",
    "            \n",
    "            #Track losses\n",
    "            epoch_losses['total'] += loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "            epoch_losses['detection'] += loss_det.item() if isinstance(loss_det, torch.Tensor) else loss_det\n",
    "            epoch_losses['tv'] += loss_tv.item()\n",
    "            epoch_losses['nps'] += loss_nps.item()\n",
    "            epoch_losses['mi'] += loss_mi.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\" if isinstance(loss, torch.Tensor) else f\"{loss:.4f}\",\n",
    "                'det': f\"{loss_det.item():.4f}\" if isinstance(loss_det, torch.Tensor) else f\"{loss_det:.4f}\"\n",
    "            })\n",
    "            \n",
    "        #Epoch summary\n",
    "        n_batches = len(dataloader)\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= n_batches\n",
    "            \n",
    "        logger.info(f\"Epoch {epoch+1} - Loss: {epoch_losses['total']:.4f}, \"\n",
    "                   f\"Det: {epoch_losses['detection']:.4f}, \"\n",
    "                   f\"TV: {epoch_losses['tv']:.4f}, \"\n",
    "                   f\"MI: {epoch_losses['mi']:.4f}\")\n",
    "        \n",
    "        #Save checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'aux_net': aux_net.state_dict(),\n",
    "                'epoch': epoch,\n",
    "            }, f\"{config['output_dir']}/stage1_epoch{epoch+1}.pth\")\n",
    "            \n",
    "            #Save sample texture\n",
    "            with torch.no_grad():\n",
    "                sample = generator.generate(batch_size=1)\n",
    "                save_texture(sample[0], f\"{config['output_dir']}/texture_epoch{epoch+1}.png\")\n",
    "                \n",
    "    return generator\n",
    "\n",
    "\n",
    "def save_texture(texture, path):\n",
    "    if isinstance(texture, torch.Tensor):\n",
    "        texture = texture.detach().cpu()\n",
    "        if texture.dim() == 3:\n",
    "            texture = texture.permute(1, 2, 0).numpy()\n",
    "        texture = (texture * 255).astype(np.uint8)\n",
    "    cv2.imwrite(str(path), cv2.cvtColor(texture, cv2.COLOR_RGB2BGR))\n",
    "    logger.info(f\"Saved texture to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db60a2-2d8a-4248-9b01-9df76cb4185d",
   "metadata": {},
   "source": [
    "## 15: Stage 2: Latent Optimization\n",
    "\n",
    "Optimize the Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c3ed1-8530-48e9-9a72-aae0cbbd04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(generator, detector, dataset, config, z_local=None):\n",
    "\n",
    "    generator.eval()  #Freeze generator\n",
    "    \n",
    "    #Initialize toroidal latent\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=9,\n",
    "        latent_channels=128,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    if z_local is not None:\n",
    "        toroidal.z_local.data = z_local\n",
    "        \n",
    "    #Optimizer for latent only\n",
    "    optimizer = torch.optim.Adam([toroidal.z_local], lr=config['stage2_lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config['stage2_iterations'], eta_min=1e-4\n",
    "    )\n",
    "    \n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=config['stage1_batch_size'], shuffle=True, num_workers=4\n",
    "    )\n",
    "    data_iter = iter(dataloader)\n",
    "    \n",
    "    logger.info(\"Starting Stage 2 latent optimization...\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(config['stage2_iterations']), desc=\"Stage 2\")\n",
    "    for iteration in pbar:\n",
    "        #Get batch (cycle through dataset)\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            batch = next(data_iter)\n",
    "            \n",
    "        #Get random crops from toroidal latent\n",
    "        z_crops = toroidal.get_random_crops(config['stage1_batch_size'])\n",
    "        \n",
    "        # Generate textures\n",
    "        with torch.no_grad():\n",
    "            #We need gradients through z, not generator\n",
    "            pass\n",
    "            \n",
    "        #Actually, we need gradients through generator for z\n",
    "        texture = generator.generate(z_crops)\n",
    "        \n",
    "        #Sample viewpoints and render\n",
    "        rendered_hats = []\n",
    "        alphas = []\n",
    "        for _ in range(config['stage1_batch_size']):\n",
    "            elev, azim, scale = renderer.sample_viewpoint()\n",
    "            rh, al = renderer.render(texture, elev, azim, scale)\n",
    "            rendered_hats.append(rh)\n",
    "            alphas.append(al)\n",
    "        rendered_hat = torch.cat(rendered_hats, dim=0)\n",
    "        alpha = torch.cat(alphas, dim=0)\n",
    "        \n",
    "        #Apply augmentations\n",
    "        rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "        rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "        rendered_hat = simulate_printing(rendered_hat)\n",
    "        \n",
    "        #Composite (placeholder mode)\n",
    "        composite = constrained_augmentation(rendered_hat)\n",
    "        composite = apply_environmental_augmentation(composite)\n",
    "        \n",
    "        #Compute loss (no MI term in Stage 2)\n",
    "        loss_det = detector.compute_loss(composite)\n",
    "        loss_tv = total_variation_loss(texture)\n",
    "        loss_nps = nps_loss(texture, config['nps_threshold'])\n",
    "        \n",
    "        loss = (loss_det + \n",
    "               config['lambda_tv'] * loss_tv + \n",
    "               config['lambda_nps'] * loss_nps)\n",
    "        \n",
    "        #Optimize\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        #Track best\n",
    "        loss_val = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_z_local = toroidal.z_local.data.clone()\n",
    "            \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss_val:.4f}\",\n",
    "            'best': f\"{best_loss:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
    "        })\n",
    "        \n",
    "        #Periodic logging\n",
    "        if (iteration + 1) % 200 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Generate final texture at full resolution\n",
    "                z_full = toroidal.get_full_latent(config['latent_size'])\n",
    "                final_texture = generator.generate(z_full)\n",
    "                save_texture(final_texture[0], \n",
    "                           f\"{config['output_dir']}/texture_stage2_iter{iteration+1}.png\")\n",
    "                \n",
    "    #Save final results\n",
    "    logger.info(f\"Stage 2 complete. Best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    #Generate final texture\n",
    "    with torch.no_grad():\n",
    "        z_full = toroidal.get_full_latent(config['latent_size'])\n",
    "        final_texture = generator.generate(z_full)\n",
    "        \n",
    "        # Resize to target output resolution\n",
    "        def tile_texture(tex, target):\n",
    "        _, _, h, w = tex.shape\n",
    "        reps = (target + h - 1) // h\n",
    "        tiled = tex.repeat(1, 1, reps, reps)\n",
    "        return tiled[:, :, :target, :target]\n",
    "    \n",
    "    final_texture = tile_texture(final_texture, 1024)\n",
    "        \n",
    "    save_texture(final_texture[0], f\"{config['output_dir']}/final_texture.png\")\n",
    "    \n",
    "    torch.save({\n",
    "        'z_local': best_z_local,\n",
    "        'generator': generator.state_dict(),\n",
    "    }, f\"{config['output_dir']}/stage2_final.pth\")\n",
    "    \n",
    "    return best_z_local, final_texture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203957cc-502f-45f4-94ad-5c4c9320c6d7",
   "metadata": {},
   "source": [
    "## 16: Evaluation\n",
    "\n",
    "See how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13817e3-c57f-4511-a7a6-733797dabb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_asr(detector, images, gt_boxes, conf_threshold=0.5, iou_threshold=0.5):\n",
    "\n",
    "    results = detector.detect(images, conf_threshold)\n",
    "    \n",
    "    n_success = 0\n",
    "    for model_name, model_results in results.items():\n",
    "        for i, det in enumerate(model_results):\n",
    "            gt_box = gt_boxes[i].cpu().numpy()\n",
    "            \n",
    "            detected = False\n",
    "            for box, score in zip(det['boxes'], det['scores']):\n",
    "                #Compute IoU\n",
    "                x1 = max(box[0], gt_box[0])\n",
    "                y1 = max(box[1], gt_box[1])\n",
    "                x2 = min(box[2], gt_box[2])\n",
    "                y2 = min(box[3], gt_box[3])\n",
    "                \n",
    "                inter = max(0, x2-x1) * max(0, y2-y1)\n",
    "                area1 = (box[2]-box[0]) * (box[3]-box[1])\n",
    "                area2 = (gt_box[2]-gt_box[0]) * (gt_box[3]-gt_box[1])\n",
    "                iou = inter / (area1 + area2 - inter + 1e-8)\n",
    "                \n",
    "                if iou >= iou_threshold:\n",
    "                    detected = True\n",
    "                    break\n",
    "                    \n",
    "            if not detected:\n",
    "                n_success += 1\n",
    "                \n",
    "    #Average across models\n",
    "    total = len(results) * len(gt_boxes)\n",
    "    return n_success / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_texture(generator, detector, dataset, z_local, config, num_samples=100):\n",
    "    generator.eval()\n",
    "    \n",
    "    #Create toroidal latent with optimized pattern\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=9,\n",
    "        device=device\n",
    "    )\n",
    "    toroidal.z_local.data = z_local\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=8, shuffle=False, num_workers=4\n",
    "    )\n",
    "    \n",
    "    asr_scores = {thresh: [] for thresh in [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            #Generate texture\n",
    "            z_crops = toroidal.get_random_crops(8)\n",
    "            texture = generator.generate(z_crops)\n",
    "            \n",
    "            #Render and composite\n",
    "            rendered_hat, alpha = renderer.render(texture, 85, 0, 1.0)\n",
    "            \n",
    "            #For placeholder, use rendered directly\n",
    "            composite = rendered_hat\n",
    "            \n",
    "            # Test at multiple thresholds\n",
    "            gt_boxes = batch.get('person_bbox', torch.zeros(8, 4))\n",
    "            for thresh in asr_scores.keys():\n",
    "                asr = compute_asr(detector, composite, gt_boxes, conf_threshold=thresh)\n",
    "                asr_scores[thresh].append(asr)\n",
    "                \n",
    "    #Compute mean ASR\n",
    "    mean_asr = {}\n",
    "    for thresh, scores in asr_scores.items():\n",
    "        mean_asr[thresh] = np.mean(scores)\n",
    "        logger.info(f\"ASR@{thresh}: {mean_asr[thresh]:.2%}\")\n",
    "        \n",
    "    overall_masr = np.mean(list(mean_asr.values()))\n",
    "    logger.info(f\"Mean ASR: {overall_masr:.2%}\")\n",
    "    \n",
    "    return mean_asr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cbc12-7460-42f4-a62e-3076cd91fdd0",
   "metadata": {},
   "source": [
    "## 17: Main\n",
    "\n",
    "Bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62395d-f498-4292-9c3b-49ffa50c131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize models\n",
    "    logger.info(\"Initializing models...\")\n",
    "    \n",
    "    generator = FCNGenerator(target_size=CONFIG['texture_size']).to(device)\n",
    "    aux_net = AuxiliaryNetwork().to(device)\n",
    "    \n",
    "    logger.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    logger.info(f\"Auxiliary params: {sum(p.numel() for p in aux_net.parameters()):,}\")\n",
    "    \n",
    "    #Stage 1: Train generator\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 1: Generator Training\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    generator = train_stage1(generator, aux_net, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Stage 2: Optimize latent\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 2: Latent Optimization\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    best_z_local, final_texture = train_stage2(generator, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Evaluation\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"EVALUATION\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    asr_results = evaluate_texture(generator, detector, dataset, best_z_local, CONFIG)\n",
    "    \n",
    "    #Final visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Final texture\n",
    "    axes[0].imshow(final_texture[0].permute(1, 2, 0).cpu())\n",
    "    axes[0].set_title('Final Adversarial Texture')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    #Rendered examples\n",
    "    with torch.no_grad():\n",
    "        rendered, alpha = renderer.render(final_texture, 85, 45, 1.0)\n",
    "    axes[1].imshow(rendered[0].permute(1, 2, 0).cpu())\n",
    "    axes[1].set_title('Rendered Hat (85, 45)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    #ASR plot\n",
    "    thresholds = list(asr_results.keys())\n",
    "    values = [asr_results[t] for t in thresholds]\n",
    "    axes[2].bar(range(len(thresholds)), values)\n",
    "    axes[2].set_xticks(range(len(thresholds)))\n",
    "    axes[2].set_xticklabels([f'{t}' for t in thresholds])\n",
    "    axes[2].set_xlabel('Confidence Threshold')\n",
    "    axes[2].set_ylabel('Attack Success Rate')\n",
    "    axes[2].set_title('ASR vs Threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/final_results.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(\"Training complete!\")\n",
    "    logger.info(f\"Final texture saved to: {CONFIG['output_dir']}/final_texture.png\")\n",
    "    \n",
    "    return generator, best_z_local, final_texture\n",
    "\n",
    "\n",
    "#Run if this is the main notebook\n",
    "if __name__ == \"__main__\":\n",
    "    generator, z_local, texture = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cloakhat)",
   "language": "python",
   "name": "cloakhat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
