{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1b43cd",
   "metadata": {},
   "source": [
    "# CloakHat Patch Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed60",
   "metadata": {},
   "source": [
    "## 1: Conda Setup\n",
    "\n",
    "Set up the environment\n",
    "\n",
    "`conda create -n cloakhat python=3.10 -y` <br>\n",
    "`conda activate cloakhat`\n",
    "\n",
    "PyTorch with CUDA <br>\n",
    "`conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y`\n",
    "\n",
    "PyTorch3D for differentiable rendering <br>\n",
    "`pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"`\n",
    "\n",
    "Detection models <br>\n",
    "`pip install ultralytics`\n",
    "\n",
    "Pip stuff <br>\n",
    "`pip install opencv-python-headless matplotlib tqdm tensorboard pyyaml trimesh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc07834",
   "metadata": {},
   "source": [
    "## 2: Python Setup\n",
    "\n",
    "Get the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346291c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (look_at_view_transform, FoVPerspectiveCameras, RasterizationSettings, MeshRenderer, MeshRasterizer, SoftPhongShader, TexturesUV, PointLights)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c892",
   "metadata": {},
   "source": [
    "## 3: Config\n",
    "\n",
    "Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_dir': './data/drone_footage', #Drone footage\n",
    "    'mesh_path': './assets/hat.obj', #Hat meshes\n",
    "    'output_dir': './outputs',\n",
    "    \n",
    "    #Generator\n",
    "    'latent_channels': 128,\n",
    "    'latent_size': 9, #Spatial size of latent input\n",
    "    'texture_size': 288, #Output texture size from generator\n",
    "    \n",
    "    #Viewpoint sampling\n",
    "    'elevation_range': (60, 90), #Degrees from horizontal (90 = overhead)\n",
    "    'scale_range': (0.3, 1.2), #Altitude proxy\n",
    "    \n",
    "    #Training Stage 1\n",
    "    'stage1_epochs': 100,\n",
    "    'stage1_batch_size': 8,\n",
    "    'stage1_lr': 2e-4,\n",
    "    \n",
    "    #Training Stage 2  \n",
    "    'stage2_iterations': 2000,\n",
    "    'stage2_lr': 0.01,\n",
    "    'local_latent_size': 18, #Size of optimizable latent pattern\n",
    "    \n",
    "    #Loss weights\n",
    "    'lambda_tv': 2.5, #Total variation\n",
    "    'lambda_nps': 0.01, #Non-printability score\n",
    "    'lambda_info': 0.1, #Mutual information (Stage 1 only)\n",
    "    \n",
    "    #T-SEA Stuff\n",
    "    'cutout_prob': 0.9,\n",
    "    'cutout_ratio': 0.4,\n",
    "    'shakedrop_prob': 0.5,\n",
    "    \n",
    "    #Rendering\n",
    "    'render_size': 256,\n",
    "    \n",
    "    #Printing (PLACEHOLDER, need details from FABLAB)\n",
    "    'nps_threshold': 0.7,  #Saturation * brightness threshold\n",
    "    \n",
    "    #Attack config (white, gray, black)\n",
    "    'attack_mode': 'gray',\n",
    "}\n",
    "\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1445b0",
   "metadata": {},
   "source": [
    "## 4: FCN Generator\n",
    "\n",
    "Make the texture (turn noise into an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            #9 -> 9\n",
    "            nn.Conv2d(latent_channels, 512, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            #9 -> 18\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #18 -> 36\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 36 -> 72\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 72 -> 144\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 144 -> 288\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 288 -> 288 (to RGB)\n",
    "            nn.Conv2d(32, 3, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.output_size = 288\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "    \n",
    "    def generate(self, z=None, batch_size=1):\n",
    "        if z is None:\n",
    "            z = torch.randn(batch_size, 128, 9, 9, device=next(self.parameters()).device)\n",
    "        return (self.forward(z) + 1) / 2\n",
    "\n",
    "# Test\n",
    "generator = FCNGenerator().to(device)\n",
    "test_out = generator.generate(batch_size=1)\n",
    "logger.info(f\"Generator output: {test_out.shape}\")  #Should be (1, 3, 288, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec701a8",
   "metadata": {},
   "source": [
    "## 5: Auxiliary Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryNetwork(nn.Module):\n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Texture encoder\n",
    "        self.tex_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Latent encoder\n",
    "        self.lat_enc = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 256, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Joint network\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texture, z):\n",
    "        tex_feat = self.tex_enc(texture)\n",
    "        lat_feat = self.lat_enc(z)\n",
    "        return self.joint(torch.cat([tex_feat, lat_feat], dim=1))\n",
    "\n",
    "\n",
    "def compute_mi_loss(aux_net, texture, z):\n",
    "\n",
    "    #Matched pairs\n",
    "    T_joint = aux_net(texture, z)\n",
    "    pos_term = -F.softplus(-T_joint).mean()\n",
    "    \n",
    "    #Mismatched pairs (shuffle z)\n",
    "    z_shuffle = z[torch.randperm(z.size(0))]\n",
    "    T_marginal = aux_net(texture, z_shuffle)\n",
    "    neg_term = F.softplus(T_marginal).mean()\n",
    "    \n",
    "    mi = pos_term - neg_term\n",
    "    return -mi  #Negate because we minimize loss but want to maximize MI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
