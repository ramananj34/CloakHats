{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acba844-1e05-4948-82a5-43edc05a995e",
   "metadata": {},
   "source": [
    "What is left: \n",
    "* Drone footage\n",
    "* Calibrate altitude_to_scale in annotations metadata\n",
    "* Replace COCO with Fine Tuned VISDRONE\n",
    "* Measure UV unwrapped hat, set mesh_unit_to_inches in CONFIG\n",
    "* Get printer specs, set DPI and gamut values in CONFIG\n",
    "* Create printer Gamut.numpy\n",
    "* Add more diverse models to blackbox: RT-DETR, YOLO-NAS, or a Faster R-CNN\n",
    "* Grid Search Tier 1: lambda_tv/lambda_nps/lambda_info. Tier 2: stage2_lr, local_latent_size, cutout_ratio, shakedrop_prob. score = mean_asr  (subject to: final_nps < threshold AND final_tv < threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b43cd",
   "metadata": {},
   "source": [
    "# CloakHat Patch Generation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ed60",
   "metadata": {},
   "source": [
    "## 1: Conda Setup\n",
    "\n",
    "* Conda is the main env manager, pip is for Python packages\n",
    "* PyTorch is the main AI/ML library\n",
    "* NVIDIA CUDA is for GPU acceleration\n",
    "* PyTorch3D is for rendering the hat\n",
    "* ipykernel allows JupyterLab to use the Conda env\n",
    "* Ultralytics has YOLO models\n",
    "* opencv-python-headless is for image processing\n",
    "* matplotlib is for plots\n",
    "* tqdm is for progress bars\n",
    "* NumPy is for data manipulation\n",
    "\n",
    "### Option 1:\n",
    "\n",
    "Activate <br>\n",
    "`conda env create -f environment.yaml` <br>\n",
    "`conda activate cloakhat` <br>\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "### Option 2: \n",
    "\n",
    "Set up the environment\n",
    "\n",
    "`conda create -n cloakhat python=3.10 -y` <br>\n",
    "`conda activate cloakhat`\n",
    "\n",
    "PyTorch with CUDA. Also ipykernel. <br>\n",
    "`conda install pytorch ipykernel pytorch-cuda=11.8 -c pytorch -c nvidia -y`<br>\n",
    "\n",
    "PyTorch3D for differentiable rendering <br>\n",
    "`conda install -c pytorch3d pytorch3d -y`\n",
    "\n",
    "Detection models <br>\n",
    "`pip install ultralytics`\n",
    "\n",
    "Other stuff <br>\n",
    "`pip install opencv-python-headless matplotlib tqdm numpy`\n",
    "\n",
    "Apply the kernel <br>\n",
    "`python -m ipykernel install --user --name cloakhat --display-name \"Python (cloakhat)\"`\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>\n",
    "\n",
    "### Option 3: \n",
    "\n",
    "Run the bash <br>\n",
    "`bash LabSetup.sh`\n",
    "\n",
    "Activate the kernel <br>\n",
    "`conda activate cloakhat` <br>\n",
    "\n",
    "Deactivate <br>\n",
    "`conda deactivate` <br>\n",
    "`conda env remove -n cloakhat` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc07834",
   "metadata": {},
   "source": [
    "## 2: Python Setup\n",
    "\n",
    "Get the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346291c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Data manipulation\n",
    "import numpy as np\n",
    "\n",
    "#Image processing\n",
    "import cv2\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Working with the file system\n",
    "from pathlib import Path\n",
    "\n",
    "#Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Better logging than print statements\n",
    "import logging\n",
    "\n",
    "#JSON utilities\n",
    "import json\n",
    "\n",
    "#PyTorch3d utilities\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (look_at_view_transform, FoVPerspectiveCameras, RasterizationSettings, MeshRasterizer, SoftPhongShader, TexturesUV, PointLights)\n",
    "\n",
    "#Gets YOLO models\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#Logging with timestamps\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Check what device is being used (especially if we want GPU) and log it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210c892",
   "metadata": {},
   "source": [
    "## 3: Config\n",
    "\n",
    "Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'dataset_dir': './data/drone_footage', #Drone footage/image samples\n",
    "    'mesh_path': './assets/Bucket_Hat.obj', #Hat mesh\n",
    "    'output_dir': './outputs2', #Where out outputs will go (textures, evaluations, stuff like that)\n",
    "    \n",
    "    #Generator\n",
    "    'latent_channels': 128, #Channels\n",
    "    'latent_size': 9, #Spatial size of latent input\n",
    "    #(so the latent is 128x9x9)\n",
    "    'texture_size': 288, #Output texture size from generator\n",
    "    #(so the latent becomes a texture that is 3x288x288)\n",
    "    \n",
    "    #Viewpoint sampling\n",
    "    'scale_jitter': 0.1, #Fraction of scale variation\n",
    "    'camera_pitch_jitter': 5.0, #Alias for elevation jitter (degrees)\n",
    "    'heading_jitter': 180, #Alias for azimuth jitter (degrees)\n",
    "\n",
    "    'num_workers': 8, #for DataLoader\n",
    "    'det_conf_floor': 0.001, #Minimum confidence for detector loss\n",
    "    \n",
    "    #Training Stage 1\n",
    "    'stage1_epochs': 150, #100 epochs\n",
    "    'stage1_batch_size': 8, #8 batch minibatch gradient descent\n",
    "    'stage1_lr': 2e-4, #learning rate\n",
    "    \n",
    "    #Training Stage 2  \n",
    "    'stage2_batch_size': 4,\n",
    "    'stage2_iterations': 5000, #Now we optimize the single tensor\n",
    "    'stage2_lr': 0.005, #Bigger learning rate\n",
    "    'stage2_generator_lr': 5e-5,\n",
    "    'local_latent_size': 9, #Size of optimizable latent pattern. Bigger than 9x9 (input), so tile seamlessly\n",
    "    \n",
    "    #Loss weights\n",
    "    'lambda_tv': .5, #Total variation - makes the textures smoother/less noisy/able to be printed\n",
    "    'lambda_nps': 0.01, #Non-printability score - penalize colors that can't print well\n",
    "    'lambda_info': 0.5, #Mutual information (Stage 1 only) - ensures latent is correlated to the texture\n",
    "    \n",
    "    #T-SEA Stuff\n",
    "    'cutout_prob': 0.9, #90% of the time, \n",
    "    'cutout_ratio': 0.4, #cut off 40% of the hat\n",
    "    'shakedrop_prob': 0.5, #50% of the time, mess with the model (for self-ensemble)\n",
    "    \n",
    "    #Rendering\n",
    "    'render_size': 512, #Output 256x256 images\n",
    "    'image_size': 1920, #Standardized input image size for all frames\n",
    "    'detector_input_size': 640,\n",
    "    \n",
    "    #Printer specifications (GET FROM FABLAB)\n",
    "    'printer': {\n",
    "        'dpi': 300,\n",
    "        'mesh_unit_to_inches': None, #CALIBRATE: how many inches = 1 unit in the .obj file. None to auto-estimate.\n",
    "        'seam_allowance_inches': 0.25, #Extra border for cutting\n",
    "        'max_saturation': 0.85,\n",
    "        'max_brightness': 0.95,\n",
    "        'min_brightness': 0.08,\n",
    "        'nps_threshold': 0.85, #Saturation * brightness threshold (penalize when saturation × brightness > 0.7)\n",
    "        'gamut_samples_path': './assets/printer_gamut.npy',\n",
    "    },\n",
    "    \n",
    "    #Attack config (white, gray, black)\n",
    "    'attack_mode': 'white',\n",
    "\n",
    "    #SAHI evaluation settings\n",
    "    'sahi_slice_size': 960,\n",
    "    'sahi_overlap': 0.3,\n",
    "}\n",
    "\n",
    "#Make sure the folder exists\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ae201-d285-4de7-9313-dea9d81753c3",
   "metadata": {},
   "source": [
    "## 4: Dataset Preparation\n",
    "\n",
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ae3ab-ca07-4d00-83ec-92db0313ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset will look like\n",
    "\"\"\"\n",
    "dataset_dir/\n",
    "        frames/\n",
    "            frame_0001.png\n",
    "            frame_0002.png\n",
    "            ...\n",
    "        masks/\n",
    "            frame_0001_mask.png  (binary mask of green hat region)\n",
    "            ...\n",
    "        annotations.json  (person bounding boxes, metadata)\n",
    "\n",
    "Annotations.json:\n",
    "{\n",
    "    \"frames\": [\n",
    "        {\n",
    "            \"frame_id\": \"frame_0001\",\n",
    "            \"image_path\": \"frames/frame_0001.png\",\n",
    "            \"mask_path\": \"masks/frame_0001_mask.png\",\n",
    "            \"person_bbox\": [x1, y1, x2, y2],\n",
    "            \"drone\": {\n",
    "                \"camera_pitch\": 82.5,\n",
    "                \"heading\": 45.0,\n",
    "                \"altitude_meters\": 15.0\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"altitude_to_scale\": {\n",
    "            \"min_altitude\": 5.0,\n",
    "            \"max_altitude\": 50.0,\n",
    "            \"min_scale\": 0.3,\n",
    "            \"max_scale\": 1.2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "#Dataset of the drone images\n",
    "class DroneDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    #Loads annotations\n",
    "    def __init__(self, dataset_dir):\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        #Makes sure placeholder exists\n",
    "        if not self.dataset_dir.exists():\n",
    "            raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n",
    "        #Load annotations\n",
    "        annotations_path = self.dataset_dir / 'annotations.json'\n",
    "        if not annotations_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing annotations.json in {dataset_dir}\")\n",
    "        #Load the JSON\n",
    "        with open(annotations_path) as f:\n",
    "            self.annotations = json.load(f)\n",
    "        #Get the lengths/number of samples\n",
    "        self.frames = self.annotations['frames']\n",
    "        self.length = len(self.frames)\n",
    "        #Altitude to scale conversion params\n",
    "        meta = self.annotations['metadata']['altitude_to_scale']\n",
    "        self.alt_min = meta['min_altitude']\n",
    "        self.alt_max = meta['max_altitude']\n",
    "        self.scale_min = meta['min_scale']\n",
    "        self.scale_max = meta['max_scale']\n",
    "        logger.info(f\"Loaded {self.length} frames from {dataset_dir}\")\n",
    "\n",
    "    #Convert altitutde to scale - needs to be tuned\n",
    "    def _altitude_to_scale(self, altitude_meters):\n",
    "        t = (altitude_meters - self.alt_min) / (self.alt_max - self.alt_min)\n",
    "        t = np.clip(t, 0, 1)\n",
    "        scale = self.scale_max - t * (self.scale_max - self.scale_min)\n",
    "        return scale\n",
    "\n",
    "    #Gets the number of images\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    #Get a sample\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        #Load image\n",
    "        image_path = self.dataset_dir / frame['image_path']\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "        target_size = CONFIG['image_size']\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        image = F.interpolate(image.unsqueeze(0), size=(target_size, target_size), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        #Load mask\n",
    "        mask_path = self.dataset_dir / frame['mask_path']\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float() / 255.0\n",
    "        mask = F.interpolate(mask.unsqueeze(0), size=(target_size, target_size), mode='nearest').squeeze(0)\n",
    "        #Bounding box (scaled to match resized image)\n",
    "        person_bbox = torch.tensor(frame['person_bbox'], dtype=torch.float32)\n",
    "        person_bbox[0] *= target_size / orig_w\n",
    "        person_bbox[1] *= target_size / orig_h\n",
    "        person_bbox[2] *= target_size / orig_w\n",
    "        person_bbox[3] *= target_size / orig_h\n",
    "        #Viewpoint\n",
    "        drone = frame['drone']\n",
    "        elevation = drone['camera_pitch']\n",
    "        azimuth = drone.get('heading', np.random.uniform(0, 360))  #Randomize if not provided\n",
    "        scale = self._altitude_to_scale(drone['altitude_meters'])\n",
    "        return {\n",
    "            'image': image,\n",
    "            'hat_mask': mask,\n",
    "            'person_bbox': person_bbox,\n",
    "            'elevation': torch.tensor(elevation, dtype=torch.float32),\n",
    "            'azimuth': torch.tensor(azimuth, dtype=torch.float32),\n",
    "            'scale': torch.tensor(scale, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "#Pre-processing utility\n",
    "def segment_green_hat(frame):\n",
    "    #Convert to HSV\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    #Narrow lime chroma green range in HSV\n",
    "    lower_green = np.array([50, 150, 150])\n",
    "    upper_green = np.array([70, 255, 255])\n",
    "    #Calculate the mask\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    #Clean up mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    #Dilate to recover edges lost to shadows/curves\n",
    "    dilate_kernel = np.ones((15, 15), np.uint8)\n",
    "    mask = cv2.dilate(mask, dilate_kernel, iterations=1)\n",
    "    return mask\n",
    "\n",
    "#Create dataset\n",
    "dataset = DroneDataset(CONFIG['dataset_dir'])\n",
    "logger.info(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1445b0",
   "metadata": {},
   "source": [
    "## 5: FCN Generator\n",
    "\n",
    "Make the texture (turn noise into an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d799e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_channels=128, latent_size=9):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        #Core network, upsample throughout, zero padding for invaraince, and LeakyReLU activation\n",
    "        self.net = nn.Sequential(\n",
    "            #9 -> 9\n",
    "            nn.Conv2d(latent_channels, 512, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),     \n",
    "            #9 -> 18\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #18 -> 36\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #36 -> 72\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #72 -> 144\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #144 -> 288\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #288 -> 288 (to RGB)\n",
    "            nn.Conv2d(32, 3, 3, 1, 1, padding_mode='zeros'),\n",
    "            nn.Tanh() #Squash\n",
    "        )\n",
    "        self.output_size = 288\n",
    "        self._init_weights()\n",
    "\n",
    "    #Initialize the weights\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2, nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    #Standard forward method\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "    #Create random noise for testing\n",
    "    def generate(self, z=None, batch_size=1):\n",
    "        if z is None:\n",
    "            z = torch.randn(batch_size, self.net[0].in_channels, self.latent_size, self.latent_size, device=next(self.parameters()).device)\n",
    "        return (self.forward(z) + 1) / 2\n",
    "\n",
    "#Test\n",
    "generator = FCNGenerator(latent_channels=CONFIG['latent_channels'], latent_size=CONFIG['latent_size']).to(device)\n",
    "test_texture = generator.generate(batch_size=1)\n",
    "#Make sure it is the correct size\n",
    "assert test_texture.shape[-1] == CONFIG['texture_size'], f\"Generator outputs {test_texture.shape[-1]}px but config expects {CONFIG['texture_size']}px\"\n",
    "logger.info(f\"Generator output: {test_texture.shape}\")  #Should be (1, 3, 288, 288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec701a8",
   "metadata": {},
   "source": [
    "## 6: Auxiliary Network\n",
    "\n",
    "Forces the texture to derive from the latent (JSD MINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Texture encoder\n",
    "        self.tex_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Latent encoder\n",
    "        self.lat_enc = nn.Sequential(\n",
    "            nn.Conv2d(latent_channels, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        #Joint network\n",
    "        self.joint = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texture, z):\n",
    "        tex_feat = self.tex_enc(texture)\n",
    "        lat_feat = self.lat_enc(z)\n",
    "        return self.joint(torch.cat([tex_feat, lat_feat], dim=1))\n",
    "\n",
    "def compute_mi_loss(aux_net, texture, z):\n",
    "    #Matched pairs\n",
    "    T_joint = aux_net(texture, z)\n",
    "    pos_term = -F.softplus(-T_joint).mean()\n",
    "    #Mismatched pairs (shuffle z)\n",
    "    z_shuffle = z[torch.randperm(z.size(0))]\n",
    "    T_marginal = aux_net(texture, z_shuffle)\n",
    "    neg_term = F.softplus(T_marginal).mean()\n",
    "    mi = pos_term - neg_term\n",
    "    #MI ≥ E_joint[-softplus(-T)] - E_marginal[softplus(T)]\n",
    "    return -mi  #Negate because we minimize loss but want to maximize MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb40bb",
   "metadata": {},
   "source": [
    "## 7: Render Hat\n",
    "\n",
    "Render the hat using the texture and capture angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatRenderer:\n",
    "    \n",
    "    def __init__(self, mesh_path, render_size=256, device='cuda'):\n",
    "        self.device = device\n",
    "        self.render_size = render_size\n",
    "        #Load mesh - getting the verts, faces, and uv mappings\n",
    "        self.mesh_loaded = False\n",
    "        \"\"\"\n",
    "        CORRECT VERSION:\n",
    "        if Path(mesh_path).exists():\n",
    "            verts, faces, aux = load_obj(mesh_path, device=device)\n",
    "            self.verts = verts\n",
    "            self.faces = faces.verts_idx\n",
    "            self.verts_uvs = aux.verts_uvs\n",
    "            self.faces_uvs = faces.textures_idx\n",
    "            self.mesh_loaded = True\n",
    "            logger.info(f\"Loaded mesh: {len(verts)} verts, {len(self.faces)} faces\")\n",
    "        \"\"\"\n",
    "        if Path(mesh_path).exists():\n",
    "            verts, faces, aux = load_obj(mesh_path, device=device)\n",
    "            center = (verts.max(0)[0] + verts.min(0)[0]) / 2\n",
    "            verts = verts - center\n",
    "            max_extent = (verts.max(0)[0] - verts.min(0)[0]).max()\n",
    "            verts = verts / max_extent  # now fits in [-0.5, 0.5]\n",
    "            self.verts = verts\n",
    "            self.faces = faces.verts_idx\n",
    "            self.verts_uvs = aux.verts_uvs\n",
    "            self.faces_uvs = faces.textures_idx\n",
    "            self.mesh_loaded = True\n",
    "            logger.info(f\"Loaded mesh: {len(verts)} verts, {len(self.faces)} faces (normalized from {max_extent:.1f} units)\")\n",
    "        else:\n",
    "            #Use the placeholder\n",
    "            logger.warning(f\"Mesh not found at {mesh_path}. Using placeholder.\")\n",
    "            self._create_placeholder_mesh()\n",
    "        #Rasterization settings\n",
    "        self.raster_settings = RasterizationSettings(\n",
    "            image_size=render_size, \n",
    "            blur_radius=0,\n",
    "            faces_per_pixel=1\n",
    "        )\n",
    "        #Create rasterizer once\n",
    "        self.rasterizer = MeshRasterizer(raster_settings=self.raster_settings)\n",
    "\n",
    "    #Make a placeholder if we don't have a mesh which is a disk.\n",
    "    def _create_placeholder_mesh(self):\n",
    "        #Simple disk\n",
    "        n_points = 32\n",
    "        angles = torch.linspace(0, 2*np.pi, n_points+1)[:-1]\n",
    "        #Vertices: center + rim\n",
    "        verts = [[0, 0, 0]]  #center\n",
    "        for a in angles:\n",
    "            verts.append([torch.cos(a).item(), torch.sin(a).item(), 0])\n",
    "        self.verts = torch.tensor(verts, dtype=torch.float32, device=self.device)\n",
    "        #Faces: triangles from center to rim\n",
    "        faces = []\n",
    "        for i in range(n_points):\n",
    "            faces.append([0, i + 1, (i + 1) % n_points + 1])\n",
    "        self.faces = torch.tensor(faces, dtype=torch.int64, device=self.device)\n",
    "        #UVs: simple radial mapping\n",
    "        uvs = [[0.5, 0.5]]  # center\n",
    "        for a in angles:\n",
    "            uvs.append([0.5 + 0.5*torch.cos(a).item(), 0.5 + 0.5*torch.sin(a).item()])\n",
    "        self.verts_uvs = torch.tensor(uvs, dtype=torch.float32, device=self.device)\n",
    "        self.faces_uvs = self.faces.clone()\n",
    "        self.mesh_loaded = True\n",
    "\n",
    "    #Now we render the texture on the hat\n",
    "    def render(self, texture, elevation=90, azimuth=0, scale=1.0):\n",
    "        batch_size = texture.shape[0]\n",
    "        #Handle both scalar and per-sample scale (so scale the geometry)\n",
    "        if isinstance(scale, (int, float)):\n",
    "            verts = self.verts * scale\n",
    "            verts_list = [verts] * batch_size\n",
    "        elif isinstance(scale, torch.Tensor) and scale.dim() == 0:\n",
    "            verts = self.verts * scale.item()\n",
    "            verts_list = [verts] * batch_size\n",
    "        else:\n",
    "            verts_list = [self.verts * s.item() for s in scale]\n",
    "        #Camera setup\n",
    "        dist = 2.5  #Camera distance\n",
    "        if not isinstance(elevation, torch.Tensor):\n",
    "            elevation = torch.tensor([elevation], device=self.device).expand(batch_size)\n",
    "            azimuth = torch.tensor([azimuth], device=self.device).expand(batch_size)\n",
    "        elif elevation.dim() == 0:\n",
    "            elevation = elevation.unsqueeze(0).expand(batch_size)\n",
    "            azimuth = azimuth.unsqueeze(0).expand(batch_size)\n",
    "        R, T = look_at_view_transform(dist=dist, elev=elevation, azim=azimuth, device=self.device)\n",
    "        cameras = FoVPerspectiveCameras(R=R, T=T, device=self.device)\n",
    "        \"\"\"\n",
    "        #Random Lighting (varying lighting)\n",
    "        light_x = np.random.uniform(-1, 1)\n",
    "        light_y = np.random.uniform(1, 3)  #Always somewhat above\n",
    "        light_z = np.random.uniform(-1, 1)\n",
    "        lights = PointLights(\n",
    "            device=self.device, \n",
    "            location=[[light_x, light_y, light_z]],\n",
    "            ambient_color=[[0.5, 0.5, 0.5]],\n",
    "            diffuse_color=[[0.3, 0.3, 0.3]],\n",
    "            specular_color=[[0.2, 0.2, 0.2]]\n",
    "        )\n",
    "        \"\"\"\n",
    "        lights = PointLights(\n",
    "            device=self.device, \n",
    "            location=[[0.0, 2.0, 0.0]],\n",
    "            ambient_color=[[0.85, 0.85, 0.85]],\n",
    "            diffuse_color=[[0.1, 0.1, 0.1]],\n",
    "            specular_color=[[0.05, 0.05, 0.05]]\n",
    "        )\n",
    "        #Create shader once per render call (lighting changes)\n",
    "        shader = SoftPhongShader(device=self.device, cameras=cameras, lights=lights)\n",
    "        #Create batched texture and bind to mesh\n",
    "        tex_maps = texture.permute(0, 2, 3, 1)  #(B, H, W, 3)\n",
    "        textures = TexturesUV(\n",
    "            maps=tex_maps,\n",
    "            faces_uvs=[self.faces_uvs] * batch_size,\n",
    "            verts_uvs=[self.verts_uvs] * batch_size\n",
    "        )\n",
    "        #Assemble and render\n",
    "        meshes = Meshes(\n",
    "            verts=verts_list,\n",
    "            faces=[self.faces] * batch_size,\n",
    "            textures=textures\n",
    "        )\n",
    "        #Render entire batch at once - 2-phase raster than shade\n",
    "        fragments = self.rasterizer(meshes, cameras=cameras)\n",
    "        images = shader(fragments, meshes, cameras=cameras, lights=lights)\n",
    "        #Match PyTorch convention, format output\n",
    "        rendered_images = images[..., :3].permute(0, 3, 1, 2)\n",
    "        alpha_masks = images[..., 3:4].permute(0, 3, 1, 2)\n",
    "        #Return result\n",
    "        return rendered_images, alpha_masks\n",
    "\n",
    "def analyze_mesh_for_printing(mesh_path, config, preloaded=None):\n",
    "    from collections import defaultdict, deque\n",
    "    _p = config['printer']\n",
    "    #Check if the mesh is here\n",
    "    if not Path(mesh_path).exists():\n",
    "        logger.warning(\"No mesh file — defaulting to 12x12 inch print area\")\n",
    "        config['physical_size_inches'] = (12.0, 12.0)\n",
    "        config['texture_output_size_w'] = int(_p['dpi'] * 12)\n",
    "        config['texture_output_size_h'] = int(_p['dpi'] * 12)\n",
    "        config['uv_islands'] = []\n",
    "        return\n",
    "    #Check if we have already parsed the .obj\n",
    "    if preloaded is not None:\n",
    "        verts_np = preloaded['verts'].cpu().numpy()\n",
    "        verts_uvs = preloaded['verts_uvs'].cpu().numpy()\n",
    "        faces_verts = preloaded['faces_verts'].cpu().numpy()\n",
    "        faces_uvs = preloaded['faces_uvs'].cpu().numpy()\n",
    "    else:\n",
    "        verts, faces, aux = load_obj(mesh_path)\n",
    "        verts_np = verts.cpu().numpy()\n",
    "        verts_uvs = aux.verts_uvs.cpu().numpy()\n",
    "        faces_verts = faces.verts_idx.cpu().numpy()\n",
    "        faces_uvs = faces.textures_idx.cpu().numpy()\n",
    "    #Auto-callibration fallback\n",
    "    if _p['mesh_unit_to_inches'] is None:\n",
    "        x_span = verts_np[:, 0].max() - verts_np[:, 0].min()\n",
    "        y_span = verts_np[:, 1].max() - verts_np[:, 1].min()\n",
    "        z_span = verts_np[:, 2].max() - verts_np[:, 2].min()\n",
    "        max_span = max(x_span, y_span, z_span)\n",
    "        _p['mesh_unit_to_inches'] = 10.0 / max_span if max_span > 0 else 1.0\n",
    "        logger.warning(f\"Auto-estimated mesh scale: {_p['mesh_unit_to_inches']:.3f} in/unit \" f\"(mesh widest: {max_span:.3f} units -> ~10 inches)\")\n",
    "        logger.warning(\"SET printer.mesh_unit_to_inches manually for accuracy\")\n",
    "    scale = _p['mesh_unit_to_inches']\n",
    "    #Adjacency map\n",
    "    uv_vert_to_faces = defaultdict(set)\n",
    "    for fi, face in enumerate(faces_uvs):\n",
    "        for vi in face:\n",
    "            uv_vert_to_faces[vi].add(fi)\n",
    "    #BFS\n",
    "    visited = set()\n",
    "    islands = []\n",
    "    for fi in range(len(faces_uvs)):\n",
    "        if fi in visited:\n",
    "            continue\n",
    "        island = []\n",
    "        queue = deque([fi])\n",
    "        while queue:\n",
    "            f = queue.popleft()\n",
    "            if f in visited:\n",
    "                continue\n",
    "            visited.add(f)\n",
    "            island.append(f)\n",
    "            for vi in faces_uvs[f]:\n",
    "                for neighbor in uv_vert_to_faces[vi]:\n",
    "                    if neighbor not in visited:\n",
    "                        queue.append(neighbor)\n",
    "        islands.append(island)\n",
    "    #Measure the size of the islands\n",
    "    island_info = []\n",
    "    for idx, island_faces in enumerate(islands):\n",
    "        island_uv_indices = set()\n",
    "        for fi in island_faces:\n",
    "            for vi in faces_uvs[fi]:\n",
    "                island_uv_indices.add(vi)\n",
    "        island_uvs_arr = verts_uvs[list(island_uv_indices)]\n",
    "        u_min, v_min = island_uvs_arr.min(axis=0)\n",
    "        u_max, v_max = island_uvs_arr.max(axis=0)\n",
    "        #Triangle area via cross product - sum the area\n",
    "        total_area_3d = 0.0\n",
    "        for fi in island_faces:\n",
    "            v0 = verts_np[faces_verts[fi][0]]\n",
    "            v1 = verts_np[faces_verts[fi][1]]\n",
    "            v2 = verts_np[faces_verts[fi][2]]\n",
    "            total_area_3d += 0.5 * np.linalg.norm(np.cross(v1 - v0, v2 - v0))\n",
    "        #Ratio between 3D surface and UV area\n",
    "        physical_area = total_area_3d * (scale ** 2)\n",
    "        #Store measurements on the islands\n",
    "        island_info.append({\n",
    "            'index': idx,\n",
    "            'num_faces': len(island_faces),\n",
    "            'uv_bounds': (u_min, v_min, u_max, v_max),\n",
    "            'physical_area_sq_inches': physical_area,\n",
    "        })\n",
    "        logger.info(f\"  Island {idx}: {len(island_faces)} faces, ~{physical_area:.1f} sq in\")\n",
    "    #Overall print dimensions\n",
    "    config['uv_islands'] = island_info\n",
    "    logger.info(f\"Found {len(islands)} UV island(s)\")\n",
    "    #Total area\n",
    "    total_area = sum(i['physical_area_sq_inches'] for i in island_info)\n",
    "    all_u_min = min(i['uv_bounds'][0] for i in island_info)\n",
    "    all_v_min = min(i['uv_bounds'][1] for i in island_info)\n",
    "    all_u_max = max(i['uv_bounds'][2] for i in island_info)\n",
    "    all_v_max = max(i['uv_bounds'][3] for i in island_info)\n",
    "    uv_width = all_u_max - all_u_min\n",
    "    uv_height = all_v_max - all_v_min\n",
    "    #Bounding box uv area for printing\n",
    "    sa = _p['seam_allowance_inches']\n",
    "    #Average 3D-to-UV scale: compare total 3D area to total UV area\n",
    "    total_uv_area = 0.0\n",
    "    for island_faces_idx in islands:\n",
    "        for fi in island_faces_idx:\n",
    "            uv0 = verts_uvs[faces_uvs[fi][0]]\n",
    "            uv1 = verts_uvs[faces_uvs[fi][1]]\n",
    "            uv2 = verts_uvs[faces_uvs[fi][2]]\n",
    "            total_uv_area += 0.5 * abs(np.cross(uv1 - uv0, uv2 - uv0))\n",
    "    #physical_area / uv_area = (scale_per_uv_unit)^2\n",
    "    scale_per_uv = np.sqrt(total_area / (total_uv_area + 1e-8))\n",
    "    phys_w = uv_width * scale_per_uv + 2 * sa\n",
    "    phys_h = uv_height * scale_per_uv + 2 * sa\n",
    "    #Store info\n",
    "    config['physical_size_inches'] = (phys_w, phys_h)\n",
    "    config['texture_output_size_w'] = int(_p['dpi'] * phys_w)\n",
    "    config['texture_output_size_h'] = int(_p['dpi'] * phys_h)\n",
    "    #Log the info\n",
    "    logger.info(f\"Print size: {phys_w:.1f} x {phys_h:.1f} inches \" f\"({config['texture_output_size_w']}x{config['texture_output_size_h']}px @ {_p['dpi']} DPI)\")\n",
    "\n",
    "#Test renderer\n",
    "renderer = HatRenderer(CONFIG['mesh_path'], CONFIG['render_size'], device)\n",
    "#Do fresh load if we don't have the mesh\n",
    "if renderer.mesh_loaded and Path(CONFIG['mesh_path']).exists():\n",
    "    analyze_mesh_for_printing(CONFIG['mesh_path'], CONFIG, preloaded={\n",
    "        'verts': renderer.verts,\n",
    "        'verts_uvs': renderer.verts_uvs,\n",
    "        'faces_verts': renderer.faces,\n",
    "        'faces_uvs': renderer.faces_uvs,\n",
    "    })\n",
    "else:\n",
    "    analyze_mesh_for_printing(CONFIG['mesh_path'], CONFIG)\n",
    "#Render overhead\n",
    "test_render, test_alpha = renderer.render(test_texture, elevation=85, azimuth=45)\n",
    "logger.info(f\"Rendered shape: {test_render.shape}, alpha shape: {test_alpha.shape}\")\n",
    "#Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(test_texture[0].permute(1,2,0).detach().cpu())\n",
    "axes[0].set_title('Texture')\n",
    "axes[1].imshow(test_render[0].permute(1,2,0).detach().cpu())\n",
    "axes[1].set_title('Rendered Hat')\n",
    "axes[2].imshow(test_alpha[0, 0].detach().cpu(), cmap='gray')\n",
    "axes[2].set_title('Alpha Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a086a",
   "metadata": {},
   "source": [
    "## 8: T-SEA Augmentations\n",
    "\n",
    "Helper methods for black/gray/white box transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly mask a region of the rendered hat. Prevents overfitting to specific texture patterns.\n",
    "def patch_cutout(rendered_hat, alpha_mask, prob=0.9, ratio=0.4, fill=0.5):\n",
    "    if np.random.random() > prob:\n",
    "        return rendered_hat\n",
    "    B, C, H, W = rendered_hat.shape\n",
    "    #Random cutout size\n",
    "    cut_h = int(H * ratio)\n",
    "    cut_w = int(W * ratio)\n",
    "    #Random position\n",
    "    top = np.random.randint(0, H - cut_h + 1)\n",
    "    left = np.random.randint(0, W - cut_w + 1)\n",
    "    #Apply cutout (only where alpha > 0)\n",
    "    mask = alpha_mask.clone()\n",
    "    mask[:, :, top:top+cut_h, left:left+cut_w] = 0\n",
    "    rendered_hat = rendered_hat * mask + fill * (1 - mask) * (alpha_mask > 0).float()\n",
    "    return rendered_hat\n",
    "\n",
    "#Mild augmentations that don't distort the image too much.\n",
    "def constrained_augmentation(image):\n",
    "    B, C, H, W = image.shape\n",
    "    #Random scale (0.9 - 1.1)\n",
    "    scale = np.random.uniform(0.9, 1.1)\n",
    "    new_h = int(H * scale)\n",
    "    new_w = int(W * scale)\n",
    "    image = F.interpolate(image, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "    #Crop/pad back to original size\n",
    "    if new_h > H:\n",
    "        start_h = (new_h - H) // 2\n",
    "        start_w = (new_w - W) // 2\n",
    "        image = image[:, :, start_h:start_h+H, start_w:start_w+W]\n",
    "    else:\n",
    "        pad_h_top = (H - new_h) // 2\n",
    "        pad_h_bot = H - new_h - pad_h_top\n",
    "        pad_w_left = (W - new_w) // 2\n",
    "        pad_w_right = W - new_w - pad_w_left\n",
    "        image = F.pad(image, [pad_w_left, pad_w_right, pad_h_top, pad_h_bot], mode='reflect')\n",
    "    #Color jitter (mild)\n",
    "    brightness = np.random.uniform(0.9, 1.1)\n",
    "    image = image * brightness\n",
    "    #Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        image = torch.flip(image, dims=[3])\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#ShakeDrop reates virtual ensemble of model variants\n",
    "def shakedrop_forward(x, drop_prob=0.5, alpha_range=(0, 2)):\n",
    "    #Simplified ShakeDrop: add scaled noise to create virtual ensemble variants\n",
    "    if np.random.random() < drop_prob:\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = torch.randn_like(x) * 0.1 * alpha\n",
    "        x = x + noise\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02ef7e",
   "metadata": {},
   "source": [
    "## 9: URAdv Augmentations\n",
    "\n",
    "For better performance under drone conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66890419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add simulated light reflections on the hat surface.\n",
    "def add_light_spots(image, alpha_mask, num_range=(0, 3), intensity_range=(0.1, 0.4)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_spots = np.random.randint(*num_range)\n",
    "    y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "    for _ in range(num_spots):\n",
    "        #Random spot position (within hat region)\n",
    "        cy = np.random.randint(H // 4, 3 * H // 4)\n",
    "        cx = np.random.randint(W // 4, 3 * W // 4)\n",
    "        #Spot parameters\n",
    "        radius = np.random.uniform(0.05, 0.15) * min(H, W)\n",
    "        intensity = np.random.uniform(*intensity_range)\n",
    "        #Create Gaussian spot\n",
    "        dist = ((x - cx) ** 2 + (y - cy) ** 2).float()\n",
    "        spot = torch.exp(-dist / (2 * radius ** 2)) * intensity\n",
    "        #Apply only within hat (where alpha > 0)\n",
    "        spot = spot.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image + spot\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Add simulated shadows on the hat surface.\n",
    "def add_shadows(image, alpha_mask, num_range=(0, 2), opacity_range=(0.2, 0.5)):\n",
    "    if np.random.random() > 0.5:\n",
    "        return image\n",
    "    B, C, H, W = image.shape\n",
    "    num_shadows = np.random.randint(*num_range)\n",
    "    y, x = torch.meshgrid(torch.arange(H, device=image.device), torch.arange(W, device=image.device), indexing='ij')\n",
    "    for _ in range(num_shadows):\n",
    "        #Random shadow as diagonal stripe\n",
    "        angle = np.random.uniform(0, np.pi)\n",
    "        opacity = np.random.uniform(*opacity_range)\n",
    "        width = np.random.uniform(0.1, 0.3) * min(H, W)\n",
    "        #Create shadow mask\n",
    "        offset = np.random.uniform(0, H)\n",
    "        dist = torch.abs(x * np.cos(angle) + y * np.sin(angle) - offset)\n",
    "        shadow = (dist < width).float() * opacity\n",
    "        #Apply only within hat\n",
    "        shadow = shadow.unsqueeze(0).unsqueeze(0) * (alpha_mask > 0).float()\n",
    "        image = image * (1 - shadow)\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Simulate printer color/brightness variation.\n",
    "def simulate_printing(texture, mul_std=0.1, add_std=0.05):\n",
    "    #Multiplicative noise\n",
    "    mul_noise = torch.randn_like(texture) * mul_std + 1.0\n",
    "    texture = texture * mul_noise\n",
    "    #Additive noise\n",
    "    add_noise = torch.randn_like(texture) * add_std\n",
    "    texture = texture + add_noise\n",
    "    return texture.clamp(0, 1)\n",
    "\n",
    "class PrinterGamut:\n",
    "\n",
    "    #Load parameters, or use simplified constraints\n",
    "    def __init__(self, config, device='cuda'):\n",
    "        self.config = config['printer']\n",
    "        self.device = device\n",
    "        gamut_path = self.config.get('gamut_samples_path')\n",
    "        if gamut_path and Path(gamut_path).exists():\n",
    "            self.gamut_samples = torch.from_numpy(np.load(gamut_path)).float().to(self.device)\n",
    "            self.use_measured_gamut = True\n",
    "            logger.info(f\"Loaded {len(self.gamut_samples)} gamut samples\")\n",
    "        else:\n",
    "            self.use_measured_gamut = False\n",
    "            logger.info(\"Using simplified gamut constraints\")\n",
    "\n",
    "    #Non-printability score loss\n",
    "    def nps_loss(self, texture):\n",
    "        loss = 0.0\n",
    "        if self.use_measured_gamut:\n",
    "            #Compare each texture pixel to the nearest printable color\n",
    "            #texture: (B, 3, H, W) -> (N, 3) flat pixel list\n",
    "            B, C, H, W = texture.shape\n",
    "            pixels = texture.permute(0, 2, 3, 1).reshape(-1, 3)  #(N, 3)\n",
    "            #gamut_samples: (G, 3) — measured printable RGB values\n",
    "            #Compute distance from each pixel to nearest gamut sample\n",
    "            chunk_size = 4096\n",
    "            gamut_dists = []\n",
    "            for i in range(0, pixels.shape[0], chunk_size):\n",
    "                chunk = pixels[i:i+chunk_size]  #(chunk, 3)\n",
    "                dists = torch.cdist(chunk.unsqueeze(0), self.gamut_samples.unsqueeze(0)).squeeze(0)  #(chunk, G)\n",
    "                min_dists = dists.min(dim=1)[0]  #(chunk,)\n",
    "                gamut_dists.append(min_dists)\n",
    "            gamut_dists = torch.cat(gamut_dists)\n",
    "            loss = loss + gamut_dists.mean()\n",
    "        else:\n",
    "            #Simplified constraints when no gamut data available\n",
    "            max_ch = texture.max(dim=1)[0]\n",
    "            min_ch = texture.min(dim=1)[0]\n",
    "            saturation = (max_ch - min_ch) / (max_ch + 1e-8)\n",
    "            brightness = max_ch\n",
    "            #Saturation * brightness threshold\n",
    "            loss = loss + F.relu(saturation * brightness - self.config['nps_threshold']).mean()\n",
    "            #Saturation cap\n",
    "            loss = loss + F.relu(saturation - self.config['max_saturation']).mean()\n",
    "            #Brightness bounds\n",
    "            loss = loss + F.relu(brightness - self.config['max_brightness']).mean()\n",
    "            loss = loss + F.relu(self.config['min_brightness'] - brightness).mean()\n",
    "        return loss\n",
    "\n",
    "    #Hard clamp for final export\n",
    "    def clamp_to_gamut(self, texture):\n",
    "        return texture.clamp(self.config['min_brightness'], self.config['max_brightness'])\n",
    "\n",
    "#Initialize globally\n",
    "printer_gamut = PrinterGamut(CONFIG, device)\n",
    "\n",
    "#Apply camera artifacts: blur, noise.\n",
    "def apply_environmental_augmentation(image, prob=0.3):\n",
    "    #Motion blur\n",
    "    if np.random.random() < prob:\n",
    "        kernel_size = np.random.choice([3, 5, 7])\n",
    "        kernel = torch.zeros(kernel_size, kernel_size, device=image.device)\n",
    "        kernel[kernel_size//2, :] = 1.0 / kernel_size\n",
    "        #Rotate kernel to random angle for directional blur\n",
    "        angle = np.random.uniform(0, 360)\n",
    "        angle_rad = np.deg2rad(angle)\n",
    "        cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
    "        #Build rotation matrix and affine grid\n",
    "        theta = torch.tensor([[cos_a, -sin_a, 0], [sin_a,  cos_a, 0]], dtype=torch.float32, device=image.device).unsqueeze(0)\n",
    "        grid = F.affine_grid(theta, [1, 1, kernel_size, kernel_size], align_corners=False)\n",
    "        kernel_rotated = F.grid_sample(kernel.unsqueeze(0).unsqueeze(0), grid, align_corners=False, mode='bilinear', padding_mode='zeros').squeeze()\n",
    "        #Renormalize so kernel sums to 1\n",
    "        kernel_rotated = kernel_rotated / (kernel_rotated.sum() + 1e-8)\n",
    "        image = F.conv2d(image, kernel_rotated.view(1, 1, kernel_size, kernel_size).expand(3, 1, -1, -1), padding=kernel_size//2, groups=3)\n",
    "    #Gaussian noise\n",
    "    if np.random.random() < prob:\n",
    "        noise_std = np.random.uniform(0.01, 0.05)\n",
    "        image = image + torch.randn_like(image) * noise_std\n",
    "    return image.clamp(0, 1)\n",
    "\n",
    "#Viewpoint jitter for robustness\n",
    "def apply_viewpoint_jitter(elevation, azimuth, scale, config):\n",
    "    elev = elevation + np.random.uniform(-config['camera_pitch_jitter'], config['camera_pitch_jitter'])\n",
    "    elev = np.clip(elev, 0, 90)\n",
    "    azim = azimuth + np.random.uniform(-config['heading_jitter'], config['heading_jitter'])\n",
    "    azim = azim % 360\n",
    "    scl = scale * (1 + np.random.uniform(-config['scale_jitter'], config['scale_jitter']))\n",
    "    scl = np.clip(scl, 0.1, 2.0)\n",
    "    return elev, azim, scl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296071",
   "metadata": {},
   "source": [
    "## 10: Toroidal Cropping\n",
    "\n",
    "Wrapping the texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to get smooth textures\n",
    "def total_variation_loss(texture):\n",
    "    diff_h = texture[:, :, 1:, :] - texture[:, :, :-1, :]\n",
    "    diff_w = texture[:, :, :, 1:] - texture[:, :, :, :-1]\n",
    "    #Toroidal wrap: penalize top↔bottom and left↔right seam\n",
    "    diff_h_wrap = texture[:, :, 0:1, :] - texture[:, :, -1:, :]\n",
    "    diff_w_wrap = texture[:, :, :, 0:1] - texture[:, :, :, -1:]\n",
    "    #Concatenate so all pairs have equal weight in the mean\n",
    "    all_h = torch.cat([diff_h, diff_h_wrap], dim=2)\n",
    "    all_w = torch.cat([diff_w, diff_w_wrap], dim=3)\n",
    "    return (all_h.pow(2).mean() + all_w.pow(2).mean()) / 2\n",
    "\n",
    "class ToroidalLatent(nn.Module):\n",
    "\n",
    "    def __init__(self, local_size, crop_size=9, latent_channels=128, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.local_size = local_size\n",
    "        self.crop_size = crop_size\n",
    "        self.latent_channels = latent_channels\n",
    "        self.z_local = nn.Parameter(torch.randn(1, latent_channels, local_size, local_size, device=device) * 0.1)\n",
    "\n",
    "    #Random crop\n",
    "    def get_random_crops(self, batch_size):\n",
    "        #Tile 3x3 for wraparound\n",
    "        z_tiled = self.z_local.repeat(1, 1, 3, 3)\n",
    "        crops = []\n",
    "        for _ in range(batch_size):\n",
    "            #Random offset within middle tile (to enable wraparound)\n",
    "            i = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            j = np.random.randint(self.local_size, 2 * self.local_size)\n",
    "            crop = z_tiled[:, :, i:i+self.crop_size, j:j+self.crop_size]\n",
    "            crops.append(crop)\n",
    "        return torch.cat(crops, dim=0)\n",
    "\n",
    "    #Crop to fill size\n",
    "    def get_full_latent(self, target_spatial_size):\n",
    "        reps = (target_spatial_size + self.local_size - 1) // self.local_size + 1\n",
    "        z_tiled = self.z_local.repeat(1, 1, reps, reps)\n",
    "        return z_tiled[:, :, :target_spatial_size, :target_spatial_size]\n",
    "\n",
    "    #Deterministic crop\n",
    "    def get_canonical_crop(self):\n",
    "        offset = (self.local_size - self.crop_size) // 2\n",
    "        return self.z_local[:, :, offset:offset+self.crop_size, offset:offset+self.crop_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fd5cd-3724-44f2-ab9f-f1d289d211a8",
   "metadata": {},
   "source": [
    "## 11: Sceen Composition\n",
    "\n",
    "Render the sceen (put hat on image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3c7d8-1f11-40fd-9e03-3d34c614c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_hat_on_scene(scene_image, hat_mask, rendered_hat, alpha_mask):\n",
    "    B, C, H, W = scene_image.shape\n",
    "    full_hat = torch.zeros_like(scene_image)\n",
    "    full_alpha = torch.zeros(B, 1, H, W, device=scene_image.device)\n",
    "    for i in range(B):\n",
    "        mask = hat_mask[i, 0]\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        ys, xs = torch.where(mask > 0.5)\n",
    "        y1, y2 = ys.min().item(), ys.max().item()\n",
    "        x1, x2 = xs.min().item(), xs.max().item()\n",
    "        hat_h, hat_w = y2 - y1 + 1, x2 - x1 + 1\n",
    "        if hat_h < 2 or hat_w < 2:\n",
    "            continue\n",
    "        a = alpha_mask[i, 0]\n",
    "        a_ys, a_xs = torch.where(a > 0.1)\n",
    "        if len(a_ys) == 0:\n",
    "            continue\n",
    "        ay1, ay2 = a_ys.min().item(), a_ys.max().item()\n",
    "        ax1, ax2 = a_xs.min().item(), a_xs.max().item()\n",
    "        hat_crop = rendered_hat[i:i+1, :, ay1:ay2+1, ax1:ax2+1]\n",
    "        alpha_crop = alpha_mask[i:i+1, :, ay1:ay2+1, ax1:ax2+1]\n",
    "        hat_resized = F.interpolate(hat_crop, size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        alpha_resized = F.interpolate(alpha_crop, size=(hat_h, hat_w), mode='bilinear', align_corners=False)\n",
    "        alpha_resized = (alpha_resized > 0.1).float()\n",
    "        full_hat[i:i+1, :, y1:y2+1, x1:x2+1] = hat_resized\n",
    "        full_alpha[i:i+1, :, y1:y2+1, x1:x2+1] = alpha_resized\n",
    "    composited = full_hat * full_alpha + scene_image * (1 - full_alpha)\n",
    "    return composited\n",
    "\n",
    "def crop_to_person(composited, person_bbox, target_size=640, pad_ratio=0.5):\n",
    "    B, C, H, W = composited.shape\n",
    "    crops = []\n",
    "    for i in range(B):\n",
    "        x1, y1, x2, y2 = person_bbox[i]\n",
    "        bw, bh = x2 - x1, y2 - y1\n",
    "        side = max(bw, bh) * (1 + pad_ratio)\n",
    "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        half = side / 2\n",
    "        cx1 = int(torch.clamp(cx - half, min=0))\n",
    "        cy1 = int(torch.clamp(cy - half, min=0))\n",
    "        cx2 = int(torch.clamp(cx + half, max=W))\n",
    "        cy2 = int(torch.clamp(cy + half, max=H))\n",
    "        crop = composited[i:i+1, :, cy1:cy2, cx1:cx2]\n",
    "        crop = F.interpolate(crop, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "        crops.append(crop)\n",
    "    return torch.cat(crops, dim=0)\n",
    "\n",
    "def sahi_aware_crop(composited, person_bbox, config):\n",
    "    B, C, H, W = composited.shape\n",
    "    slice_size = config.get('sahi_slice_size', 960)\n",
    "    target_size = config['detector_input_size']\n",
    "    crops = []\n",
    "    for i in range(B):\n",
    "        bbox = person_bbox[i]\n",
    "        cx = (bbox[0] + bbox[2]).item() / 2\n",
    "        cy = (bbox[1] + bbox[3]).item() / 2\n",
    "        bw = (bbox[2] - bbox[0]).item()\n",
    "        bh = (bbox[3] - bbox[1]).item()\n",
    "        person_side = max(bw, bh)\n",
    "        r = np.random.random()\n",
    "        if r < 0.7:\n",
    "            #SAHI-scale: ~960 tile with ±15% variation\n",
    "            crop_size = int(slice_size * np.random.uniform(0.85, 1.15))\n",
    "        elif r < 0.9:\n",
    "            #Medium: 2-4× person bbox\n",
    "            crop_size = int(person_side * np.random.uniform(2.0, 4.0))\n",
    "        else:\n",
    "            #Tight: 1.5× person bbox (original crop_to_person)\n",
    "            crop_size = int(person_side * 1.5)\n",
    "        crop_size = max(min(crop_size, H, W), 64)\n",
    "        #Random position with person inside (not centered)\n",
    "        margin = crop_size * 0.05\n",
    "        x_lo = max(0, cx - crop_size + margin)\n",
    "        x_hi = min(W - crop_size, cx - margin)\n",
    "        y_lo = max(0, cy - crop_size + margin)\n",
    "        y_hi = min(H - crop_size, cy - margin)\n",
    "        x1 = int(np.clip(np.random.uniform(x_lo, max(x_hi, x_lo + 1)), 0, W - crop_size))\n",
    "        y1 = int(np.clip(np.random.uniform(y_lo, max(y_hi, y_lo + 1)), 0, H - crop_size))\n",
    "        crop = composited[i:i+1, :, y1:y1+crop_size, x1:x1+crop_size]\n",
    "        crop = F.interpolate(crop, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "        crops.append(crop)\n",
    "    return torch.cat(crops, dim=0)\n",
    "\n",
    "def crop_to_head(composited, person_bbox, hat_mask, target_size=640):\n",
    "    B, C, H, W = composited.shape\n",
    "    crops = []\n",
    "    for i in range(B):\n",
    "        mask = hat_mask[i, 0]\n",
    "        if mask.sum() < 10:\n",
    "            #Fallback to top 25% of person bbox\n",
    "            x1, y1, x2, y2 = person_bbox[i]\n",
    "            bh = y2 - y1\n",
    "            head_y2 = y1 + bh * 0.25\n",
    "        else:\n",
    "            ys, xs = torch.where(mask > 0.5)\n",
    "            y1, y2 = ys.min(), ys.max()\n",
    "            x1, x2 = xs.min(), xs.max()\n",
    "            head_y2 = y2\n",
    "        #Square crop centered on hat with some context\n",
    "        x1_p, y1_p, x2_p, y2_p = person_bbox[i]\n",
    "        hat_cx = (x1 + x2) / 2 if mask.sum() >= 10 else (x1_p + x2_p) / 2\n",
    "        hat_cy = (y1 + head_y2) / 2 if mask.sum() >= 10 else y1_p + (y2_p - y1_p) * 0.125\n",
    "        bw = x2_p - x1_p\n",
    "        side = max(bw * 0.6, 100)  # head-width crop with context\n",
    "        half = side / 2\n",
    "        cx1 = int(torch.clamp(hat_cx - half, min=0))\n",
    "        cy1 = int(torch.clamp(hat_cy - half, min=0))\n",
    "        cx2 = int(torch.clamp(hat_cx + half, max=W))\n",
    "        cy2 = int(torch.clamp(hat_cy + half, max=H))\n",
    "        crop = composited[i:i+1, :, cy1:cy2, cx1:cx2]\n",
    "        crop = F.interpolate(crop, size=(target_size, target_size), mode='bilinear', align_corners=False)\n",
    "        crops.append(crop)\n",
    "    return torch.cat(crops, dim=0)\n",
    "\n",
    "def crop_hat_mask(hat_mask, person_bbox, target_size=640, pad_ratio=0.5):\n",
    "    B, C, H, W = hat_mask.shape\n",
    "    crops = []\n",
    "    for i in range(B):\n",
    "        x1, y1, x2, y2 = person_bbox[i]\n",
    "        bw, bh = x2 - x1, y2 - y1\n",
    "        side = max(bw, bh) * (1 + pad_ratio)\n",
    "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        half = side / 2\n",
    "        cx1 = int(torch.clamp(cx - half, min=0))\n",
    "        cy1 = int(torch.clamp(cy - half, min=0))\n",
    "        cx2 = int(torch.clamp(cx + half, max=W))\n",
    "        cy2 = int(torch.clamp(cy + half, max=H))\n",
    "        crop = hat_mask[i:i+1, :, cy1:cy2, cx1:cx2]\n",
    "        crop = F.interpolate(crop, size=(target_size, target_size), mode='nearest')\n",
    "        crops.append(crop)\n",
    "    return torch.cat(crops, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c362705-2ee2-448b-ac4b-6401dd4de414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline Sanity Check render untrained hat on random frames\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "sahi_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path='yolov8m.pt',\n",
    "    confidence_threshold=0.5,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "logger.info(\"Rendering untrained hat on sample frames...\")\n",
    "num_preview = min(15, len(dataset))\n",
    "preview_indices = np.random.choice(len(dataset), num_preview, replace=False)\n",
    "#Grid layout\n",
    "cols = 3\n",
    "rows = (num_preview + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "with torch.no_grad():\n",
    "    #Random untrained texture\n",
    "    random_texture = generator.generate(batch_size=1)\n",
    "    for i, idx in enumerate(preview_indices):\n",
    "        sample = dataset[idx]\n",
    "        scene = sample['image'].unsqueeze(0).to(device)\n",
    "        hat_mask = sample['hat_mask'].unsqueeze(0).to(device)\n",
    "        elev = sample['elevation'].item()\n",
    "        azim = sample['azimuth'].item()\n",
    "        scl = sample['scale'].item()\n",
    "        rendered_hat, alpha = renderer.render(\n",
    "            random_texture,\n",
    "            elevation=torch.tensor([elev], device=device),\n",
    "            azimuth=torch.tensor([azim], device=device),\n",
    "            scale=torch.tensor([scl], device=device)\n",
    "        )\n",
    "        composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "        #Run SAHI detection on full composite\n",
    "        comp_np = (composite[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "        comp_bgr = cv2.cvtColor(comp_np, cv2.COLOR_RGB2BGR)\n",
    "        tmp_path = f\"{CONFIG['output_dir']}/tmp_sanity.png\"\n",
    "        cv2.imwrite(tmp_path, comp_bgr)\n",
    "        sahi_result = get_sliced_prediction(\n",
    "            tmp_path, sahi_model,\n",
    "            slice_height=CONFIG['sahi_slice_size'],\n",
    "            slice_width=CONFIG['sahi_slice_size'],\n",
    "            overlap_height_ratio=CONFIG['sahi_overlap'],\n",
    "            overlap_width_ratio=CONFIG['sahi_overlap'],\n",
    "            verbose=0,\n",
    "        )\n",
    "        n_det = sum(1 for p in sahi_result.object_prediction_list if p.category.id == 0)\n",
    "        #Show composite\n",
    "        axes[i].imshow(composite[0].permute(1, 2, 0).cpu())\n",
    "        #Draw person bbox in blue\n",
    "        bbox = sample['person_bbox']\n",
    "        rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=1, edgecolor='cyan', facecolor='none', linestyle='--')\n",
    "        axes[i].add_patch(rect)\n",
    "        #Draw hat mask outline in green\n",
    "        mask_np = hat_mask[0, 0].cpu().numpy()\n",
    "        contours, _ = cv2.findContours((mask_np > 0.5).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for c in contours:\n",
    "            c = c.squeeze()\n",
    "            if c.ndim == 2 and len(c) > 2:\n",
    "                axes[i].plot(c[:, 0], c[:, 1], 'lime', linewidth=1)\n",
    "        #Hat coverage in this frame\n",
    "        mask_area = (mask_np > 0.5).sum()\n",
    "        bw = (bbox[2] - bbox[0]).item()\n",
    "        bh = (bbox[3] - bbox[1]).item()\n",
    "        bbox_area = bw * bh\n",
    "        cov = mask_area / bbox_area if bbox_area > 0 else 0\n",
    "        axes[i].set_title(f'E{elev:.0f}° A{azim:.0f}° | cov={cov:.1%} | det={n_det}', fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "#Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "fig.suptitle('Pipeline Check: Random Texture on Dataset Frames\\n(cyan=person bbox, green=hat mask, det=detections at 0.5)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['output_dir']}/pipeline_check.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "#Summary\n",
    "logger.info(f\"Rendered {num_preview} frames with untrained texture\")\n",
    "logger.info(\"CHECK: hat should appear inside green mask region on every frame\")\n",
    "logger.info(\"CHECK: every frame should show det >= 1 (untrained texture shouldn't fool detector)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64a18b-3226-42bf-80fc-0de9e91e51a5",
   "metadata": {},
   "source": [
    "## 12: Ensamble\n",
    "\n",
    "Ensamble detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e065e4-b4d7-41a4-95e3-7148c56e3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monkey Patching\n",
    "\"\"\"\n",
    "from ultralytics.nn.modules.block import C2f, Bottleneck\n",
    "from ultralytics.nn.modules.head import Detect\n",
    "_orig_c2f = C2f.forward\n",
    "def _safe_c2f(self, x):\n",
    "    y = list(self.cv1(x).chunk(2, 1))\n",
    "    y = [yi.clone() for yi in y]\n",
    "    for m in self.m:\n",
    "        y.append(m(y[-1]))\n",
    "    return self.cv2(torch.cat(y, 1))\n",
    "C2f.forward = _safe_c2f\n",
    "_orig_bn = Bottleneck.forward\n",
    "def _safe_bn(self, x):\n",
    "    return x.clone() + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "Bottleneck.forward = _safe_bn\n",
    "_orig_fh = Detect.forward_head\n",
    "def _safe_fh(self, x, box_head, cls_head):\n",
    "    bs = x[0].shape[0]\n",
    "    boxes = torch.cat([box_head[i](x[i].clone()).reshape(bs, 4 * self.reg_max, -1) for i in range(self.nl)], dim=-1)\n",
    "    scores = torch.cat([cls_head[i](x[i].clone()).reshape(bs, self.nc, -1) for i in range(self.nl)], dim=-1)\n",
    "    return dict(boxes=boxes, scores=scores, feats=x)\n",
    "Detect.forward_head = _safe_fh\n",
    "\"\"\"\n",
    "\n",
    "class DetectorEnsemble:\n",
    "    \n",
    "    def __init__(self, attack_mode='gray', device='cuda', conf_floor=0.001):\n",
    "        self.device = device\n",
    "        self.conf_floor = conf_floor\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        if attack_mode == 'white':\n",
    "            self.models['yolov8m'] = YOLO('yolov8m.pt')\n",
    "            self.models['yolov8m'].model.to(device)\n",
    "            for m in self.models['yolov8m'].model.modules():\n",
    "                if hasattr(m, 'inplace'):\n",
    "                    m.inplace = False\n",
    "                if isinstance(m, nn.SiLU):\n",
    "                    m.inplace = False\n",
    "                if isinstance(m, nn.ReLU):\n",
    "                    m.inplace = False\n",
    "            self.weights['yolov8m'] = 1.0\n",
    "        elif attack_mode == 'gray':\n",
    "            model_configs = [\n",
    "                ('yolov8s', 0.20),\n",
    "                ('yolov8m', 0.25),\n",
    "                ('yolov8l', 0.20),\n",
    "                ('yolov5m', 0.20),\n",
    "                ('yolov5l', 0.15),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.models[name].model.to(device)\n",
    "                    for m in self.models[name].model.modules():\n",
    "                        if hasattr(m, 'inplace'):\n",
    "                            m.inplace = False\n",
    "                        if isinstance(m, nn.SiLU):\n",
    "                            m.inplace = False\n",
    "                        if isinstance(m, nn.ReLU):\n",
    "                            m.inplace = False\n",
    "                    self.weights[name] = weight\n",
    "                    logger.info(f\"Loaded {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\")\n",
    "        elif attack_mode == 'black':\n",
    "            #Add More\n",
    "            model_configs = [\n",
    "                ('yolov8m', 0.30),\n",
    "                ('yolov8l', 0.25),\n",
    "                ('yolov5l', 0.25),\n",
    "                ('yolov5m', 0.20),\n",
    "            ]\n",
    "            for name, weight in model_configs:\n",
    "                try:\n",
    "                    self.models[name] = YOLO(f'{name}.pt')\n",
    "                    self.models[name].model.to(device)\n",
    "                    for m in self.models[name].model.modules():\n",
    "                        if hasattr(m, 'inplace'):\n",
    "                            m.inplace = False\n",
    "                        if isinstance(m, nn.SiLU):\n",
    "                            m.inplace = False\n",
    "                        if isinstance(m, nn.ReLU):\n",
    "                            m.inplace = False\n",
    "                    self.weights[name] = weight\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load {name}: {e}\") \n",
    "        #Normalize weights\n",
    "        total = sum(self.weights.values())\n",
    "        if total == 0:\n",
    "            logger.error(\"No detector models loaded. Cannot proceed.\")\n",
    "            raise RuntimeError(\"No detector models loaded successfully\")\n",
    "        self.weights = {k: v/total for k, v in self.weights.items()}\n",
    "        logger.info(f\"Detector ensemble ({attack_mode}): {list(self.weights.keys())}\")\n",
    "        \n",
    "    def compute_loss(self, images, return_detections=False):\n",
    "        total_loss = 0.0\n",
    "        all_detections = [] if return_detections else None\n",
    "        for name, model in self.models.items():\n",
    "            weight = self.weights[name]\n",
    "            preds = model.model(images)\n",
    "            #Use pre-sigmoid logits from the second output\n",
    "            if isinstance(preds, (list, tuple)) and len(preds) >= 2 and isinstance(preds[1], dict) and 'scores' in preds[1]:\n",
    "                #preds[1]['scores'] shape: [B, num_classes, num_anchors]\n",
    "                #Class 0 = person\n",
    "                person_conf = preds[1]['scores'][:, 0, :]\n",
    "            elif isinstance(preds, (list, tuple)):\n",
    "                preds = preds[0]\n",
    "                if preds.dim() == 3 and preds.shape[1] == 84:\n",
    "                    person_conf = preds[:, 4, :]\n",
    "                elif preds.dim() == 3 and preds.shape[2] >= 85:\n",
    "                    person_conf = preds[..., 4] * preds[..., 5]\n",
    "                else:\n",
    "                    logger.warning(f\"{name}: unexpected pred shape {preds.shape}, skipping\")\n",
    "                    continue\n",
    "            elif isinstance(preds, dict):\n",
    "                person_conf = preds['scores'][:, 0, :]\n",
    "            else:\n",
    "                logger.warning(f\"{name}: unexpected pred type {type(preds)}, skipping\")\n",
    "                continue\n",
    "            B = person_conf.shape[0]\n",
    "            per_image_loss = []\n",
    "            for b in range(B):\n",
    "                img_confs = person_conf[b].reshape(-1)\n",
    "                k = min(25, img_confs.numel())\n",
    "                top_confs, _ = img_confs.topk(k)\n",
    "                per_image_loss.append(top_confs.mean())\n",
    "            model_loss = torch.stack(per_image_loss).mean()\n",
    "            total_loss = total_loss + weight * model_loss\n",
    "            if return_detections:\n",
    "                all_detections.append({'model': name, 'preds': preds.detach() if isinstance(preds, torch.Tensor) else None})\n",
    "        if return_detections:\n",
    "            return total_loss, all_detections\n",
    "        return total_loss\n",
    "    \n",
    "    def detect(self, images, conf_threshold=0.5):\n",
    "        images_np = [(images[i] * 255).byte().permute(1, 2, 0).cpu().numpy() for i in range(images.shape[0])]\n",
    "        all_results = {}\n",
    "        for name, model in self.models.items():\n",
    "            results = model.predict(images_np, conf=self.conf_floor, classes=[0], verbose=False)\n",
    "            all_results[name] = []\n",
    "            for r in results:\n",
    "                if len(r.boxes) > 0:\n",
    "                    scores = r.boxes.conf.cpu().numpy()\n",
    "                    boxes = r.boxes.xyxy.cpu().numpy()\n",
    "                    #Filter by the actual requested threshold\n",
    "                    keep = scores >= conf_threshold\n",
    "                    all_results[name].append({'boxes': boxes[keep],'scores': scores[keep]})\n",
    "                else:\n",
    "                    all_results[name].append({'boxes': np.array([]),'scores': np.array([])})\n",
    "        return all_results\n",
    "\n",
    "#Initialize detector ensemble\n",
    "detector = DetectorEnsemble(CONFIG['attack_mode'], device, conf_floor=CONFIG['det_conf_floor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cf65e-225e-43cf-82f7-0390b5b60c12",
   "metadata": {},
   "source": [
    "## 13: Stage 1: Generator Training\n",
    "\n",
    "Train the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e1d11-926e-4fb4-a190-166106a0c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(generator, aux_net, detector, dataset, config):\n",
    "\n",
    "    generator.train()\n",
    "    aux_net.train()\n",
    "    #Optimizers\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=config['stage1_lr'], betas=(0.5, 0.999))\n",
    "    opt_aux = torch.optim.Adam(aux_net.parameters(), lr=config['stage1_lr'] * 5, betas=(0.5, 0.999))\n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=config['stage1_batch_size'], shuffle=True, num_workers=config['num_workers'], pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    #Training loop\n",
    "    logger.info(\"Starting Stage 1 training...\")\n",
    "    for epoch in range(config['stage1_epochs']):\n",
    "        epoch_losses = {'total': 0, 'detection': 0, 'tv': 0, 'nps': 0, 'mi': 0}\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config['stage1_epochs']}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            \n",
    "            #Sample latent\n",
    "            actual_batch = batch['image'].shape[0]\n",
    "            z = torch.randn(actual_batch, config['latent_channels'], config['latent_size'], config['latent_size'], device=device)\n",
    "            #Generate texture\n",
    "            texture = generator.generate(z)  #[0, 1]\n",
    "            #Sample viewpoint and render\n",
    "            elevs, azims, scls = [], [], []\n",
    "            for i in range(texture.shape[0]):\n",
    "                elev, azim, scl = apply_viewpoint_jitter(batch['elevation'][i].item(),batch['azimuth'][i].item(),batch['scale'][i].item(), config)\n",
    "                elevs.append(elev)\n",
    "                azims.append(azim)\n",
    "                scls.append(scl)\n",
    "            rendered_hat, alpha = renderer.render(texture,elevation=torch.tensor(elevs, dtype=torch.float32, device=device),azimuth=torch.tensor(azims, dtype=torch.float32, device=device),scale=torch.tensor(scls, dtype=torch.float32, device=device))\n",
    "    \n",
    "            #T-SEA augmentations on rendered hat (before compositing)\n",
    "            rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "            rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "            rendered_hat = add_shadows(rendered_hat, alpha)\n",
    "            #rendered_hat = simulate_printing(rendered_hat)\n",
    "            #Composite onto scene\n",
    "            scene = batch['image'].to(device)\n",
    "            hat_mask = batch['hat_mask'].to(device)\n",
    "            composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            #Crop to person, then apply camera augmentations\n",
    "            person_crops = sahi_aware_crop(composite, batch['person_bbox'].to(device), config)\n",
    "            #person_crops = apply_environmental_augmentation(person_crops)\n",
    "            loss_det = detector.compute_loss(person_crops)\n",
    "\n",
    "            #Regularization losses\n",
    "            loss_tv = total_variation_loss(texture)\n",
    "            loss_nps = printer_gamut.nps_loss(texture)\n",
    "            #Mutual information (maximize = negate for min)\n",
    "            loss_mi = compute_mi_loss(aux_net, texture, z)\n",
    "            #Total loss\n",
    "            if epoch < 10:\n",
    "                #Warmup\n",
    "                loss = (config['lambda_tv'] * loss_tv + config['lambda_nps'] * loss_nps + config['lambda_info'] * loss_mi)\n",
    "            else:\n",
    "                #Scale det loss by 5x so it dominates over MI regularization\n",
    "                loss = (loss_det + config['lambda_tv'] * loss_tv + config['lambda_nps'] * loss_nps + config['lambda_info'] * loss_mi)\n",
    "            \n",
    "            #Optimize\n",
    "            opt_g.zero_grad()\n",
    "            opt_aux.zero_grad()\n",
    "            if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "                torch.nn.utils.clip_grad_norm_(aux_net.parameters(), 1.0)\n",
    "                opt_g.step()\n",
    "                opt_aux.step()\n",
    "            #Track losses\n",
    "            epoch_losses['total'] += loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "            epoch_losses['detection'] += loss_det.item() if isinstance(loss_det, torch.Tensor) else loss_det\n",
    "            epoch_losses['tv'] += loss_tv.item()\n",
    "            epoch_losses['nps'] += loss_nps.item()\n",
    "            epoch_losses['mi'] += loss_mi.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\" if isinstance(loss, torch.Tensor) else f\"{loss:.4f}\", 'det': f\"{loss_det.item():.4f}\" if isinstance(loss_det, torch.Tensor) else f\"{loss_det:.4f}\"})\n",
    "        \n",
    "        #Epoch summary\n",
    "        n_batches = len(dataloader)\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= n_batches\n",
    "        logger.info(f\"Epoch {epoch+1} - Loss: {epoch_losses['total']:.4f}, \" f\"Det: {epoch_losses['detection']:.4f}, \" f\"TV: {epoch_losses['tv']:.4f}, \" f\"MI: {epoch_losses['mi']:.4f}\")\n",
    "        #Save sample texture every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                sample = generator.generate(batch_size=1)\n",
    "                save_texture(sample[0], f\"{config['output_dir']}/texture_epoch{epoch+1}.png\")   \n",
    "    return generator\n",
    "\n",
    "#Preview\n",
    "def save_texture(texture, path):\n",
    "    if isinstance(texture, torch.Tensor):\n",
    "        texture = texture.detach().cpu()\n",
    "        if texture.dim() == 3:\n",
    "            texture = texture.permute(1, 2, 0).numpy()\n",
    "        texture = (texture * 255).astype(np.uint8)\n",
    "    cv2.imwrite(str(path), cv2.cvtColor(texture, cv2.COLOR_RGB2BGR))\n",
    "    logger.info(f\"Saved texture to {path}\")\n",
    "\n",
    "#Pinter ready\n",
    "def save_final_texture(texture, config, path):\n",
    "    from PIL import Image\n",
    "    out_w = config['texture_output_size_w']\n",
    "    out_h = config['texture_output_size_h']\n",
    "    #Resize to print resolution\n",
    "    texture_highres = F.interpolate(texture, size=(out_h, out_w), mode='bilinear', align_corners=False)\n",
    "    #Clamp to printable gamut\n",
    "    texture_highres = printer_gamut.clamp_to_gamut(texture_highres)\n",
    "    #Convert to uint8\n",
    "    texture_np = texture_highres[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    texture_np = (texture_np * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(texture_np)\n",
    "    #Save TIFF with DPI metadata (for print)\n",
    "    tiff_path = Path(path).with_suffix('.tiff')\n",
    "    img.save(tiff_path, dpi=(config['printer']['dpi'], config['printer']['dpi']))\n",
    "    #Save PNG for preview\n",
    "    img.save(path)\n",
    "    logger.info(f\"Saved: {path} and {tiff_path}\")\n",
    "    logger.info(f\"  {out_w}x{out_h}px at {config['printer']['dpi']} DPI\")\n",
    "\n",
    "#Export texture with UV seam cut lines drawn on top.\n",
    "def export_print_template(texture, mesh_path, config, path):\n",
    "    from PIL import Image, ImageDraw\n",
    "    from collections import Counter\n",
    "    _p = config['printer']\n",
    "    out_w = config['texture_output_size_w']\n",
    "    out_h = config['texture_output_size_h']\n",
    "    texture_highres = F.interpolate(texture, size=(out_h, out_w), mode='bilinear', align_corners=False)\n",
    "    texture_highres = printer_gamut.clamp_to_gamut(texture_highres)\n",
    "    texture_np = texture_highres[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    texture_np = (texture_np * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(texture_np)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    if Path(mesh_path).exists():\n",
    "        _, faces, aux = load_obj(mesh_path)\n",
    "        verts_uvs = aux.verts_uvs.cpu().numpy()\n",
    "        faces_uvs = faces.textures_idx.cpu().numpy()\n",
    "        edge_count = Counter()\n",
    "        for face in faces_uvs:\n",
    "            for i in range(3):\n",
    "                e = (min(face[i], face[(i+1)%3]), max(face[i], face[(i+1)%3]))\n",
    "                edge_count[e] += 1\n",
    "        for face in faces_uvs:\n",
    "            for i in range(3):\n",
    "                v0 = verts_uvs[face[i]]\n",
    "                v1 = verts_uvs[face[(i+1) % 3]]\n",
    "                x0, y0 = int(v0[0] * out_w), int((1 - v0[1]) * out_h)\n",
    "                x1, y1 = int(v1[0] * out_w), int((1 - v1[1]) * out_h)\n",
    "                e = (min(face[i], face[(i+1)%3]), max(face[i], face[(i+1)%3]))\n",
    "                \n",
    "                if edge_count[e] == 1:\n",
    "                    draw.line([(x0, y0), (x1, y1)], fill=(255, 0, 0), width=3)\n",
    "        if 'uv_islands' in config:\n",
    "            for info in config['uv_islands']:\n",
    "                bounds = info['uv_bounds']\n",
    "                cx = int(((bounds[0] + bounds[2]) / 2) * out_w)\n",
    "                cy = int((1 - (bounds[1] + bounds[3]) / 2) * out_h)\n",
    "                area = info['physical_area_sq_inches']\n",
    "                draw.text((cx, cy), f\"Panel {info['index']} ({area:.1f} sq in)\", fill=(255, 255, 0))\n",
    "    tiff_path = Path(path).with_suffix('.tiff')\n",
    "    img.save(tiff_path, dpi=(_p['dpi'], _p['dpi']))\n",
    "    img.save(path)\n",
    "    clean_img = Image.fromarray(texture_np)\n",
    "    clean_path = Path(path).with_name(Path(path).stem + '_clean.png')\n",
    "    clean_img.save(clean_path)\n",
    "    logger.info(f\"Print template: {tiff_path}\")\n",
    "    logger.info(f\"  Red lines = CUT here\")\n",
    "    logger.info(f\"  {out_w}x{out_h}px, {config['physical_size_inches'][0]:.1f} x {config['physical_size_inches'][1]:.1f} inches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db60a2-2d8a-4248-9b01-9df76cb4185d",
   "metadata": {},
   "source": [
    "## 14: Stage 2: Latent Optimization\n",
    "\n",
    "Optimize the Latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c3ed1-8530-48e9-9a72-aae0cbbd04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage2(generator, detector, dataset, config, z_local=None):\n",
    "\n",
    "    #Joint optimization generator is fine-tuned, not frozen\n",
    "    #BN layers use stable running stats (eval mode) but weights still update\n",
    "    generator.eval()\n",
    "    \n",
    "    #Initialize toroidal latent\n",
    "    toroidal = ToroidalLatent(local_size=config['local_latent_size'], crop_size=config['latent_size'], latent_channels=config['latent_channels'], device=device)\n",
    "    if z_local is not None:\n",
    "        toroidal.z_local.data = z_local\n",
    "        \n",
    "    #Joint optimizer: latent at normal rate, generator at much lower rate\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': toroidal.parameters(), 'lr': config['stage2_lr']},\n",
    "        {'params': generator.parameters(), 'lr': config['stage2_generator_lr']},\n",
    "    ])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['stage2_iterations'], eta_min=1e-5)\n",
    "    \n",
    "    #DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=config['stage2_batch_size'], shuffle=True, num_workers=config['num_workers'], pin_memory=True, persistent_workers=True)\n",
    "    data_iter = iter(dataloader)\n",
    "    logger.info(\"Starting Stage 2 joint optimization (latent + generator fine-tune)...\")\n",
    "    best_loss = float('inf')\n",
    "    best_z_local = toroidal.z_local.data.clone()\n",
    "    best_generator_state = {k: v.clone() for k, v in generator.state_dict().items()}\n",
    "    pbar = tqdm(range(config['stage2_iterations']), desc=\"Stage 2\")\n",
    "    for iteration in pbar:\n",
    "        #Get batch (cycle through dataset)\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            batch = next(data_iter)\n",
    "        \n",
    "        actual_batch = batch['image'].shape[0]\n",
    "        #Use canonical crop (with local_size=crop_size=9, this IS z_local)\n",
    "        z_canon = toroidal.get_canonical_crop()\n",
    "        z_crops = z_canon.expand(actual_batch, -1, -1, -1)\n",
    "        #Forward through generator (gradients flow to BOTH z_local AND generator weights)\n",
    "        texture = generator.generate(z_crops)\n",
    "        \n",
    "        #Sample viewpoints and render\n",
    "        elevs, azims, scls = [], [], []\n",
    "        for i in range(texture.shape[0]):\n",
    "            elev, azim, scl = apply_viewpoint_jitter(batch['elevation'][i].item(), batch['azimuth'][i].item(), batch['scale'][i].item(), config)\n",
    "            elevs.append(elev)\n",
    "            azims.append(azim)\n",
    "            scls.append(scl)\n",
    "        rendered_hat, alpha = renderer.render(texture, elevation=torch.tensor(elevs, dtype=torch.float32, device=device), azimuth=torch.tensor(azims, dtype=torch.float32, device=device), scale=torch.tensor(scls, dtype=torch.float32, device=device))\n",
    "        \n",
    "        #Augmentations (same as Stage 1)\n",
    "        rendered_hat = patch_cutout(rendered_hat, alpha, config['cutout_prob'], config['cutout_ratio'])\n",
    "        rendered_hat = add_light_spots(rendered_hat, alpha)\n",
    "        rendered_hat = add_shadows(rendered_hat, alpha)\n",
    "        #rendered_hat = simulate_printing(rendered_hat)\n",
    "        \n",
    "        #Composite and crop\n",
    "        scene = batch['image'].to(device)\n",
    "        hat_mask = batch['hat_mask'].to(device)\n",
    "        composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "        person_crops = sahi_aware_crop(composite, batch['person_bbox'].to(device), config)\n",
    "        #person_crops = apply_environmental_augmentation(person_crops)\n",
    "        loss_det = detector.compute_loss(person_crops)\n",
    "        loss_tv = total_variation_loss(texture)\n",
    "        loss_nps = printer_gamut.nps_loss(texture)\n",
    "        loss = loss_det + config['lambda_tv'] * loss_tv + config['lambda_nps'] * loss_nps\n",
    "        \n",
    "        #Optimize\n",
    "        optimizer.zero_grad()\n",
    "        if isinstance(loss, torch.Tensor) and loss.requires_grad:\n",
    "            loss.backward()\n",
    "            # Clip gradients for both parameter groups\n",
    "            torch.nn.utils.clip_grad_norm_(toroidal.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        #Track best (save both latent AND generator state)\n",
    "        loss_val = loss.item() if isinstance(loss, torch.Tensor) else loss\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_z_local = toroidal.z_local.data.clone()\n",
    "            best_generator_state = {k: v.clone() for k, v in generator.state_dict().items()}\n",
    "        pbar.set_postfix({'loss': f\"{loss_val:.4f}\", 'best': f\"{best_loss:.4f}\", 'det': f\"{loss_det.item():.4f}\", 'tv': f\"{loss_tv.item():.4f}\", 'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "        \n",
    "        #Periodic logging\n",
    "        if (iteration + 1) % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                z_canon = toroidal.get_canonical_crop()\n",
    "                sample_texture = generator.generate(z_canon)\n",
    "                save_texture(sample_texture[0], f\"{config['output_dir']}/texture_stage2_iter{iteration+1}.png\")\n",
    "                logger.info(f\"Iter {iteration+1}: loss={loss_val:.4f}, det={loss_det.item():.4f}, tv={loss_tv.item():.4f}\")\n",
    "    \n",
    "    #Restore best state\n",
    "    logger.info(f\"Stage 2 complete. Best loss: {best_loss:.4f}\")\n",
    "    toroidal.z_local.data = best_z_local\n",
    "    generator.load_state_dict(best_generator_state)\n",
    "    \n",
    "    #Generate final texture NO TILING\n",
    "    with torch.no_grad():\n",
    "        z_canon = toroidal.get_canonical_crop()\n",
    "        final_texture = generator.generate(z_canon)\n",
    "    save_final_texture(final_texture, config, f\"{config['output_dir']}/final_texture.png\")\n",
    "    torch.save({\n",
    "        'z_local': best_z_local,\n",
    "        'generator': generator.state_dict(),\n",
    "    }, f\"{config['output_dir']}/stage2_final.pth\")\n",
    "    \n",
    "    return best_z_local, final_texture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203957cc-502f-45f4-94ad-5c4c9320c6d7",
   "metadata": {},
   "source": [
    "## 15: Evaluation\n",
    "\n",
    "See how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13817e3-c57f-4511-a7a6-733797dabb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "def evaluate_texture(generator, detector, dataset, z_local, config):\n",
    "    generator.eval()\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"EVALUATION: SAHI full-image detection\")\n",
    "    logger.info(f\"  Slice: {config['sahi_slice_size']}px, Overlap: {config['sahi_overlap']}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    eval_sahi_model = AutoDetectionModel.from_pretrained(\n",
    "        model_type='yolov8',\n",
    "        model_path='yolov8m.pt',\n",
    "        confidence_threshold=0.5,\n",
    "        device=str(device)\n",
    "    )\n",
    "    #Generate best texture from z_local\n",
    "    toroidal_eval = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=config['latent_size'],\n",
    "        latent_channels=config['latent_channels'],\n",
    "        device=device\n",
    "    )\n",
    "    toroidal_eval.z_local.data = z_local\n",
    "    with torch.no_grad():\n",
    "        z_canon = toroidal_eval.get_canonical_crop()\n",
    "        best_texture = generator.generate(z_canon)\n",
    "    baseline_detected = 0\n",
    "    attacked_detected = 0\n",
    "    total_frames = len(dataset)\n",
    "    frame_results = []\n",
    "    tmp_dir = Path(config['output_dir']) / 'eval_tmp'\n",
    "    tmp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(total_frames):\n",
    "            sample = dataset[idx]\n",
    "            scene = sample['image'].unsqueeze(0).to(device)\n",
    "            hat_mask = sample['hat_mask'].unsqueeze(0).to(device)\n",
    "            elev = sample['elevation'].item()\n",
    "            azim = sample['azimuth'].item()\n",
    "            scl = sample['scale'].item()\n",
    "            #Baseline: original scene\n",
    "            baseline_np = (scene[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            baseline_bgr = cv2.cvtColor(baseline_np, cv2.COLOR_RGB2BGR)\n",
    "            baseline_path = str(tmp_dir / f'baseline_{idx}.png')\n",
    "            cv2.imwrite(baseline_path, baseline_bgr)\n",
    "\n",
    "            baseline_result = get_sliced_prediction(\n",
    "                baseline_path, eval_sahi_model,\n",
    "                slice_height=config['sahi_slice_size'],\n",
    "                slice_width=config['sahi_slice_size'],\n",
    "                overlap_height_ratio=config['sahi_overlap'],\n",
    "                overlap_width_ratio=config['sahi_overlap'],\n",
    "                verbose=0,\n",
    "            )\n",
    "            baseline_persons = [p for p in baseline_result.object_prediction_list if p.category.id == 0]\n",
    "            baseline_found = len(baseline_persons) > 0\n",
    "            if baseline_found:\n",
    "                baseline_detected += 1\n",
    "                baseline_conf = max(p.score.value for p in baseline_persons)\n",
    "            else:\n",
    "                baseline_conf = 0.0\n",
    "            #Attacked: render adversarial hat onto scene\n",
    "            rendered_hat, alpha = renderer.render(\n",
    "                best_texture,\n",
    "                elevation=torch.tensor([elev], device=device),\n",
    "                azimuth=torch.tensor([azim], device=device),\n",
    "                scale=torch.tensor([scl], device=device)\n",
    "            )\n",
    "            composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            attack_np = (composite[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            attack_bgr = cv2.cvtColor(attack_np, cv2.COLOR_RGB2BGR)\n",
    "            attack_path = str(tmp_dir / f'attack_{idx}.png')\n",
    "            cv2.imwrite(attack_path, attack_bgr)\n",
    "            attack_result = get_sliced_prediction(\n",
    "                attack_path, eval_sahi_model,\n",
    "                slice_height=config['sahi_slice_size'],\n",
    "                slice_width=config['sahi_slice_size'],\n",
    "                overlap_height_ratio=config['sahi_overlap'],\n",
    "                overlap_width_ratio=config['sahi_overlap'],\n",
    "                verbose=0,\n",
    "            )\n",
    "            attack_persons = [p for p in attack_result.object_prediction_list if p.category.id == 0]\n",
    "            attack_found = len(attack_persons) > 0\n",
    "            if attack_found:\n",
    "                attacked_detected += 1\n",
    "                attack_conf = max(p.score.value for p in attack_persons)\n",
    "            else:\n",
    "                attack_conf = 0.0\n",
    "            frame_results.append({\n",
    "                'frame_idx': idx,\n",
    "                'baseline_detected': baseline_found,\n",
    "                'baseline_conf': round(baseline_conf, 3),\n",
    "                'attack_detected': attack_found,\n",
    "                'attack_conf': round(attack_conf, 3),\n",
    "                'suppressed': baseline_found and not attack_found,\n",
    "            })\n",
    "            if idx % 20 == 0:\n",
    "                logger.info(f\"  Eval {idx}/{total_frames}...\")\n",
    "    #Results\n",
    "    baseline_miss = total_frames - baseline_detected\n",
    "    suppressed = sum(1 for r in frame_results if r['suppressed'])\n",
    "    asr = suppressed / baseline_detected if baseline_detected > 0 else 0.0\n",
    "    still_detected = [r for r in frame_results if r['baseline_detected'] and r['attack_detected']]\n",
    "    if still_detected:\n",
    "        avg_conf_drop = np.mean([r['baseline_conf'] - r['attack_conf'] for r in still_detected])\n",
    "    else:\n",
    "        avg_conf_drop = 0.0\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"RESULTS\")\n",
    "    logger.info(f\"  Total frames:           {total_frames}\")\n",
    "    logger.info(f\"  Baseline detected:      {baseline_detected}/{total_frames} ({baseline_detected/total_frames:.1%})\")\n",
    "    logger.info(f\"  Baseline missed:        {baseline_miss}/{total_frames} ({baseline_miss/total_frames:.1%})\")\n",
    "    logger.info(f\"  Attack detected:        {attacked_detected}/{total_frames}\")\n",
    "    logger.info(f\"  Suppressed (ASR):       {suppressed}/{baseline_detected} ({asr:.1%})\")\n",
    "    logger.info(f\"  Avg conf drop (partial):{avg_conf_drop:.3f}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    #Save detailed results\n",
    "    eval_results = {\n",
    "        'summary': {\n",
    "            'total_frames': total_frames,\n",
    "            'baseline_detected': baseline_detected,\n",
    "            'attacked_detected': attacked_detected,\n",
    "            'suppressed': suppressed,\n",
    "            'asr': round(asr, 4),\n",
    "            'avg_conf_drop': round(avg_conf_drop, 4),\n",
    "        },\n",
    "        'per_frame': frame_results,\n",
    "        'config': {\n",
    "            'sahi_slice_size': config['sahi_slice_size'],\n",
    "            'sahi_overlap': config['sahi_overlap'],\n",
    "            'eval_conf_threshold': 0.5,\n",
    "        }\n",
    "    }\n",
    "    with open(f\"{config['output_dir']}/eval_results.json\", 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    logger.info(f\"Detailed results: {config['output_dir']}/eval_results.json\")\n",
    "    #Visualization\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "\n",
    "    failed = sorted([r for r in frame_results if r['baseline_detected'] and r['attack_detected']],\n",
    "                    key=lambda r: -r['attack_conf'])[:5]\n",
    "    for i, r in enumerate(failed):\n",
    "        img = cv2.imread(str(tmp_dir / f'attack_{r[\"frame_idx\"]}.png'))\n",
    "        axes[0, i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, i].set_title(f'FAIL #{r[\"frame_idx\"]}\\nbase={r[\"baseline_conf\"]:.2f} atk={r[\"attack_conf\"]:.2f}', fontsize=9)\n",
    "        axes[0, i].axis('off')\n",
    "    for j in range(len(failed), 5):\n",
    "        axes[0, j].axis('off')\n",
    "    axes[0, 0].set_ylabel('Attack Failed', fontsize=12)\n",
    "\n",
    "    succeeded = [r for r in frame_results if r['suppressed']][:5]\n",
    "    for i, r in enumerate(succeeded):\n",
    "        img = cv2.imread(str(tmp_dir / f'attack_{r[\"frame_idx\"]}.png'))\n",
    "        axes[1, i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        axes[1, i].set_title(f'SUCCESS #{r[\"frame_idx\"]}\\nbase={r[\"baseline_conf\"]:.2f} atk={r[\"attack_conf\"]:.2f}', fontsize=9)\n",
    "        axes[1, i].axis('off')\n",
    "    for j in range(len(succeeded), 5):\n",
    "        axes[1, j].axis('off')\n",
    "    axes[1, 0].set_ylabel('Attack Succeeded', fontsize=12)\n",
    "\n",
    "    fig.suptitle(f'Evaluation: ASR = {asr:.1%} ({suppressed}/{baseline_detected})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/eval_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    #Cleanup\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    #Return ASR at conf=0.5 for main() to use\n",
    "    return {0.5: asr}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cbc12-7460-42f4-a62e-3076cd91fdd0",
   "metadata": {},
   "source": [
    "## 16: Main\n",
    "\n",
    "Bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62395d-f498-4292-9c3b-49ffa50c131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Initialize models\n",
    "    logger.info(\"Initializing models...\")\n",
    "    generator = FCNGenerator(latent_channels=CONFIG['latent_channels'], latent_size=CONFIG['latent_size']).to(device)\n",
    "    aux_net = AuxiliaryNetwork(latent_channels=CONFIG['latent_channels']).to(device)\n",
    "    logger.info(f\"Generator params: {sum(p.numel() for p in generator.parameters()):}\")\n",
    "    logger.info(f\"Auxiliary params: {sum(p.numel() for p in aux_net.parameters()):}\")\n",
    "    \n",
    "    #Stage 1: Train generator\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 1: Generator Training\")\n",
    "    logger.info(\"=\"*50)\n",
    "    generator = train_stage1(generator, aux_net, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Stage 2: Optimize latent\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"STAGE 2: Latent Optimization\")\n",
    "    logger.info(\"=\"*50)\n",
    "    best_z_local, final_texture = train_stage2(generator, detector, dataset, CONFIG)\n",
    "    \n",
    "    #Evaluation\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"EVALUATION\")\n",
    "    logger.info(\"=\"*50)\n",
    "    asr_results = evaluate_texture(generator, detector, dataset, best_z_local, CONFIG)\n",
    "    export_print_template(final_texture, CONFIG['mesh_path'], CONFIG, f\"{CONFIG['output_dir']}/print_template.png\")\n",
    "    \n",
    "    #Final visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    #Final texture\n",
    "    axes[0].imshow(final_texture[0].permute(1, 2, 0).cpu())\n",
    "    axes[0].set_title('Final Adversarial Texture')\n",
    "    axes[0].axis('off')\n",
    "    #Rendered examples\n",
    "    with torch.no_grad():\n",
    "        toroidal_viz = ToroidalLatent(\n",
    "            local_size=CONFIG['local_latent_size'],\n",
    "            crop_size=CONFIG['latent_size'],\n",
    "            latent_channels=CONFIG['latent_channels'],\n",
    "            device=device\n",
    "        )\n",
    "        toroidal_viz.z_local.data = best_z_local\n",
    "        z_canon = toroidal_viz.get_canonical_crop()\n",
    "        viz_texture = generator.generate(z_canon)\n",
    "        rendered, alpha = renderer.render(viz_texture, 85, 45, 1.0)\n",
    "    axes[1].imshow(rendered[0].permute(1, 2, 0).cpu())\n",
    "    axes[1].set_title('Rendered Hat (85°, 45°)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    #ASR plot\n",
    "    thresholds = list(asr_results.keys())\n",
    "    values = [asr_results[t] for t in thresholds]\n",
    "    axes[2].bar(range(len(thresholds)), values)\n",
    "    axes[2].set_xticks(range(len(thresholds)))\n",
    "    axes[2].set_xticklabels([f'{t}' for t in thresholds])\n",
    "    axes[2].set_xlabel('Confidence Threshold')\n",
    "    axes[2].set_ylabel('Attack Success Rate')\n",
    "    axes[2].set_title('ASR vs Threshold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/final_results.png\", dpi=150)\n",
    "    plt.show()\n",
    "    logger.info(\"Training complete!\")\n",
    "    logger.info(f\"Final texture saved to: {CONFIG['output_dir']}/final_texture.png\")\n",
    "    return generator, best_z_local, final_texture\n",
    "\n",
    "#Run if this is the main notebook\n",
    "if __name__ == \"__main__\":\n",
    "    generator, z_local, texture = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05def843-399c-4a90-8217-251e741f3ce3",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166a7df-7133-4227-8c1a-522368483a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(generator, detector, dataset, z_local, config):\n",
    "    generator.eval()\n",
    "    toroidal = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=config['latent_size'],\n",
    "        latent_channels=config['latent_channels'],\n",
    "        device=device\n",
    "    )\n",
    "    toroidal.z_local.data = z_local\n",
    "    with torch.no_grad():\n",
    "        z_canon = toroidal.get_canonical_crop()\n",
    "        texture = generator.generate(z_canon)\n",
    "    \n",
    "    #1. The Texture Itself\n",
    "    fig1, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.imshow(texture[0].permute(1, 2, 0).cpu())\n",
    "    ax.set_title(f'Adversarial Texture ({config[\"texture_size\"]}×{config[\"texture_size\"]})')\n",
    "    ax.axis('off')\n",
    "    plt.savefig(f\"{config['output_dir']}/viz_texture.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    #2. Multi-Viewpoint Render Grid\n",
    "    elevations = [70, 75, 80, 85, 90]\n",
    "    azimuths = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "    fig2, axes2 = plt.subplots(len(elevations), len(azimuths), figsize=(len(azimuths) * 2.5, len(elevations) * 2.5))\n",
    "    with torch.no_grad():\n",
    "        for i, elev in enumerate(elevations):\n",
    "            for j, azim in enumerate(azimuths):\n",
    "                rendered, alpha = renderer.render(texture, elevation=elev, azimuth=azim, scale=1.0)\n",
    "                # White background for visibility\n",
    "                white_bg = torch.ones_like(rendered)\n",
    "                vis = rendered * alpha + white_bg * (1 - alpha)\n",
    "                axes2[i, j].imshow(vis[0].permute(1, 2, 0).cpu())\n",
    "                axes2[i, j].set_title(f'E{elev}° A{azim}°', fontsize=7)\n",
    "                axes2[i, j].axis('off')\n",
    "    fig2.suptitle('Adversarial Hat All Viewpoints', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/viz_viewpoints.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    #3. Composited on Sample Frames with Detection\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    num_samples = min(4, len(dataset))\n",
    "    fig3, axes3 = plt.subplots(num_samples, 4, figsize=(20, num_samples * 5))\n",
    "    if num_samples == 1:\n",
    "        axes3 = axes3[np.newaxis, :]\n",
    "    col_titles = ['Original Scene', 'With Adversarial Hat', 'Clean Crop + Detections', 'Attacked Crop + Detections']\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            if idx >= num_samples:\n",
    "                break\n",
    "            scene = batch['image'].to(device)\n",
    "            hat_mask = batch['hat_mask'].to(device)\n",
    "            actual_batch = scene.shape[0]\n",
    "            elev = batch['elevation'].tolist()\n",
    "            azim = batch['azimuth'].tolist()\n",
    "            scl = batch['scale'].tolist()\n",
    "            tex = texture.expand(actual_batch, -1, -1, -1)\n",
    "            rendered_hat, alpha = renderer.render(tex,\n",
    "                elevation=torch.tensor(elev, dtype=torch.float32, device=device),\n",
    "                azimuth=torch.tensor(azim, dtype=torch.float32, device=device),\n",
    "                scale=torch.tensor(scl, dtype=torch.float32, device=device))\n",
    "            composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            person_crops = crop_to_person(composite, batch['person_bbox'].to(device), target_size=config['detector_input_size'])\n",
    "            clean_crops = crop_to_person(scene, batch['person_bbox'].to(device), target_size=config['detector_input_size'])\n",
    "            results_atk = detector.detect(person_crops, conf_threshold=0.3)\n",
    "            results_clean = detector.detect(clean_crops, conf_threshold=0.3)\n",
    "            #Column 0: Original scene\n",
    "            axes3[idx, 0].imshow(scene[0].permute(1, 2, 0).cpu())\n",
    "            #Column 1: Scene with adversarial hat\n",
    "            axes3[idx, 1].imshow(composite[0].permute(1, 2, 0).cpu())\n",
    "            #Column 2: Clean crop with green detection boxes\n",
    "            axes3[idx, 2].imshow(clean_crops[0].permute(1, 2, 0).cpu().numpy())\n",
    "            n_clean = 0\n",
    "            for name, dets in results_clean.items():\n",
    "                for box in dets[0]['boxes']:\n",
    "                    rect = plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor='lime', facecolor='none')\n",
    "                    axes3[idx, 2].add_patch(rect)\n",
    "                n_clean = len(dets[0]['boxes'])\n",
    "                break  #Just first model for display\n",
    "            axes3[idx, 2].set_title(f'Clean ({n_clean} detections)', fontsize=9)\n",
    "            #Column 3: Attacked crop with red detection boxes\n",
    "            axes3[idx, 3].imshow(person_crops[0].permute(1, 2, 0).cpu().numpy())\n",
    "            n_atk = 0\n",
    "            for name, dets in results_atk.items():\n",
    "                for box in dets[0]['boxes']:\n",
    "                    rect = plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor='red', facecolor='none')\n",
    "                    axes3[idx, 3].add_patch(rect)\n",
    "                n_atk = len(dets[0]['boxes'])\n",
    "                break\n",
    "            axes3[idx, 3].set_title(f'Attacked ({n_atk} detections)', fontsize=9)\n",
    "            for j in range(4):\n",
    "                axes3[idx, j].axis('off')\n",
    "                if idx == 0:\n",
    "                    axes3[idx, j].set_title(col_titles[j] + ('\\n' + axes3[idx, j].get_title() if j >= 2 else ''), fontsize=9)\n",
    "    fig3.suptitle('Attack Visualization on Sample Frames', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/viz_composites.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    #4. Print Preview: how the texture looks at physical scale\n",
    "    fig4, axes4 = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    #Raw texture\n",
    "    axes4[0].imshow(texture[0].permute(1, 2, 0).cpu())\n",
    "    axes4[0].set_title('Raw Texture (288×288)')\n",
    "    axes4[0].axis('off')\n",
    "    #Upscaled to print resolution (what gets printed)\n",
    "    out_h = config.get('texture_output_size_h', 1024)\n",
    "    out_w = config.get('texture_output_size_w', 1024)\n",
    "    texture_print = F.interpolate(texture, size=(out_h, out_w), mode='bilinear', align_corners=False)\n",
    "    #Show center 500×500 crop at print res\n",
    "    crop_size = min(500, out_h, out_w)\n",
    "    ch, cw = out_h // 2, out_w // 2\n",
    "    print_crop = texture_print[0, :, ch-crop_size//2:ch+crop_size//2, cw-crop_size//2:cw+crop_size//2]\n",
    "    axes4[1].imshow(print_crop.permute(1, 2, 0).cpu())\n",
    "    axes4[1].set_title(f'Print Detail (center {crop_size}×{crop_size} of {out_w}×{out_h})')\n",
    "    axes4[1].axis('off')\n",
    "    #Rendered on hat at typical drone angle\n",
    "    rendered_85, alpha_85 = renderer.render(texture, elevation=85, azimuth=0, scale=1.0)\n",
    "    white_bg = torch.ones_like(rendered_85)\n",
    "    hat_vis = rendered_85 * alpha_85 + white_bg * (1 - alpha_85)\n",
    "    axes4[2].imshow(hat_vis[0].permute(1, 2, 0).cpu())\n",
    "    axes4[2].set_title('On Hat (85° overhead)')\n",
    "    axes4[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config['output_dir']}/viz_print_preview.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    logger.info(f\"All visualizations saved to {config['output_dir']}/viz_*.png\")\n",
    "\n",
    "#Run visualization\n",
    "visualize_results(generator, detector, dataset, z_local, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02be2c-1b4b-4a07-b887-8fa92c5f6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_crop_based(generator, detector, dataset, z_local, config):\n",
    "    generator.eval()\n",
    "    \n",
    "    toroidal_eval = ToroidalLatent(\n",
    "        local_size=config['local_latent_size'],\n",
    "        crop_size=config['latent_size'],\n",
    "        latent_channels=config['latent_channels'],\n",
    "        device=device\n",
    "    )\n",
    "    toroidal_eval.z_local.data = z_local\n",
    "    with torch.no_grad():\n",
    "        z_canon = toroidal_eval.get_canonical_crop()\n",
    "        best_texture = generator.generate(z_canon)\n",
    "\n",
    "    baseline_det = 0\n",
    "    attack_det = 0\n",
    "    total = len(dataset)\n",
    "    conf_drops = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(total):\n",
    "            sample = dataset[idx]\n",
    "            scene = sample['image'].unsqueeze(0).to(device)\n",
    "            hat_mask = sample['hat_mask'].unsqueeze(0).to(device)\n",
    "            bbox = sample['person_bbox'].unsqueeze(0).to(device)\n",
    "            elev = sample['elevation'].item()\n",
    "            azim = sample['azimuth'].item()\n",
    "            scl = sample['scale'].item()\n",
    "\n",
    "            #Baseline crop\n",
    "            clean_crop = crop_to_person(scene, bbox, target_size=config['detector_input_size'])\n",
    "            clean_results = detector.detect(clean_crop, conf_threshold=0.5)\n",
    "            clean_found = False\n",
    "            clean_conf = 0.0\n",
    "            for name, dets in clean_results.items():\n",
    "                if len(dets[0]['scores']) > 0:\n",
    "                    clean_found = True\n",
    "                    clean_conf = float(dets[0]['scores'].max())\n",
    "                break\n",
    "\n",
    "            if clean_found:\n",
    "                baseline_det += 1\n",
    "\n",
    "            #Attacked crop\n",
    "            rendered_hat, alpha = renderer.render(\n",
    "                best_texture,\n",
    "                elevation=torch.tensor([elev], device=device),\n",
    "                azimuth=torch.tensor([azim], device=device),\n",
    "                scale=torch.tensor([scl], device=device)\n",
    "            )\n",
    "            composite = composite_hat_on_scene(scene, hat_mask, rendered_hat, alpha)\n",
    "            attack_crop = crop_to_person(composite, bbox, target_size=config['detector_input_size'])\n",
    "            attack_results = detector.detect(attack_crop, conf_threshold=0.5)\n",
    "            attack_found = False\n",
    "            attack_conf = 0.0\n",
    "            for name, dets in attack_results.items():\n",
    "                if len(dets[0]['scores']) > 0:\n",
    "                    attack_found = True\n",
    "                    attack_conf = float(dets[0]['scores'].max())\n",
    "                break\n",
    "\n",
    "            if attack_found:\n",
    "                attack_det += 1\n",
    "\n",
    "            if clean_found:\n",
    "                conf_drops.append(clean_conf - attack_conf)\n",
    "\n",
    "    suppressed = baseline_det - attack_det\n",
    "    #Only count suppression on frames where baseline detected\n",
    "    asr = suppressed / baseline_det if baseline_det > 0 else 0.0\n",
    "    avg_drop = np.mean(conf_drops) if conf_drops else 0.0\n",
    "\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"CROP-BASED EVALUATION (comparison with old pipeline)\")\n",
    "    logger.info(f\"  Baseline detected:  {baseline_det}/{total} ({baseline_det/total:.1%})\")\n",
    "    logger.info(f\"  Attack detected:    {attack_det}/{total}\")\n",
    "    logger.info(f\"  Suppressed (ASR):   {suppressed}/{baseline_det} ({asr:.1%})\")\n",
    "    logger.info(f\"  Avg conf drop:      {avg_drop:.3f}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    return asr\n",
    "\n",
    "#Run it\n",
    "crop_asr = evaluate_crop_based(generator, detector, dataset, z_local, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39215c-4e9a-491d-958e-1e00c7b88d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transcriber)",
   "language": "python",
   "name": "transcriber"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
